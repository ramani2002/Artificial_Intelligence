{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a477fb8a",
   "metadata": {},
   "source": [
    "Reference: https://medium.com/@jainashish.079/get-insight-from-your-business-data-build-llm-application-with-langchain-and-hugging-face-using-b32c442ea6cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bba404",
   "metadata": {},
   "source": [
    "The rise of LLM revolutionized the industry and everyone’s thinking about how we can utilize the LLM’s power to support our business and make our customer’s life easier. When we see LLM’s space, we see either these models are proprietary like ChatGPT family of model from OpenAI or if its open source like FLAN-T5 family of model from Google (encoder-decoder model), these are trained on public internet based data.\n",
    "\n",
    "<b><i>Question arises how we can utilize LLM’s with our own business data?</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2e0e0",
   "metadata": {},
   "source": [
    "#  Fine Tune vs RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d24d61",
   "metadata": {},
   "source": [
    "There are two approaches either we can Fine tune the LLMs with our own data for a specific task (like question-answer, summarization etc) or we can use RAG which provides how to incorporate your business data with the LLMs while executing customer query on the business data. Choosing between RAG and fine-tuning the LLM will depends on various factors.\n",
    "\n",
    "* Fine-tuning is great choice when we have a large amount of task’s specific labeled data and want to get insight of the data. For example we want to summarize agent and customer’s chat in call center for getting better insight of the complex data. We can fine-tune the LLMs with chat history database with labeled summery and then inference the model with real time chat history. Fine tuning can be computationally expensive, time consuming and requires big infra (GPUs and memory) resources. However we can fast track the training and consume less memory using method like PEFT (Parameter Efficient fine-tuning) and deal with computational challenges with techniques like Quantization, Purning etc.\n",
    "\n",
    "* RAG is advantageous when we have a retrieval corpus available, covering relevant information for the task (may be question-answer). It provides a way that customers can have conversation with these document or corpus and get answer to their query from these documents using the LLM. For example, we may have data on corporate wiki, web-sites, pdfs and want to run customer query on these documents using LLMs. RAG is more efficient in terms of resource utilization and provides faster results, making it suitable for applications with limited computation resources, real time requirements or low latency needs.\n",
    "\n",
    "It’s cheaper to keep retrieval indices up-to-date (RAG) than to continuously pre train an LLM using Fine tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b707bd",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468cbe22",
   "metadata": {},
   "source": [
    "RAG is a framework for building the LLM powered applications that make use of external data sources outside the model and enhances the input with data, providing the richer context to improve output. One of the option for getting answers from LLM from our own data is that we can pass full data as a context window with question as a prompt which we want to query. Problem with this approach is that LLM are constrained with context window as a prompt (4096 tokens in GPT3). However context window are getting larger and larger with new releases of model (32,768 in GPT4) but still we can not pass a full corpus which may be in Gigabytes.\n",
    "\n",
    "Intuition behind the RAG is that if we can first run customer query with corpus and fetch the relevant specific information in much smaller size and then pass the retrieve information to LLMs in context window with the customer query to get the desired result. This means we need to divide the corpus in multiple chunks and store in a form where we can fetch the relevant chunks based on customer’s query. Best way is to convert the chunks into text embedding and store them in the vector database. A text embedding is a compressed, abstract representation of text data where text of arbitrary length can be represented as a vector of numbers. Embedding is usually learnt from a corpus of text data such as Wikipedia. Think of them as a universal encoding for text, where text with similar content will have similar vectors. We can now use this vector store to find the relevant chunks by doing the similarity search on vector store. Finally we can use the relevant information to create a prompt with customer query and pass that prompt to llm to get the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5cdfd",
   "metadata": {},
   "source": [
    "# Implementing RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a256e",
   "metadata": {},
   "source": [
    "In the blog we will use LangChain (https://www.langchain.com/) — which is excellent open source developer framework for building LLM applications. It provides abstraction to hide all the complexity for building LLM application and provide very easy to use simpler interfaces using python and java-script library. It also provides various integration point for other library/system for document loading, vector stores, calling various LLMs using API and loading the LLM model from Hugging Face model hub. We will also use Hugging Face, it is a platform where machine learning community collaborates on models, datasets and applications. We will use Hugging Face to download one of the open source LLM model FLAN_T5 (https://huggingface.co/google/flan-t5-large) from Google and sentence transformer all-MiniLM-L6-v2 model (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to get the result from these LLMs. We will load these from the local machine. You can easily download these models from Hugging Face by cloning the model repository.It will help you out to run this code without internet or in very constrained environment. Downloading model can take time depending on your network speed:\n",
    "* git lfs install \n",
    "* git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 \n",
    "* git clone https://huggingface.co/google/flan-t5-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086ab0a",
   "metadata": {},
   "source": [
    "We will use python code for illustration purpose. You can use this code for your applications.You can also refer LangChain site for more code references. You also need to install below required python library to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1efc1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (0.0.273)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (0.0.26)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (2.3.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\raman\\appdata\\roaming\\python\\python310\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Install only if the package is absent. Otherwise, comment out the following lines\n",
    "\n",
    "!pip install langchain\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install pypdf\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d17d",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05dd3b",
   "metadata": {},
   "source": [
    "<b>Step1</b> - 1-) Load the corpus from multiple sources. Corpus can be in multiple form like PDFs, Microsoft Words, online or corporate Wiki. LangChain provides different document loader to load the data from different sources likes PDFs, CSV, File directory, HTML, JSON, Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# pdfLoader = PyPDFLoader(\"example_data/Large_language_model.pdf\")\n",
    "pdfLoader = PyPDFLoader(r\"C:\\Users\\raman\\OneDrive\\Documents\\Datasets/large_language_models.pdf\")\n",
    "documents = pdfLoader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8fbff",
   "metadata": {},
   "source": [
    "<b/>Step2</b> - 2-) Once the document is loaded into memory we can divide them into smaller chunks. <b><i>It sounds easy but is tricky to divide the document without loosing relationships between the chunks</i></b>. LangChain provides different types of Text splitters like Split by character, Split code, MarkdownHeaderTextSplitter, Recursively split by character, Split by tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6a1f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "232\n",
      "====================\n",
      "[Document(page_content='JOURNAL OF L ATEX 1\\nA Comprehensive Overview of Large Language\\nModels\\nHumza Naveed, Asad Ullah Khan*, Shi Qiu*, Muhammad Saqib*,\\nSaeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian\\nAbstract —\\nLarge Language Models (LLMs) have recently demonstrated\\nremarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of\\nresearch contributions in this direction. These works encompass\\ndiverse topics such as architectural innovations of the underlying\\nneural networks, context length improvements, model alignment,\\ntraining datasets, benchmarking, efficiency and more. With the\\nrapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive\\nthe bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is\\nimperative that the research community is able to benefit from a', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='the rapidly emerging plethora of literature on LLMs, it is\\nimperative that the research community is able to benefit from a\\nconcise yet comprehensive overview of the recent developments\\nin this field. This article provides that overview to the research\\ncommunity. It not only focuses on a systematic treatment of the\\nexisting literature on a broad range of LLM related concept, but\\nalso pays special attention to providing comprehensive summaries\\nwith extensive details about the individual existing models,\\ndatasets and major insights. We also pay heed to aligning our\\noverview with the emerging outlook of this research direction\\nby accounting for the other recently materializing reviews of\\nthe broader research direction of LLMs. Our self-contained\\ncomprehensive overview of LLMs discusses relevant background\\nconcepts along with covering the advanced topics at the frontier\\nof this research direction. This review article is intended to not', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='concepts along with covering the advanced topics at the frontier\\nof this research direction. This review article is intended to not\\nonly provide a systematic survey, but also a quick comprehensive\\nreference for the researchers and practitioners to draw insights\\nfrom extensive informative summaries of the existing works to\\nadvance the LLM research direction.\\nIndex Terms —\\nLarge Language Models, LLMs, chatGPT, LLM training,\\nLLM Benchmarking\\nI. I NTRODUCTION\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans, and likewise, com-\\nmunication holds paramount importance for machines in their\\ninteractions with humans and other systems. Large Language\\nModels (LLMs) have emerged as cutting-edge artificial intel-\\nligence systems designed to process and generate text, aiming\\nto communicate coherently [1]. The need for LLMs stems\\nfrom the growing demand for machines to handle complex lan-\\nguage tasks, including translation, summarization, information', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='from the growing demand for machines to handle complex lan-\\nguage tasks, including translation, summarization, information\\nretrieval, and conversational interactions. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to deep learning techniques, advancements in\\nVersion: 01 (update on July 10, 2023).\\nGitHub link: https://github.com/humza909/LLM_Survey.git\\n* is for equal contribution.\\nContact e-mail: humza_naveed@yahoo.com\\nFig. 1: The trends in the number of LLM models introduced\\nover the years.\\nneural architectures like transformers, increased computational\\ncapabilities, and the accessibility of training data extracted\\nfrom the internet [2]. These developments have brought about\\na revolutionary transformation by enabling the creation of\\nLarge Language Models (LLMs) that can approximate human-\\nlevel performance on certain evaluation benchmarks [3], [4].\\nLLMs, particularly pre-trained language models (PLM),', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='level performance on certain evaluation benchmarks [3], [4].\\nLLMs, particularly pre-trained language models (PLM),\\nhave shown tremendous generalization abilities for text under-\\nstanding and generation tasks while trained in a self-supervised\\nsetting on a large corpus of text [5], [6], [7]. The performance\\nof pre-trained language models (PLMs) improves significantly\\nwhen fine-tuned for downstream tasks, surpassing the perfor-\\nmance of models trained from scratch. These characteristics of\\nlanguage models motivated researchers to train larger PLMs on\\neven bigger datasets and found that scaling model and dataset\\nsize further improve the generalization abilities.\\nNow modern LLMs are capable of performing various tasks\\nlike code generation, text generation, tool manipulation, rea-\\nsoning, and understanding in zero-shot and few-shot settings\\nin diverse domains, even without requiring any fine-tuning\\non downstream tasks [8], [9], [10]. Such generalization was', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='in diverse domains, even without requiring any fine-tuning\\non downstream tasks [8], [9], [10]. Such generalization was\\npreviously unattainable with smaller models, marking a signif-\\nicant advancement in language modeling. This development\\nhas sparked enthusiasm and excitement within the research\\ncommunity for the enhancement of LLM architectures and\\ntraining strategies, leading to the development of numerous\\nLLMs [11], [12], [13], [8], [9], [10], [14].\\nThe graph presented in Fig 1 depicts an increasing trendarXiv:2307.06435v2  [cs.CL]  18 Aug 2023', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 0}), Document(page_content='JOURNAL OF L ATEX 2\\nFig. 2: The progressive introduction of LLM models demonstrates advances in natural language processing explicitly adapted\\nto various fields and provides increased research, analysis, and application capabilities.\\nin the number of released LLMs, including open-source and\\nclosed-source models, over the years. Furthermore, Fig 2\\nhighlights the names of significant releases of various LLMs\\nand Fig 3 provides a broader overview of LLMs.\\nDuring the early days of Large Language Models (LLMs),\\nmany research efforts focused on developing models for\\ntransfer learning to downstream tasks [11], [12], [15] until\\nthe emergence of models like GPT-3 [8], which demonstrated\\nimpressive performance even without fine-tuning. Due to the\\nclosed-source nature of GPT-3, there was a demand for open-\\nsource alternatives, leading to the development of various\\nmodels [9], [10] operating at the scale of GPT-3 and trained\\non extensive web-based datasets [16], [17], [18], [19]. Subse-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 1}), Document(page_content='models [9], [10] operating at the scale of GPT-3 and trained\\non extensive web-based datasets [16], [17], [18], [19]. Subse-\\nquently, researchers proposed several architectural designs and\\ntraining strategies that showed superior performance compared\\nto GPT-3 across various tasks [15], [14], [20], [21].\\nThe performance of LLMs improves further with instruc-\\ntion fine-tuning, outperforming pre-trained LLMs on various\\nbenchmarks [22], [23]. Instruction fine-tuning of LLMs refers\\nto a specific training approach by incorporating additional\\nprompts or instructions during the fine-tuning phase to guide\\nthe output and thus enable the users to have more fine-\\ngrained control over the outputs of LLMs. These prompts can\\nbe natural language instructions or example demonstrations\\nbased on the task’s requirement. In the literature, differentdatasets have been curated for instruction fine-tuning. These\\ndatasets include more instances and tasks that further improve', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 1}), Document(page_content='datasets include more instances and tasks that further improve\\nthe performance over baselines [24], [23], [25], [26]. When\\nperforming instruction fine-tuning, all the model parameters\\nneed to be updated. However, parameter-efficient fine-tuning\\ntakes a different approach by updating only a small number\\nof parameters while still maintaining good performance. This\\nmethod keeps the original model frozen and adds a few extra\\nparameters at different locations within the model [27], [28],\\n[29], [30], [31]. This approach helps achieve efficient fine-\\ntuning while minimizing the impact on the model’s overall\\nperformance.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM related contributions. Naturally, the research community\\nhas started the effort of organizing this literature as survey\\narticles. For instance, Zhou et al. [32] presented an overview\\nof the foundation models. An impressive effort is recently', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 1}), Document(page_content='articles. For instance, Zhou et al. [32] presented an overview\\nof the foundation models. An impressive effort is recently\\nmade by Zhou et al. [33] in their survey that also discusses\\naspects related to model architectures, fine-tuning, emergent\\nabilities, and more. Another recent survey on augmented lan-\\nguage models provides a historical account of the foundation\\nmodels [34]. In contrast to these surveys, our contribution\\nfocuses on providing a comprehensive yet concise overview\\nof the general direction of LLM research. On one hand, this', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 1}), Document(page_content='JOURNAL OF L ATEX 3\\nFig. 3: A broader overview of LLMs, dividing LLMs into four branches: 1. Training 2. Inference 3. Applications 4. Challenges\\narticle summarizes more details of the individual models as\\ncompared to the existing efforts. On the other, it also covers\\nmore models in providing their summaries. It also delves\\ninto the details of model development, architectures, training\\ndatasets, and other related concepts to provide a self-contained\\ncomprehensive overview of this direction. Hence, this article\\naddresses an important gap of providing a concise yet compre-\\nhensive overview of the rapidly developing general direction\\nof LLM research. Our key contributions are summarized as\\nfollows.\\n•We present the first survey on the developments in LLM\\nresearch with the specific aim of providing concise yet\\ncomprehensive overview of the direction. We present\\nextensive summaries that include fine-grained details of\\nthe reviewed contributions.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 2}), Document(page_content='comprehensive overview of the direction. We present\\nextensive summaries that include fine-grained details of\\nthe reviewed contributions.\\n•In this self-contained article, we cover a range of concepts\\nto comprehend the general direction of LLMs, including\\nbackground concepts, popular models, crucial discover-\\nies, related datasets and evaluation details etc.\\n•Besides paying special attention to the chronological\\norder of LLMs throughout the article, we also summarize\\nmajor findings of the popular contributions, and provide\\ndetailed discussion on the key design and deploymentaspects of LLMs to help practitioners to effectively\\nleverage this technology.\\nIt is noteworthy that although this article is the first contri-\\nbution in its own right in terms of providing a concise yet\\ncomprehensive overview of LLMs, our work complements\\nthe recent (and emerging) surveys of this direction, e.g.,\\n[33], [32]. Infrequently, we also loosely follow the existing', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 2}), Document(page_content='the recent (and emerging) surveys of this direction, e.g.,\\n[33], [32]. Infrequently, we also loosely follow the existing\\nterminologies to ensure providing a more standardized outlook\\nof this research direction. For instance, following [33], our\\nsurvey considers a language model to be large if it has 10B\\nparameters or more. Hence, we discuss such models in detail\\nin this survey. We refer the readers interested in smaller models\\nto [35], [36], [32].\\nThe organization of this paper is as follows. Section II dis-\\ncusses the background of LLMs. Section III focuses on LLMs\\noverview, architectures, and training pipelines and strategies.\\nSection IV presents the key findings derived from each LLM.\\nSection V highlights the configuration and parameters that\\nplay a crucial role in the functioning of these models. The\\nLLM training and evaluation benchmarks are discussed in sec-\\ntion VI, followed by concluding remarks and future direction\\nin the conclusion section.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 2}), Document(page_content='JOURNAL OF L ATEX 4\\nII. B ACKGROUND\\nWe provide the relevant background to understand the key\\nconcepts related to LLM in this section. Aligned with our\\nobjective of providing a comprehensive overview of this di-\\nrection, this section offers a comprehensive yet concise outline\\nof the fundamental concepts. In natural language processing\\nliterature, these concepts are of standard nature. Hence, we\\nfocus more on the intuitive aspects and refer the readers\\ninterested in details to the original works we cite in our\\ndiscussion.\\nA. Tokenization\\nLLMs are trained on text to predict text, and similar to\\nother natural language processing systems, they use tokeniza-\\ntion [37] as the essential preprocessing step. It aims to parse\\nthe text into non-decomposing units called tokens. Tokens\\ncan be characters, subwords [38], symbols [39], or words,\\ndepending on the size and type of the model. Some of the\\ncommonly used tokenization schemes in LLMs are briefed', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='depending on the size and type of the model. Some of the\\ncommonly used tokenization schemes in LLMs are briefed\\nhere. Readers are encouraged to refer to [40] for a detailed\\nsurvey.\\n1. WordPiece [41]: It was introduced in [41] as a novel text\\nsegmentation technique for Japanese and Korean languages to\\nimprove the language model for voice search systems. Word-\\nPiece selects tokens that increase the likelihood of an n-gram-\\nbased language model trained on the vocabulary composed of\\ntokens.\\n2. BPE [39]: Byte Pair Encoding (BPE) has its origin in\\ncompression algorithms. It is an iterative process of generating\\ntokens where pairs of adjacent symbols are replaced by a new\\nsymbol, and the occurrences of the most occurring symbols in\\nthe input text are merged together.\\n3. UnigramLM [38]: In this tokenization, a simple unigram\\nlanguage model (LM) is trained using an initial vocabulary\\nofsubword units. The vocabulary is pruned iteratively by', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='language model (LM) is trained using an initial vocabulary\\nofsubword units. The vocabulary is pruned iteratively by\\nremoving the lowest-probability items from the list, which are\\nthe worst performing on the unigram LM.\\nB. Attention\\nAttention, particularly selective attention , has been widely\\nstudied under perception, psychophysics and psychology. Se-\\nlective attention can be conceived as “the programming by\\nthe O of which stimuli will be processed or encoded and in\\nwhat order this will occur” [42]. While this definition has its\\nroots in visual perception, it has uncanny similarities with the\\nrecently formulated attention [43], [44] (which stimuli will\\nbe processed) and positional encoding (in what order this\\nwill occur) [44] in LLMs. We discuss both in sections II-C\\nand II-D, respectively.\\nC. Attention in LLMs\\nThe attention mechanism computes a representation of the\\ninput sequences by relating different positions ( tokens ) of these', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='C. Attention in LLMs\\nThe attention mechanism computes a representation of the\\ninput sequences by relating different positions ( tokens ) of these\\nsequences. There are various approaches to calculating and\\nimplementing attention, out of which some famous types are\\ngiven below.1. Self-Attention [44]: The self-attention is also known as\\nintra-attention since all the queries, keys and values come from\\nthe same block (encoder or decoder). The self-attention layer\\nconnects all the sequence positions to each other with O(1)\\nspace complexity which is highly desirable for learning long-\\nrange dependencies in the input.\\n2. Cross Attention: In encoder-decoder architectures, the\\noutputs of the encoder blocks act as the queries to the\\nintermediate representation of the decoder, which provides the\\nkeys and values to calculate a representation of the decoder\\nconditioned on the encoder. This attention is called cross-\\nattention.\\n3. Full Attention: The naive implementation of calculating', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='conditioned on the encoder. This attention is called cross-\\nattention.\\n3. Full Attention: The naive implementation of calculating\\nself-attention is known as full attention.\\n4. Sparse Attention [45]: The self-attention has a time\\ncomplexity of O(n2), which becomes prohibitive when scaling\\nthe LLMs to large context windows. An approximation to the\\nself-attention was proposed in [45], which greatly enhanced\\nthe capacity of GPT series LLMs to process a greater number\\nof input tokens in a reasonable time.\\n5. Flash Attention [46]: The bottleneck for calculating the\\nattention using GPUs lies in the memory access rather than the\\ncomputational speed. Flash Attention uses the classical input\\ntiling approach in order to process the blocks of the input\\nin GPU on-chip SRAM rather than doing IO for every token\\nfrom the High Bandwith Memory (HBM). An extension of\\nthis approach to sparse attention follows the speed gains of the\\nfull attention implementation. This trick allows even greater', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='this approach to sparse attention follows the speed gains of the\\nfull attention implementation. This trick allows even greater\\ncontext-length windows in the LLMs as compared to those\\nLLMs with sparse attention.\\nD. Encoding Positions\\nTheattention modules do not consider the order of process-\\ning by design. Transformer [44] introduced “positional encod-\\nings” to feed information about the position of the tokens in\\ninput sequences. Several variants of positional encoding have\\nbeen proposed [47], [48]. Interestingly, a recent study [49]\\nsuggests that adding this information may not matter for the\\nstate-of-the-art decoder-only Transformers.\\n1. Absolute: This is the most straightforward approach to\\nadding the sequence order information by assigning a unique\\nidentifier to each position of the sequence before passing it to\\nthe attention module.\\n2. Relative: In order to pass the information of the rel-\\native dependencies of different tokens appearing at different', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='the attention module.\\n2. Relative: In order to pass the information of the rel-\\native dependencies of different tokens appearing at different\\nlocations in the sequence, a relative positional encoding is\\ncalculated by some kind of learning. Two famous types of\\nrelative encodings are:\\nAlibi [47] In this approach, a scalar bias is subtracted from the\\nattention score calculated using two tokens which increases\\nwith the distance between the positions of the tokens. This\\nlearned approach effectively favors using recent tokens for\\nattention.\\nRoPE Keys, queries and values are all vectors in the LLMs.\\nRoPE [48] involves the rotation of the query and key represen-\\ntations at an angle proportional to their absolute positions of\\nthe tokens in the input sequence. This step results in a relative', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 3}), Document(page_content='JOURNAL OF L ATEX 5\\npositional encoding scheme which decays with the distance\\nbetween the tokens.\\nE. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of the neural networks, as proved in [50]. The\\nmodern activation functions used in LLMs are different from\\nthe earlier squashing functions but are critical to the success\\nof LLMs. We discuss these activation functions in this section.\\n1. ReLU [51]: Rectified linear unit (ReLU) is defined as\\nReLU (x) =max(0, x) (1)\\n2. GeLU [52]: Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [53] and zoneout [54]. It is the\\nmost widely used activation function in contemporary LLM\\nliterature.\\n3. GLU variants [55]: Gated Linear Unit [56] is a neural\\nnetwork layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed ( σ) linear projection\\nof the input given as\\nGLU (x, W, V, b, c ) = (xW+b)⊗σ(xV+c), (2)', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='transformation and a sigmoid transformed ( σ) linear projection\\nof the input given as\\nGLU (x, W, V, b, c ) = (xW+b)⊗σ(xV+c), (2)\\nwhere Xis the input of layer and l,W, b, V andcare learned\\nparameters.\\nGLU was modified in [55] to evaluate the effect of different\\nvariations in the training and testing of transformers, resulting\\nin better empirical results. Here are the different GLU varia-\\ntions introduced in [55] and used in LLMs.\\nReGLU (x, W, V, b, c ) =max(0, xW +b)⊗,\\nGEGLU (x, W, V, b, c ) =GELU (xW+b)⊗(xV+c),\\nSwiGLU (x, W, V, b, c, β ) =Swishβ (xW+b)⊗(xV+c).\\nF . Layer Normalization\\nLayer normalization leads to faster convergence and is a\\nwidely used component in transformers. In this section, we\\nprovide different normalization techniques widely used in\\nLLM literature.\\n1. LayerNorm: Layer norm computes statistics over all the\\nhidden units in a layer (l)as follows:\\nul=1\\nnnX\\nial\\ni σl=vuut1\\nnnX\\ni(al\\ni−ul)2, (3)\\nwhere nis the number of neurons in the layer landal\\niis the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='hidden units in a layer (l)as follows:\\nul=1\\nnnX\\nial\\ni σl=vuut1\\nnnX\\ni(al\\ni−ul)2, (3)\\nwhere nis the number of neurons in the layer landal\\niis the\\nsummed input of the ineuron in layer l. LayerNorm provides\\ninvariance to rescaling of the weights and re-centering of the\\ndistribution.\\n2. RMSNorm: [57] proposed that the invariance properties\\nof LayerNorm are spurious, and we can achieve the same\\nperformance benefits as we get from LayerNorm by using a\\ncomputationally efficient normalization technique that trades\\noff re-centering invariance with speed. LayerNorm gives the\\nnormalized summed input to layer las follows\\nal\\ni=al\\ni−ul\\nσgl\\ni (4)where gl\\niis the gain parameter. RMSNorm [57] modifies al\\ni\\nas\\nal\\ni=al\\ni\\nRMS(al)gl\\ni,where RMS (al) =vuut1\\nnnX\\ni(al\\ni)2.(5)\\n3. Pre-Norm and Post-Norm: LLMs use transformer [44]\\narchitecture with some variations. The original implementa-\\ntion [44] used layer normalization after the residual con-\\nnection, commonly called post-LN, concerning the order of', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='tion [44] used layer normalization after the residual con-\\nnection, commonly called post-LN, concerning the order of\\nMultihead attention – Residual – LN . There is another order\\nof the normalization, referred to as pre-LN [58] due to the\\nposition of the normalization step before the self-attention\\nlayer as in LN – Multihead attention – Residual . Pre-LN is\\nknown to provide more stability in the training [59].\\n4. DeepNorm: While pre-LN has certain benefits over post-\\nLN training, pre-LN training has an unwanted effect on the\\ngradients [59]. The earlier layers have larger gradients than\\nthose at the bottom. DeepNorm [60] mitigates these adverse\\neffects on the gradients. It is given as\\nxlf=LN(αxlp+Glp(xlp, θlp), (6)\\nwhere αis a constant and θlprepresents the parameters of\\nlayer lp. These parameters are scaled by another constant β.\\nBoth of these constants depend only on the architecture.\\nG. Distributed LLM Training\\nThis section describes distributed LLM training approaches', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='Both of these constants depend only on the architecture.\\nG. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [9], [61], [62], [63].\\n1. Data Parallelism: Data parallelism replicates the model\\non multiple devices where data in a batch gets divided across\\ndevices. At the end of each training iteration weights are\\nsynchronized across all devices.\\n2. Tensor Parallelism: Tensor parallelism shards a tensor\\ncomputation across devices. It is also known as horizontal\\nparallelism or intra-layer model parallelism.\\n3. Pipeline Parallelism: Pipeline parallelism shards model\\nlayers across different devices. This is also known as vertical\\nparallelism.\\n4. Model Parallelism: A combination of tensor and pipeline\\nparallelism is known as model parallelism.\\n5. 3D Parallelism: A combination of data, tensor, and\\nmodel parallelism is known as 3D parallelism.\\n6. Optimizer Parallelism: Optimizer parallelism also', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='model parallelism is known as 3D parallelism.\\n6. Optimizer Parallelism: Optimizer parallelism also\\nknown as zero redundancy optimizer [61] implements opti-\\nmizer state partitioning, gradient partitioning, and parameter\\npartitioning across devices to reduce memory consumption\\nwhile keeping the communication costs as low as possible.\\nH. Libraries\\nSome commonly used libraries for LLM training are: 1)\\nTransformer [64], 2) DeepSpeed [65], 3) Megatraon-LM [62],\\n4) JAX [66], 5) Colossal-AI [67], 6) BMTrain [63], 7)\\nFastMoE [68], and frameworks are 1) MindSpore [69], 2)\\nPyTorch [70], 3) Tensorflow [71], 4) MXNet [72].', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 4}), Document(page_content='JOURNAL OF L ATEX 6\\nI. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\n1. Quality Filtering: For better results, training data quality\\nis essential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\n2. Data Deduplication: Duplicated data can affect model\\nperformance and increase data memorization; therefore, to\\ntrain LLMs, data deduplication is one of the preprocessing\\nsteps. This can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\n3. Privacy Reduction: Most of the training data for LLMs\\nis collected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='is collected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning the mentioned information.\\nJ. Architectures\\nHere we discuss the variants of the transformer architectures\\nat a higher level which arise due to the difference in the\\napplication of the attention and the connection of transformer\\nblocks. An illustration of attention patterns of these architec-\\ntures is shown in Figure 4.\\n1. Encoder Decoder: Transformers were originally de-\\nsigned as sequence transduction models and followed other\\nprevalent model architectures for machine translation systems.\\nThey selected encoder-decoder architecture to train human\\nlanguage translation tasks. This architecture is adopted by [11],\\n[15]. In this architectural scheme, an encoder encodes the\\ninput sequences to variable length context vectors, which are', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='[15]. In this architectural scheme, an encoder encodes the\\ninput sequences to variable length context vectors, which are\\nthen passed to the decoder to maximize a joint objective of\\nminimizing the gap between predicted token labels and the\\nactual target token labels.\\n2. Causal Decoder: The underlying objective of an LLM\\nis to predict the next token based on the input sequence. While\\nadditional information from the encoder binds the prediction\\nstrongly to the context, it is found in practice that the LLMs\\ncan learn as well absent this encoder [73] and adding the\\ncontext in the decoder. Similar to the original encoder-decoder\\narchitecture’s decoder block, this decoder restricts the flow\\nof information backward, i.e., the predicted token tkonly\\ndepends on the tokens preceded by and up to tk−1. This is\\nthe most widely used variant in the state-of-the-art LLMs.\\n3. Prefix Decoder: The causal masked attention is reason-\\nable in the encoder-decoder architectures where the encoder', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='3. Prefix Decoder: The causal masked attention is reason-\\nable in the encoder-decoder architectures where the encoder\\ncan attend to all the tokens in the sentence from every position\\nusing self-attention. This means that the encoder can also\\nattend to tokens tk+1totnin addition to the tokens from t1\\ntotk−1while calculating the representation for tk. But when\\nwe drop the encoder and only keep the decoder, we also lose\\nthis flexibility in attention. A variation in the decoder-only\\narchitectures is by changing the mask from strictly causal to\\nfully visible on a portion of the input sequence, as shown\\nin Figure 4. The Prefix decoder is also known as non-causal\\ndecoder architecture.\\nFig. 4: An example of attention patterns in language models,\\nimage is taken from [74].\\nFig. 5: An example of language model training objectives,\\nimage from [74].\\nK. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [74].', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='image from [74].\\nK. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [74].\\n1. Full Language Modeling: An autoregressive language\\nmodeling objective where the model is asked to predict future\\ntokens given the previous tokens, an example is shown in\\nFigure 5.\\n2. Prefix Language Modeling: A non-causal training objec-\\ntive, where a prefix is chosen randomly and only remaining\\ntarget tokens are used to calculate the loss. An example is\\nshown in Figure 5.\\n3. Masked Language Modeling: In this training objective,\\ntokens or spans (a sequence of tokens) are masked randomly\\nand the model is asked to predict masked tokens given the\\npast and future context. An example is shown in Figure 5.\\n4. Unified Language Modeling: Unified language model-\\ning [75] is a combination of causal, non-causal, and masked\\nlanguage training objectives. Here in masked language mod-\\neling, the attention is not bidirectional but unidirectional,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='language training objectives. Here in masked language mod-\\neling, the attention is not bidirectional but unidirectional,\\nattending either left-to-right or right-to-left context.\\nL. Model Adaptation\\nThis section discusses various model adaptation techniques,\\nwhere a model is pre-trained on large data and then adapted\\nfor downstream tasks.\\n1. Transfer Learning: Fine-tuning a pre-trained model with\\ndata for the downstream task is known as transfer learning. In\\nthis type of model adaptation, the model is initialized with\\npre-trained weights and updated according to the new data.\\nSome of the LLMs employing this technique are [11], [12],\\n[15], [20].\\n2. Parameter Efficient Learning: The parameter efficient\\nlearning fine-tunes a few parameters either by adding new\\nparameters to the model or the existing ones.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 5}), Document(page_content='JOURNAL OF L ATEX 7\\nPrompt Tuning: [30], [76] adds trainable prompt token em-\\nbeddings as prefixes or free-style to the input token embed-\\ndings. During fine-tuning only these embeddings parameters\\nare trained for the downstream task while keeping the rest of\\nthe weights frozen.\\nPrefix Tuning: [31] adds task-specific trainable prefix vectors\\nto the transformer layers, where only prefix parameters are\\nfine-tuned, and the rest of the model stays frozen. The input\\nsequence tokens can attend prefixes acting as virtual tokens.\\nAdapter Tuning: module is an encoder-decoder architecture\\nthat is placed either sequential or parallel to the attention and\\nfeed-forward layers in the transformer block [77], [28], [29].\\nOnly these layers are fine-tuned, and the rest of the model is\\nkept frozen.\\n3. Instruction Finetuning: Instruction tuning is an approach\\nto fine-tuning pre-trained models on instruction formatted\\ndata. Instructions generally comprise multiple tasks in plain', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='to fine-tuning pre-trained models on instruction formatted\\ndata. Instructions generally comprise multiple tasks in plain\\nnatural language, guiding the model to respond according to\\nthe prompt and the input. The training data consists of an\\ninstruction and an input-output pair. More details on formatting\\ninstruction data and its various styles are available in [33].\\n4. Alignment Tuning: Alignment techniques play a crucial\\nrole in ensuring large language models (LLMs) operate ac-\\ncording to human intentions and values. These models can\\ngenerate text and make decisions, making it vital to control\\ntheir behavior and outputs to avoid undesirable outcomes.\\nAlignment techniques aim to bridge the gap between what\\nhumans expect from LLMs and their actual behavior. A model\\nis defined to be an “aligned” model if the model fulfils three\\ncriteria’s of helpful, honest and harmless or “HHH” [78].\\nThe researchers investigated alignment techniques from hu-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='criteria’s of helpful, honest and harmless or “HHH” [78].\\nThe researchers investigated alignment techniques from hu-\\nman feedback, specifically emphasizing reinforcement learn-\\ning from human feedback (RLHF) [79].\\n4.1 Reward modelling: The RLHF (Reinforcement\\nLearning from Human Feedback) [79] encompasses reward\\nmodelling and reinforcement learning. During the reward\\nmodelling phase, the data collection approach shifts towards\\ncomparisons. A dataset is created, starting with an identical\\nprompt, and the task is to create multiple completions using the\\n(Supervised Fine-tuned) SFT model, which human evaluators\\nthen rank. Assuming one completion is significantly superior\\nto the others based on the ranking, a binary classification is\\nperformed on all possible pairs of completions. This stage is\\nknown as reward modelling Training. A token in each row\\nrepresents the completions generated by the SFT model. An\\nadditional special reward readout token is appended at the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='represents the completions generated by the SFT model. An\\nadditional special reward readout token is appended at the\\nend of each row. The focus of supervision lies solely on a\\nreward token within the transformer. The transformer predicts\\na reward value indicating the quality of each completion for\\na given prompt. The ground truth ranking is available for\\nreference.\\n4.2 Reinforcement Learning: In the previous stage, a re-\\nward model assigns a high score to completions, and therefore\\nthe associated token in this stage will receive reinforcement,\\nincreasing their probabilities for future occurrences. On the\\nother hand, when the reward model assigns a low score will\\nhave lower probabilities for the future. This iterative process\\nFig. 6: Unified text-to-text training example, source image\\nfrom [11].\\nis repeated across multiple prompts and batches, leading to the\\ndevelopment of a policy that generates excellent outputs. This\\nprocess represents the RLHF (Reinforcement Learning from', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='development of a policy that generates excellent outputs. This\\nprocess represents the RLHF (Reinforcement Learning from\\nHuman Feedback) pipeline, which culminates in the deploy-\\nment of a model. For example, ChatGPT is an example of an\\nRLHF model, while models like Vicuna-13B are categorized\\nas SFT (Supervised Fine-Tuning) models.\\n5. In-context Learning: No fine-tuning is involved in this\\ntype of model adaptation. The model is shown multiple\\ninput-output demonstration pairs to generate a desired input\\nresponse. This adaptation is similar to a few-shot learning\\nbut without requiring any parameter update. More details on\\nformatting demonstrations are available in [80], [33].\\n6. Chain-of-thought Prompting: Chain-of-thought prompt-\\ning (CoT) is a special case of prompting where demonstra-\\ntions contain reasoning information aggregated with inputs\\nand outputs so that the model generates outcomes with rea-\\nsonings. Some examples in literature train LLMs with CoT', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='and outputs so that the model generates outcomes with rea-\\nsonings. Some examples in literature train LLMs with CoT\\nreasoning, whereas other utilizes LLMs’ CoT abilities without\\nfine-tuning. More details on designing prompts are available\\nin [81], [33].\\nIII. L ARGE LANGUAGE MODELS\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\nA. Pre-Trained Models\\n1. General Purpose:\\n1.1 T5 [11]: An encoder-decoder model trained on the\\nColossal Clean Crwal Corpus (C4) dataset with a unified text-\\nto-text training for all NLP problems, shown in Figure 6.\\nThe model differs from the traditional transformer model [44].\\nThese changes include no bias in layer normalization, using\\nrelative positional embedding, and placing layer normalization\\noutside the residual path. The masked language modeling\\nis used as a pre-training objective where spans (consecutive\\ntokens) were replaced with a single mask instead of separate', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='is used as a pre-training objective where spans (consecutive\\ntokens) were replaced with a single mask instead of separate\\nmasks for each token. This type of masking speeds up the\\ntraining as it produces shorter sequences. After pre-training,\\nthe model is fine-tuned using adapter layers [77] for down-\\nstream tasks.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 6}), Document(page_content='JOURNAL OF L ATEX 8\\nFig. 7: The image is the article of [84], showing an example\\nof PanGu- αarchitecture.\\n1.2 GPT-3 [8]: The architecture of GPT-3 is mostly the\\nsame as GPT-2 [82] but with dense and sparse attention in\\ntransformer layers similar to the Sparse Transformer [45]. The\\nmodel is trained on data taken from CommonCrawl, Web-\\ntext dataset, books corpora, and English-language Wikipedia.\\nLarge models can train on larger batch sizes with a lower\\nlearning rate; in order to decide the batch size during training,\\nGPT-3 uses the gradient noise scale as in [83]. Overall,\\nGPT-3 increases model parameters to 175B showing that the\\nperformance of large language models improves with the scale\\nand is competitive with the fine-tuned models.\\n1.3 mT5 [12]: A multilingual T5 model [11] trained\\non the mC4 dataset with 101 languages. The dataset is\\nextracted from the public common crawl scrape. The model\\nuses GeGLU activation and trains with a vocab size of 250,000', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='extracted from the public common crawl scrape. The model\\nuses GeGLU activation and trains with a vocab size of 250,000\\nto cover multiple languages. To avoid over-fitting or under-\\nfitting for a language, mT5 employs a data sampling procedure\\nto select samples from all languages. The paper suggests using\\na small amount of pre-training datasets, including all languages\\nwhen fine-tuning for a task using English language data. This\\nallows the model to generate non-English outputs.\\n1.4 PanGu- α[84]: An autoregressive model trained on\\n1.1TB Chinese data collected from Common Crawl, e-Books,\\nencyclopedia, etc. Additional to the standard transformer\\nmodel, it has a query layer after stacked transformer layers,\\nexample shown in Figure 7. The purpose of the query layer\\nis to predict the next token. Its structure is similar to the\\ntransformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 7.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 7.\\nThe model is trained using MindSpore with five-dimensional\\nparallelism, i.e., data parallelism, op-level model parallelism,\\npipeline model parallelism, optimizer parallelism, and rema-\\nterialization.\\na=pnWq\\nhWk\\nhTHT\\nL (7)\\n1.5 CPM-2 [13]: Cost-efficient Pre-trained language\\nModels (CPM-2) pre-trains bilingual (English and Chinese)\\n11B and 198B mixture-of-experts (MoE) models on the Wu-\\nDaoCorpus [85] dataset. It has an encoder-decoder architecture\\nwith a bidirectional encoder and a unidirectional decoder. The\\ntokenization process removes “_” white space tokens in thesentencepiece tokenizer. The models are trained with knowl-\\nedge inheritance, starting with only the Chinese language in\\nthe first stage and then adding English and Chinese data. This\\ntrained model gets duplicated multiple times to initialize the\\n198B MoE model. Moreover, to use the model for downstream', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='trained model gets duplicated multiple times to initialize the\\n198B MoE model. Moreover, to use the model for downstream\\ntasks, CPM-2 experimented with both complete fine-tuning\\nand prompt fine-tuning as in [27] where only prompt-related\\nparameters are updated by inserting prompts at various posi-\\ntions, front, middle, and back. CPM-2 also proposes INFMOE,\\na memory-efficient framework with a strategy to dynamically\\noffload parameters to the CPU for inference at a 100B scale. It\\noverlaps data movement with inference computation for lower\\ninference time.\\n1.6 ERNIE 3.0 [86]: ERNIE 3.0 takes inspiration from\\nmulti-task learning to build a modular architecture using\\nTransformer-XL [87] as the backbone. The universal repre-\\nsentation module is shared by all the tasks, which serve as the\\nbasic block for task-specific representation modules, which are\\nall trained jointly for natural language understanding, natural\\nlanguage generation, and knowledge extraction. This LLM is', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='all trained jointly for natural language understanding, natural\\nlanguage generation, and knowledge extraction. This LLM is\\nprimarily focused on the Chinese language, claims to train\\non the largest Chinese text corpora for LLM training, and\\nachieved state-of-the-art in 54 Chinese NLP tasks.\\n1.7 Jurassic-1 [88]: A pair of auto-regressive language\\nmodels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model. The Jurassic-1 models are mainly\\nstructured on the Transformer decoder module [44], while\\nthe architecture modifications proposed by GPT-2 [82] are\\nalso incorporated. In particular, the training vocabulary items\\nof Jurassic-1 comprise word pieces, complete words, and\\nmulti-word expressions without any word boundaries, where\\npossible out-of-vocabulary instances are interpreted as Uni-\\ncode bytes. In practice, data collected from publicly available\\nresources are formed in the GPT-3’s data structure to train the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='code bytes. In practice, data collected from publicly available\\nresources are formed in the GPT-3’s data structure to train the\\nJurassic-1 models following the conventional self-supervised\\nauto-regressive training objective. Compared to the GPT-3\\ncounterparts, the Jurassic-1 models apply a more balanced\\ndepth-to-width self-attention architecture [89] and an improved\\ntokenizer for a faster prediction based on broader resources,\\nachieving a comparable performance in zero-shot learning\\ntasks and a superior performance in few-shot learning tasks\\ngiven the ability to feed more examples as a prompt.\\n1.8 HyperCLOVA [90]: The architecture is the same as\\nthat of GPT3 [8] with morphene aware byte level encoding\\ntokenization step. A large Korean-centric corpus gathered from\\nvarious sources (see table for details) is trained using Megatron\\nLM. Prompt-based tuning is also applied to enhance perfor-\\nmance on downstream tasks. The main objective of training', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='LM. Prompt-based tuning is also applied to enhance perfor-\\nmance on downstream tasks. The main objective of training\\nthis model is to see how the non-English language model fares\\ncompared to universally found English-based LMs.\\n1.9 Yuan 1.0 [91]: A large singleton language model\\nwith 245B parameters. The Yuan 1.0 is structured as a Trans-\\nformer [44]. A Chinese corpus with 5TB of high-quality text is\\ncreated to train Yuan 1.0 model, where the raw data is collected\\nfrom Internet resources. A Massive Data Filtering System\\n(MDFS) built on Spark is developed to process the raw data via\\ncoarse and fine filtering techniques. To speed up the training\\nof Yuan 1.0 with the aim of saving energy expenses and', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 7}), Document(page_content='JOURNAL OF L ATEX 9\\ncarbon emissions, a collaborative design of model architecture\\nand large-scale distributed training is introduced. In practice,\\nthe Yuan 1.0 model performs well on text classification,\\nWinograd Schema, natural language inference, and reading\\ncomprehension tasks.\\n1.10 Gopher [92]: It is the largest of six causal decoder\\nLLMs trained on the subsets of MassiveWeb, Books, C4,\\nNews, GitHub, and Wikipedia samples from high-quality\\ncurated MassiveText. The model is a modified version of\\nTransformer architecture used in [82]. The Gopher family of\\nmodels ranges from 44M to 280B parameters in size to study\\nthe effect of scale on the LLMs performance. The 280B model\\nbeats GPT-3 [8], Jurrasic-1 [88], MT-NLG [21], and others on\\n81% of the evaluated tasks.\\n1.11 ERNIE 3.0 TITAN [93]: ERNIE 3.0 Titan extends\\nERNIE 3.0 by training a larger model with 26x the number of\\nparameters of the latter. This bigger model outperformed other', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='ERNIE 3.0 by training a larger model with 26x the number of\\nparameters of the latter. This bigger model outperformed other\\nstate-of-the-art models in 68 NLP tasks. LLMs produce text\\nwith incorrect facts. In order to have control of the generated\\ntext with factual consistency, ERNIE 3.0 Titan adds another\\ntask, Credible and Controllable Generations , to its multi-\\ntask learning setup. It introduces additional self-supervised\\nadversarial and controllable language modeling losses to the\\npre-training step, which enables ERNIE 3.0 Titan to beat\\nother LLMs in their manually selected Factual QA task set\\nevaluations.\\n1.12 GPT-NeoX-20B [94]: An auto-regressive model\\nthat largely follows GPT-3 with a few deviations in architec-\\nture design, trained on the Pile dataset without any data dedu-\\nplication. GPT-NeoX has parallel attention and feed-forward\\nlayers in a transformer block, given in Eq. 8, that increases\\nthroughput by 15%. It uses rotary positional embedding [48],', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='layers in a transformer block, given in Eq. 8, that increases\\nthroughput by 15%. It uses rotary positional embedding [48],\\napplying it to only 25% of embedding vector dimension as\\nin [95]. This reduces the computation without performance\\ndegradation. Opposite to GPT-3, which uses dense and sparse\\nlayers, GPT-NeoX-20B uses only dense layers. The hyperpa-\\nrameter tuning at this scale is difficult; therefore, the model\\nchooses hyperparameters from the method [8] and interpolates\\nvalues between 13B and 175B models for the 20B model. The\\nmodel training is distributed among GPUs using both tensor\\nand pipeline parallelism.\\nx+Attn(LN 1(x)) +FF(LN 2(x)) (8)\\n1.13 OPT [10]: It is a clone of GPT-3, developed\\nwith the intention to open-source a model that replicates\\nGPT-3 performance. The model was trained using RoBERTa,\\nThe Pile, and PushShift.io Reddit datasets. Training of OPT\\nemploys dynamic loss scaling [96] and restarts from an\\nearlier checkpoint with a lower learning rate whenever loss', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='employs dynamic loss scaling [96] and restarts from an\\nearlier checkpoint with a lower learning rate whenever loss\\ndivergence is observed. Overall, the performance of OPT-175B\\nmodels is comparable to the GPT3-175B model.\\n1.14 BLOOM [9]: A causal decoder model trained on\\nROOTS corpus with the aim of open-sourcing an LLM. The\\narchitecture of BLOOM is shown in Figure 8, with differences\\nlike ALiBi positional embedding, an additional normalization\\nlayer after the embedding layer as suggested by the bitsand-\\nbytes1library. These changes stabilize training with improved\\n1https://github.com/TimDettmers/bitsandbytes\\nFig. 8: The BLOOM architecture example sourced from [9].\\ndownstream performance.\\n1.15 GLaM [97]: Generalist Language Model (GLaM)\\nrepresents a family of language models using a sparsely ac-\\ntivated mixture-of-experts (MoE) structure [98], [99]. Specif-\\nically, the architecture of GLaM is derived from a Decoder-\\nonly Transformer [44]. To gain more model capacity while', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='ically, the architecture of GLaM is derived from a Decoder-\\nonly Transformer [44]. To gain more model capacity while\\nreducing computation, the experts are sparsely activated where\\nonly the best two experts are used to process each input token.\\nThe largest GLaM model, GLaM (64B/64E), is about 7 ×\\nlarger than GPT-3 [8], while only a part of the parameters is\\nactivated per input token. To effectively compare with GPT-3,\\nthe evaluation of GLaM follows the similar zero, one, and few-\\nshot learning protocols as in GPT-3. Specifically, the largest\\nGLaM (64B/64E) model achieves better overall results while\\nconsuming only one-third of GPT-3’s training energy.\\n1.16 MT-NLG [21]: A causal decoder transformer\\ntrained on two snapshots of Common Crawl along with\\nsome other datasets given in table VIII. MT-NLG uses 8-way\\ntensor slicing by Megatron for memory efficiency and 35-way\\npipeline parallelism using DeepSpeed for compute efficiency\\nto train a 530B model, roughly 3 ×GPT-3 model parameters.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='pipeline parallelism using DeepSpeed for compute efficiency\\nto train a 530B model, roughly 3 ×GPT-3 model parameters.\\nThis model beats GPT-3 on a number of evaluations.\\n1.17 Chinchilla [100]: A causal decoder trained on the\\nsame dataset as the Gopher [92] but with a little different\\ndata sampling distribution (sampled from MassiveText). The\\nmodel architecture is similar to the one used for Gopher,\\nwith the exception of AdamW optimizer instead of Adam.\\nChinchilla identifies the relationship that model size should\\nbe doubled for every doubling of training tokens. Over 400\\nlanguage models ranging from 70 million to over 16 billion\\nparameters on 5 to 500 billion tokens are trained to get the\\nestimates for compute-optimal training under a given budget.\\nThe authors train a 70B model with the same compute budget\\nas Gopher (280B) but with 4 times more data. It outperforms\\nGopher [92], GPT-3 [8], and others on various downstream\\ntasks, after fine-tuning.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='as Gopher (280B) but with 4 times more data. It outperforms\\nGopher [92], GPT-3 [8], and others on various downstream\\ntasks, after fine-tuning.\\n1.18 AlexaTM [101]: The first multilingual sequence-\\nto-sequence model (20B parameter) is capable of in-context\\nlearning. The pre-training data is collected from Wikipedia\\nand mC4 dataset [12] covering 12 programming languages. To\\nenable the AlexaTM 20B model to perform on both spoken\\nand written cases, all data is converted into spoken format via a\\nwritten-to-spoken formatter. In addition to pre-training on the\\ndenoising task, an extra Causal Language Modeling (CLM)', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 8}), Document(page_content='JOURNAL OF L ATEX 10\\ntask is performed for 20% of the time to help the model with\\nefficient in-context learning. In practice, the model is asked\\nto continue the input instead of denoising the input once a\\nspecial CLM token is attached to the beginning of the input.\\n1.19 PaLM: A causal decoder model trained on a dataset\\nof 780B tokens collected from webpages, books, Wikipedia,\\nnews, and others, given in Table VIII. The PaLM has parallel\\nattention and feed-forward layers similar to Eq. 8, speeding\\nup training 15 times faster. Additional changes to the conven-\\ntional transformer model include SwiGLU activation, RoPE\\nembeddings, multi-query attention that saves computation cost\\nduring decoding, and shared input-output embeddings. During\\ntraining, loss spiking was observed, and to fix it, model\\ntraining was restarted from a 100 steps earlier checkpoint\\nby skipping 200-500 batches around the spike. Moreover, the\\nmodel was found to memorize around 2.4% of the training', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='by skipping 200-500 batches around the spike. Moreover, the\\nmodel was found to memorize around 2.4% of the training\\ndata at the 540B model scale, whereas this number was lower\\nfor smaller models.\\n1.20 U-PaLM [20]: This method trains PaLM for 0.1%\\nadditional compute with UL2 (also named as UL2Restore)\\nobjective [15] using the same dataset and outperforms baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal\\ndecoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n1.21 UL2 [15]: An encoder-decoder architecture trained\\nusing a mixture of denoisers (MoD) objectives. Denoisers\\ninclude 1) R-Denoiser: a regular span masking, 2) S-Denoiser:\\nwhich corrupts consecutive tokens of a large sequence and\\n3) X-Denoiser: which corrupts a large number of tokens', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='which corrupts consecutive tokens of a large sequence and\\n3) X-Denoiser: which corrupts a large number of tokens\\nrandomly. During pre-training, UL2 includes a denoiser token\\nfrom R, S, X to represent a denoising setup. It helps improve\\nfine-tuning performance for downstream tasks that bind the\\ntask to one of the upstream training modes. This MoD style\\nof training outperforms the T5 model on many benchmarks.\\n1.22 GLM-130B [102]: GLM-130B is a bilingual (En-\\nglish and Chinese) model trained using an auto-regressive\\nmask infilling pre-training objective similar to the GLM [103].\\nThis training style makes the model bidirectional as compared\\nto GPT-3, which is unidirectional. Opposite to the GLM, the\\ntraining of GLM-130B includes a small amount of multi-task\\ninstruction pre-training data (5% of the total data) along with\\nthe self-supervised mask infilling. To stabilize the training, it\\napplies embedding layer gradient shrink.\\n1.23 LLaMA [104]: A set of foundation language models', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='applies embedding layer gradient shrink.\\n1.23 LLaMA [104]: A set of foundation language models\\nvarying from 7B to 65B parameters. The overall architec-\\nture of LLaMA follows the Transformer [44], while a few\\nsubsequently proposed improvements of normalization [57],\\nactivation [55], and positional embedding [48] operations are\\nincorporated for better performances. About 67% of LLaMA’s\\npre-training data is collected from English CommonCrawl\\nfollowing the CCNet method [105]. LLaMA and the asso-\\nciated variants are widely used for parameter-efficient tuning,\\nespecially for instruction following tasks.\\n1.24 PanGu- Σ[106]: An autoregressive model with\\nparameters copied from PanGu- αand extended to a trillion\\nscale with Random Routed Experts (RRE), the architecturaldiagram is shown in Figure 9. RRE is similar to the MoE\\narchitecture, with distinctions at the second level, where tokens\\nare randomly routed to experts in a domain instead of using a', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='architecture, with distinctions at the second level, where tokens\\nare randomly routed to experts in a domain instead of using a\\nlearnable gating method. The model has bottom layers densely\\nactivated and shared across all domains, whereas top layers are\\nsparsely activated according to the domain. This training style\\nallows extracting task-specific models and reduces catastrophic\\nforgetting effects in case of continual learning.\\n2. Coding:\\n2.1 CodeGen [107]: CodeGen has similar architecture to\\nthe PaLM [14], i.e., parallel attention, MLP layers, and RoPE\\nembeddings. The model is trained on both natural language\\nand programming language data sequentially (trained on the\\nfirst dataset, then the second and so on) on the following\\ndatasets 1) PILE, 2) BIGQUERY and 3) BIGPYTHON. Code-\\nGen proposed a multi-step approach to synthesizing code. The\\npurpose is to simplify the generation of long sequences where\\nthe previous prompt and generated code are given as input with', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='purpose is to simplify the generation of long sequences where\\nthe previous prompt and generated code are given as input with\\nthe next prompt to generate the next code sequence. CodeGen\\nopensource a Multi-Turn Programming Benchmark (MTPB)\\nto evaluate multi-step program synthesis.\\n2.2 Codex [108]: This LLM is trained on a subset\\nof public Python Github repositories to generate code from\\ndocstrings. Computer programming is an iterative process\\nwhere the programs are often debugged and updated before\\nfulfilling the requirements. Similarly to this, Codex generates\\n100 versions of a program by repetitive sampling for a given\\ndescription, which produces a working solution for 77.5% of\\nthe problems passing unit tests. Its powerful version powers\\nGithub Copilot2.\\n2.3 AlphaCode [109]: A set of large language models\\ndesigned for competition-level code generation tasks. Ba-\\nsically, the AlphaCode models follow an encoder-decoder\\ntransformer architecture [44] ranging from 300M to 41B', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='sically, the AlphaCode models follow an encoder-decoder\\ntransformer architecture [44] ranging from 300M to 41B\\nparameters. Moreover, the multi-query attention [110] is ap-\\nplied to reduce memory and cache costs. Since competitive\\nprogramming problems highly require deep reasoning and an\\nunderstanding of complex natural language algorithms, the\\nAlphaCode models are pre-trained on filtered GitHub code in\\npopular languages and then fine-tuned on a new competitive\\nprogramming dataset named CodeContests. Particularly, the\\nCodeContests dataset mainly contains problems, solutions,\\nand test cases collected from the Codeforces platform3. In\\npractice, standard language modeling objectives are used for\\nthe pre-training on GitHub code data, while GOLD [111] with\\ntempering [112] serve as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='AlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\n2.4 CodeT5+ [113]: CodeT5+ is based on\\nCodeT5 [114], with shallow encoder and deep decoder,\\ntrained in multiple stages initially unimodal data (code) and\\nlater bimodal data (text-code pairs). Each training stage has\\n2https://github.com/features/copilot\\n3https://codeforces.com/', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 9}), Document(page_content='JOURNAL OF L ATEX 11\\ndifferent training objectives and activates different model\\nblocks encoder, decoder, or both according to the task. The\\nunimodal pre-training includes span denoising and CLM\\nobjectives, whereas bimodal pre-training objectives contain\\ncontrastive learning, matching, and CLM for text-code pairs.\\nCodeT5+ adds special tokens with the text to enable task\\nmodes, for example, [CLS]for contrastive loss, [Match ]for\\ntext-code matching, etc.\\n2.5 StarCoder [115]: A decoder-only model with San-\\ntaCoder architecture, employing Flash attention to scale up\\nthe context length to 8k. The StarCoder trains an encoder to\\nfilter names, emails, and other personal data from the training\\ndata. Its fine-tuned variant outperforms PaLM, LLaMA, and\\nLAMDA on HumanEval and MBPP benchmarks.\\n3. Scientific Knowledge:\\n3.1 Galactica [116]: A large curated corpus of human\\nscientific knowledge with 48 million papers, textbooks, lecture\\nnotes, millions of compounds and proteins, scientific websites,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='scientific knowledge with 48 million papers, textbooks, lecture\\nnotes, millions of compounds and proteins, scientific websites,\\nencyclopedias, and more are trained using metaseq library3,\\nwhich is built on PyTorch and fairscale [117]. The model\\nwraps reasoning datasets with < work > token to provide\\nstep-by-step reasoning context to the model, which has been\\nshown to improve the performance on reasoning tasks.\\n4. Dialog:\\n4.1 LaMDA [118]: A family of Transformer-based neu-\\nral language models for dialog ranging from 2B to 137B\\nparameters. The model architecture of LaMDA follows a\\ndecoder-only Transformer [44] language model. LaMDA is\\npre-trained on public dialog data, public dialog utterances,\\nand public web documents. Particularly, more than 90% of\\nthe pre-training data is in English. Particularly, LaMDA aims\\nto produce responses that exhibit high levels of quality, safety,\\nand groundedness. To achieve this, discriminative and gener-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='to produce responses that exhibit high levels of quality, safety,\\nand groundedness. To achieve this, discriminative and gener-\\native fine-tuning techniques are incorporated to enhance the\\nmodel’s safety and quality aspects. As a result, the LaMDA\\nmodels can be utilized as a general language model performing\\nvarious tasks.\\n4.2 Sparrow [119]: An information-seeking dialogue\\nagent is trained to gain more helpfulness and correctness\\nwith less harm. Two additions are proposed to help human\\nraters judge agent behavior: the first is the specific natural\\nlanguage rules that need raters to rate separately, and the\\nsecond is to make the agent show proof from sources that\\nsupport factual claims when collecting opinions about the\\nmodel’s statements. The architecture of the Sparrow models\\nis based on Dialogue Prompted Chinchila 70B [100]. Human\\ndata is collected for rule violations and per-turn response\\npreferences, which mainly aims to train preference reward', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='data is collected for rule violations and per-turn response\\npreferences, which mainly aims to train preference reward\\nmodels (preference RMs) and a rule reward model (rule RM).\\nIn practice, reinforcement learning with advantage actor-critic\\n(A2C) [120] is used to train the initialized Chinchilla model;\\nthe rule RM estimated rule violation rate and the prefer-\\nence RMs estimated per-turn response preferences are jointly\\noptimized. Given experimental data, Sparrow’s evidence can\\nsupport the sampled response for factual questions 78% of\\nthe time. Moreover, Sparrow is highly resistant to human\\nadversarial probing since it only violates the defined rules 8%\\nof the time when probed.\\nFig. 9: This example illustrates the PanGu-Parchitecture, as\\ndepicted in the image sourced from [106].\\n5. Finance:\\n5.1 BloombergGPT [121]: A non-causal decoder model\\ntrained using both financial (\"FINPILE\" from the Bloomberg\\narchive) and general-purpose datasets. The model’s architec-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='trained using both financial (\"FINPILE\" from the Bloomberg\\narchive) and general-purpose datasets. The model’s architec-\\nture is similar to the BLOOM [9] and OPT [10]. It allocates\\n50B parameters to different blocks of the model using the\\napproach [122]. For effective training, BloombergGPT packs\\ndocuments together with <|endoftext |>to use maximum\\nsequence length, use warmup batch size starting from 1024 to\\n2048, and manually reduces the learning rate multiple times\\nduring the training.\\n5.2 Xuan Yuan 2.0 [123]: A Chinese financial chat\\nmodel with BLOOM’s [9] architecture trained on a combina-\\ntion of general purpose, financial, general purpose instructions,\\nand financial institutions datasets. Xuan Yuan 2.0 combined\\nthe pre-training and fine-tuning stages to avoid catastrophic\\nforgetting.\\nB. Fine-Tuned Models\\n1. Instruction-Tuning with Manually Created Datasets:\\n1.1 T0 [22]: Similar to Flan [25], T0 fine-tunes the LM-\\nadapted T5 model [27] with multi-task instruction prompts.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='1.1 T0 [22]: Similar to Flan [25], T0 fine-tunes the LM-\\nadapted T5 model [27] with multi-task instruction prompts.\\nTo train the model, T0 designs various prompt templates to\\nconvert different datasets into prompts. The model trained with\\nexplicit multi-task prompting improves zero-shot performance,\\noutperforming models 6 to 16 times larger in size.\\n1.2 WebGPT [126]: A set of fine-tuned GPT-3 [8] mod-\\nels that can answer long-form questions in a text-based web-\\nbrowsing environment. During browsing, the models must\\ncollect references to support their answers in order to exploit\\neasier human evaluation. In addition to the Q&A data in the\\nELI5 dataset [129], two more types of data, demonstrations\\nof humans and comparisons between two model-generated\\nanswers, are collected for training. Using human labelers’\\nfeedback on whether the retrieved information is useful to\\nanswer the given inputs, the data is tested with four usages:\\nbehavior cloning, reward modeling, reinforcement learning,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='answer the given inputs, the data is tested with four usages:\\nbehavior cloning, reward modeling, reinforcement learning,\\nand rejection sampling. Practically, the combination of be-\\nhavior cloning and rejection sampling contributes to the best-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 10}), Document(page_content='JOURNAL OF L ATEX 12\\nTABLE I: Noteworthy findings and insights from pre-trained Large Language Model.\\nModels Findings & Insights\\nT5•Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n•Fine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers\\nGPT-3•Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-learners\\nmT5•Large multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi-\\nlingual models perform worse\\nPanGu- α•LLMs are good at a few shot capabilities\\nCPM-2•Prompt fine-tuning requires updating very few parameters while achieving performance comparable to full model fine-tuning\\n•Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n•Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long\\nsequences', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='•Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long\\nsequences\\n•In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator (aggregate information with\\nthe input text) for the model\\nCodex•This LLM focuses on code evaluations and introduces a novel way of selecting the best code samples.\\n•The results indicate it is possible to accurately select code samples using heuristic ranking in lieu of a detailed evaluation of\\neach sample, which may not be feasible or feasible in some situations.\\nERNIE 3.0•ERNIE 3.0 shows that a modular LLM architecture with a universal representation module and task-specific representation\\nmodule helps in finetuning phase.\\n•Optimizing the parameters of a task-specific representation network during the fine-tuning phase is an efficient way to take\\nadvantage of the powerful pretrained model.\\nJurassic-1•The performance of an LLM is highly related to the network size.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='advantage of the powerful pretrained model.\\nJurassic-1•The performance of an LLM is highly related to the network size.\\n•To improve runtime performance, more operations can be performed in parallel (width) rather than sequentially (depth).\\n•To efficiently represent and fit more text in the same context length, the model uses a larger vocabulary to train a SentencePiece\\ntokenizer without restricting it to word boundaries. This tokenizer improvement can further benefit few-shot learning tasks.\\nHyperCLOV A•By employing prompt-based tuning, the performances of models can be improved, often surpassing those of state-of-the-art\\nmodels when the backward gradients of inputs are accessible.\\nYuan 1.0•The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and\\nfew-shot learning.\\nGopher•Relative encodings enable models to be evaluated for longer sequences than those on which it was trained.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='few-shot learning.\\nGopher•Relative encodings enable models to be evaluated for longer sequences than those on which it was trained.\\nERNIE 3.0 Titan•This LLM builds on top of ERNIE 3.0 and add a self-supervised adversarial loss to distinguish whether a text is generated\\nor the original one.\\n•This distinction ability between real and generate text improves the LLM’s performance as compared to ERNIE 3.0.\\nGPT-NeoX-20B•Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded layers\\n•Initializing feed-forward output layers before residuals with scheme in [124] avoids activations from growing with increasing\\ndepth and width\\n•Training on Pile outperforms GPT-3 on five-shot\\nOPT•Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n•Model is prone to generate repetitive text and stuck in a loop\\nBLOOM•None', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='•Model is prone to generate repetitive text and stuck in a loop\\nBLOOM•None\\nGalactica•Galactica’s performance has continued to improve across validation set, in-domain, and out-of-domain benchmarks, even\\nwith multiple repetitions of the corpus, which is superior to existing research on LLMs.\\n•A working memory token approach can achieve strong performance over existing methods on mathematical MMLU and\\nMATH benchmarks. It sets a new state-of-the-art on several downstream tasks such as PubMedQA (77.6%) and MedMCQA\\ndev (52.9%).\\nGLaM•The feed-forward component of each Transformer layer can be replaced with a mixture-of-experts (MoE) module consisting\\nof a set of independent feed-forward networks ( i.e., the ‘experts’). By sparsely activating these experts, the model capacity\\ncan be maintained while much computation is saved.\\n•By leveraging sparsity, we can make significant strides toward developing high-quality NLP models while simultaneously', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='•By leveraging sparsity, we can make significant strides toward developing high-quality NLP models while simultaneously\\nreducing energy consumption. Consequently, MoE emerges as a robust candidate for future scaling endeavors.\\n•The model trained on filtered data shows consistently better performances on both NLG and NLU tasks, where the effect of\\nfiltering is more significant on the former tasks.\\n•Filtered pretraining corpora plays a crucial role in the generation capability of LLMs, especially for the downstream tasks.\\n•The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given a\\nfixed budget of computation, more experts contribute to better predictions.\\nLaMDA•The model can be fine-tuned to learn to call different external information resources and tools.\\nMT-NLG•None.\\nAlphaCode•For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed with a shallower encoder and\\na deeper decoder.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='a deeper decoder.\\n•To achieve better performances, it is necessary to employ strategies such as massively scaling up sampling, followed by the\\nfiltering and clustering of samples into a compact set.\\n•The utilization of novel sampling-efficient transformer architectures designed to facilitate large-scale sampling is crucial.\\n•Simplifying problem descriptions can effectively improve the model’s performance.\\nTable Continued on Next Page', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 11}), Document(page_content='JOURNAL OF L ATEX 13\\nModels Findings & Insights\\nChinchilla•The experiments that culminated in the development of Chinchilla determined that for optimal computation during training,\\nthe model size and the number of training tokens should be scaled proportionately: for each doubling of the model size, the\\nnumber of training tokens should be doubled as well.\\nPaLM•English-centric models produce better translations when translating to English as compared to non-English\\n•Generalized models can have equivalent performance for language translation to specialized small models\\n•Larger models have a higher percentage of training data memorization\\n•Performance has not yet saturated even at 540B scale, which means larger models are likely to perform better\\nAlexaTM•Compared to commonly used Decoder-only Transformer models, seq2seq architecture is more suitable for training generative\\nLLMs given stronger bidirectional attention to the context.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='LLMs given stronger bidirectional attention to the context.\\n•An extra Causal Language Modeling (CLM) task can be added to benefit the model with a more efficient in-context learning,\\nespecially for few-shot learning tasks.\\n•The key to training powerful seq2seq-based LLMs lies in mixed pre-training, rather than additional multitask training.\\n•Placing layernorms at the beginning of each transformer layer can improve the training stability of large models.\\nSparrow•Reinforcement learning from multi-objective human feedback can be leveraged to train the models, in order to maximize\\npreference rates and minimize rule violations.\\n•The judgments of labelers and the alignments with defined rules can help the model generate better responses.\\n•Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.\\n•Constructing useful and reliable agents from generative models requires a combination of width and depth: the width aspect', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='•Constructing useful and reliable agents from generative models requires a combination of width and depth: the width aspect\\nenables addressing the complexities of goals and topics; while the depth aspect ensures accurate handling of them.\\n•The combination of reinforcement learning (RL) with reranking yields optimal performance in terms of preference win rates\\nand resilience against adversarial probing.\\nU-PaLM•Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n•Training with a mixture of denoisers improves the infilling ability and open-ended text generation diversity\\nUL2•Mode switching training enables better performance on downstream tasks\\n•CoT prompting outperforms standard prompting for UL2\\nGLM-130B•Pre-training data with a small proportion of multi-task instruction data improves the overall model performance\\nCodeGen•Multi-step prompting for code synthesis leads to a better user intent understanding and code generation', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='CodeGen•Multi-step prompting for code synthesis leads to a better user intent understanding and code generation\\nLLaMA•LLaMA is open-source and can be fine-tuned or continually pre-trained to develop new models or instruction-based tools.\\n•A few optimizations are proposed to improve the training efficiency of LLaMA, such as efficient implementation of multi-head\\nself-attention and a reduced amount of activations during back-propagation.\\n•Training exclusively on public data can also achieve state-of-the-art performance.\\n•A constant performance improvement is gained when scaling the model.\\n•Smaller models can also realize good performances using more training data and time.\\nPanGu- Σ•Sparse models provide the benefits of large models at a lower computation cost\\n•Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for continual learning\\n•Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='•Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while\\nmaintaining a performance similar to the original\\nBloombergGPT•Pre-training with general-purpose and task-specific data improves task performance without hurting other model capabilities\\nXuanYuan 2.0•Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+•Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n•Multiple training objectives like span corruption, Causal LM, matching, etc complement each other for better performance\\nStarCoder•HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nperforming model containing 175B parameters. To evaluate\\nthe performance of the WebGPT model, the model’s answers\\nare compared with the human demonstrators’ written ones, the\\nhighest-voted answers in ELI5 [129], and the adversarial short-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='are compared with the human demonstrators’ written ones, the\\nhighest-voted answers in ELI5 [129], and the adversarial short-\\nform questions and answers in the TruthfulQA [130] dataset.\\n1.3 Tk-INSTRUCT [26]: Tk-INSTRUCT instruction\\nfine-tunes T5 model on self-proposed SUPER-\\nNATURALINSTRUCTIONS dataset of 1616 different\\nNLP tasks. To analyze the generalization capabilities of\\nTk-INSTRUCT, the model is evaluated on unseen tasks with\\na few examples as ground-truth in-context instructions. The\\nmodel outperforms GPT-3 and Instruct-GPT by large margins,\\nalthough it is smaller in size 11B, opposite to 175B.\\n1.4 mT0 and BLOOMZ [23]: This method fine-tunes\\nBLOOM and T0 using a multilingual multi-task prompt\\ndataset. It creates a new dataset named xP3, adding 46 different\\nlanguage datasets with new tasks not present previously in P3.Training with this dataset improves zero-shot generalization\\nfor both English and non-English. This also leads to better\\ngeneralization on held-out tasks.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='for both English and non-English. This also leads to better\\ngeneralization on held-out tasks.\\n1.5 OPT-IML [24]: An instruction-tuned OPT model,\\ntrained on instruction meta-learning benchmark of 2000 NLP\\ntasks that is a combination of 8 meta-datasets including,\\nSuper-NaturalInstructions, PromptSource, FLAN, and others\\nas given in Table VIII. For computational efficiency, OPT-\\nIML utilizes the maximum sequence length of 2048 tokens by\\npacking multiple instances together, separated by the < eos >\\ntoken during training. It employs a masking mechanism to\\nseparate instances in a sequence to avoid attending tokens from\\ndifferent instances. Overall, OPT-IML outperforms baseline\\nmodel OPT with instruction-finetuning on zero and few-shot\\ngeneralization abilities.\\n1.6 Flan [25]: Fine-tuning language models (Flan), fine-\\ntunes T5, PaLM, and UPaLM with 1836 instruction tasks', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 12}), Document(page_content='JOURNAL OF L ATEX 14\\nTABLE II: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels Findings & Insights\\nT0•Multi-task prompting enables zero-shot generalization and outperforms baselines\\n•Even a single prompt per dataset task is enough to improve performance\\nWebGPT•The answer quality of LLMs can be further improved with human feedback.\\n•To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering\\nquestions regarding the usefulness of the retrieved documents.\\n•Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and\\nsynthesis via imitation learning and reinforcement learning.\\n•Generating answers with references can make labelers easily judge the factual accuracy of answers.\\nTk-INSTRUCT•Instruction tuning leads to a stronger generalization of unseen tasks', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='Tk-INSTRUCT•Instruction tuning leads to a stronger generalization of unseen tasks\\n•More tasks improve generalization whereas only increasing task instances does not help\\n•Supervised trained models are better than generalized models\\n•Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ•Instruction tuning enables zero-shot generalization to the tasks never seen before\\n•Multi-lingual training leads to even better zero-shot generalization for both English and non-English\\n•Training on machine-translated prompts improves performance for held-out tasks with non-English prompts\\n•English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language\\ntasks\\nOPT-IML•Task size sampling to create a batch with most of the task examples is important for better performance\\n•Only example proportional sampling is not enough, training datasets/benchmarks should also be proportional for better', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='•Only example proportional sampling is not enough, training datasets/benchmarks should also be proportional for better\\ngeneralization/performance\\n•Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised\\ntasks have no effect\\n•Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n•Only 1% reasoning data improves the performance, adding more deteriorates performance\\n•Adding dialogue data makes the performance worse\\nFlan•Finetuning with CoT improves performance on held-out tasks\\n•Fine-tuning along with CoT data improves reasoning abilities\\n•CoT tuning improves zero-shot reasoning\\n•Performance improves with more tasks\\n•Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n•Improving the model’s performance with instruction tuning is compute efficient\\n•Multitask prompting enables zero-shot generalization abilities in LLM', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='•Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder•Fine-tuning with re-written instruction-tuning data into a complex set improves the performance significantly\\nFig. 10: An example image shows an instance of the Flan\\ntraining paradigm, taken from [25].\\ntaken from Muffin (80 tasks), T0-SF (193 tasks), NIV2 (1554\\ntasks), and CoT (taken from nine datasets), as shown in\\nFigure 10. Instruction fine-tuning improves the model perfor-\\nmance significantly with minimal computing, only 0.2% of the\\ntotal pre-training compute in the case of PaLM 540B. Flan\\nalso suggests that adding more instruction fine-tuning tasks\\nwith CoT reasoning data will likely improve the performance\\nfurther.\\n2. Instruction-Tuning with LLMs Generated Datasets:\\nGenerating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are oftenwritten by humans, smaller in size, and less diverse. To\\novercome this, self-instruct [131] proposed an approach to', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='overcome this, self-instruct [131] proposed an approach to\\nprompt available LLMs to generate instruction-tuning datasets.\\nSelf-instruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [26] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nGPT-3 [8]. Contrary to this, Dynosaur [132] uses the meta-\\ndata of datasets on Huggingface to prompt LLMs to generate\\nmultiple task instruction-tuning datasets.\\nLLaMA Tuned Various models in literature instruction-tune\\nLLaMA [133] with GPT-3 [8] or GPT-4 [134] generated\\ndatasets. Among these, Alpaca, Vicuna, and LLaMA-GPT-4\\nare a few general-purpose fine-tuned models, where Alpaca\\nis trained on 52k samples from text-davinci-003, Vicuna on\\n70k samples from ShareGPT.com, and LLaMA-GPT-4 by re-\\ncreating Alpaca instructions from GPT-4. Goat [135] fine-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='70k samples from ShareGPT.com, and LLaMA-GPT-4 by re-\\ncreating Alpaca instructions from GPT-4. Goat [135] fine-\\ntunes LLaMA for arithmetic tasks (1 million samples) by\\ngenerating data from ChatGPT and outperforms GPT-4, PaLM,\\nBLOOM, OPT, etc, attributing its success to the LLaMA’s\\nconsistent tokenization of numbers. HuaTuo [136] is a medical\\nknowledge model, fine-tuned with a generated QA dataset of\\n8k instructions.\\nComplex Instructions Evol-Instruct [137], [128] prompts\\nLLMs to convert given instructions into a more complex', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 13}), Document(page_content='JOURNAL OF L ATEX 15\\nTABLE III: Summary of pre-trained LLMs. Only the LLMs discussed individually in the previous sections are summarized.\\n“Data/Tokens” is the model’s pre-training data which is either the number of tokens or data size. “Data Cleaning” indicates\\nwhether the data cleaning is performed or not. This includes heuristics (Heur), deduplication (Dedup), quality filtering (QF),\\nand privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs hourly rate with the\\nnumber of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting\\na discounted rate, re-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed\\ntraining using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), model parallelism (M), optimizer parallelism', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"training using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), model parallelism (M), optimizer parallelism\\n(OP), and rematerialization (R), where for “Library” column, “DS” is a short form for Deep Speed. In column “Commercial\\nUse”, we assumed a model is for non-commercial purposes if its license is not available.ModelsPublication\\nVenueLicense\\nTypeModel\\nCreators PurposeNo. of\\nParamsCommercial\\nUseSteps\\nTrainedData/\\nTokensData\\nCleaningNo. of\\nProcessing UnitsProcessing\\nUnit TypeTraining\\nTimeCalculated\\nTrain. CostTraining\\nParallelism Library\\nT5 [11] JMLR'20 Apache-2.0 Google General 11B ✓ 1M 1T Heur+Dedup 1024 TPU v3 - - D+M Mesh TensorFlow\\nGPT-3 [8] NeurIPS'20 - OpenAI General 175B × - 300B Dedup+QF - V100 - - M -\\nmT5 [12] NAACL'21 Apache-2.0 Google General 13B ✓ 1M 1T - - - - - - -\\nPanGu- α[84] arXiv'21 Apache-2.0 Huawei General 200B ✓ 260k 1.1TB Heur+Dedup 2048 Ascend 910 - - D+OP+P+O+R MindSpore\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"PanGu- α[84] arXiv'21 Apache-2.0 Huawei General 200B ✓ 260k 1.1TB Heur+Dedup 2048 Ascend 910 - - D+OP+P+O+R MindSpore\\nCPM-2 [13] AI Open'21 MIT Tsinghua General 198B ✓ 1M 2.6TB Dedup - - - - D+M JAXFormer\\nCodex [108] arXiv'21 - OpenAI Coding 12B × - 100B Heur - - - - - -\\nERNIE 3.0 [86] arXiv'21 - Baidu General 10B × 120k∗375B Heur+Dedup 384 V100 - - M∗PaddlePaddle\\nJurassic-1 [88] White-Paper'21 Apache-2.0 AI21 General 178B ✓ - 300B - 800 GPU - - D+M+P Megatron+DS\\nHyperCLOV A [90] EMNLP'21 - Naver General 82B × - 300B Clf+Dedup+PF 1024 A100 321h 1.32 Mil M Megatron\\nYuan 1.0 [91] arXiv'21 Apache-2.0 - General 245B ✓ 26k∗180B Heur+Clf+Dedup 2128 GPU - - D+T+P -\\nGopher [92] arXiv'21 - Google General 280B × - 300B QF+Dedup 4096 TPU v3 920h 13.19 Mil D+M JAX+Haiku\\nERNIE 3.0 Titan [93] arXiv'21 - Baidu General 260B × - 300B Heur+Dedup - Ascend 910 - - D+M+P+D* PaddlePaddle\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"ERNIE 3.0 Titan [93] arXiv'21 - Baidu General 260B × - 300B Heur+Dedup - Ascend 910 - - D+M+P+D* PaddlePaddle\\nGPT-NeoX-20B [125] BigScience'22 Apache-2.0 EleutherAI General 20B ✓ 150k 825GB None 96 40G A100 - - M Megatron+DS+PyTorch\\nOPT [10] arXiv'22 MIT Meta General 175B ✓ 150k 180B Dedup 992 80G A100 - - D+T Megatron\\nBLOOM [9] arXiv'22 RAIL-1.0 BigScience General 176B ✓ - 366B Dedup+PR 384 80G A100 2520h 3.87 Mil D+T+P Megatron+DS\\nGalactica [116] arXiv'22 Apache-2.0 Meta Science 120B × 225k 106B Dedup 128 80GB A100 - - - Metaseq\\nGLaM [97] ICML'22 - Google General 1.2T × 600k∗600B Clf 1024 TPU v4 - - M GSPMD\\nLaMDA [118] arXiv'22 - Google Dialog 137B × 3M 2.81T Filtered 1024 TPU v3 1384h 4.96 Mil D+M Lingvo\\nMT-NLG [21] arXiv'22 Apache-v2.0 MS.+Nvidia General 530B × - 270B - 4480 80G A100 - - D+T+P Megatron+DS\\nAlphaCode [109] Science'22 Apache-v2.0 Google Coding 41B ✓ 205k 967B Heur+Dedup - TPU v4 - - M JAX+Haiku\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"AlphaCode [109] Science'22 Apache-v2.0 Google Coding 41B ✓ 205k 967B Heur+Dedup - TPU v4 - - M JAX+Haiku\\nChinchilla [100] arXiv'22 - Google General 70B × - 1.4T QF+Dedup - TPUv4 - - - JAX+Haiku\\nPaLM [14] arXiv'22 - Google General 540B × 255k 780B Heur 6144 TPU v4 - - D+M JAX+T5X\\nAlexaTM [101] arXiv'22 Apache v2.0 Amazon General 20B × 500k 1.1T Filtered 128 A100 2880h 1.47 Mil M DS\\nSparrow [119] arXiv'22 - Google Dialog 70B × - - - 64 TPU v3 - - M -\\nU-PaLM [20] arXiv'22 - Google General 540B × 20k - - 512 TPU v4 120h 0.25 Mil - -\\nUL2 [15] ICLR'23 Apache-2.0 Google General 20B ✓ 2M 1T - 512 TPU v4 - - M JAX+T5X\\nGLM [102] ICLR'23 Apache-2.0 Multiple General 130B × - 400B - 768 40G A100 1440h 3.37 Mil M -\\nCodeGen [107] ICLR'23 Apache-2.0 Salesforce Coding 16B ✓ 650k 577B Heur+Dedup - TPU v4 - - D+M JAXFormer\\nLLaMA [104] arXiv'23 - Meta General 65B × 350k 1.4T Clf+Heur+Dedup 2048 80G A100 504h 4.12 Mil D+M xFormers\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"LLaMA [104] arXiv'23 - Meta General 65B × 350k 1.4T Clf+Heur+Dedup 2048 80G A100 504h 4.12 Mil D+M xFormers\\nPanGuΣ[106] arXiv'23 - Huawei General 1.085T × - 329B - 512 Ascend 910 2400h - D+OP+P+O+R MindSpore\\nBloombergGPT [121] arXiv23 - Bloomberg Finance 50B × 139k 569B Dedup 512 40G A100 1272h 1.97 Mil M PyTorch\\nXuan Yuan 2.0 [123] arXiv23 RAIL-1.0 Du Xiaoman Finance 176B ✓ - 366B Filtered 80GB A100 - - P DS\\nCodeT5+ [113] arXiv'23 BSD-3 Salesforce Coding 16B ✓ 110k 51.5B Dedup 16 40G A100 - - - DS\\nStarCoder [115] arXiv'23 OpenRAIL-M BigCode Coding 15.5B ✓ 250k 1T Dedup+QF+PF 512 80G A100 624h 1.28 Mil D+T+P Megatron-LMTABLE IV: Summary of instruction tuned LLMs. All abbreviations are the same as Table III. Entries in “Data/Tokens” starting\\nwith “S-” represents the number of training samples.\\nModelsPublication\\nVenueLicense\\nTypeModel\\nCreators PurposeNo. of\\nParamsCommercial\\nUsePre-trained\\nModelsSteps\\nTrainedData/\\nTokensNo. of\\nProcessing UnitsProcessing\\nUnit TypeTrain.\\nTimeCalculated\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content=\"ParamsCommercial\\nUsePre-trained\\nModelsSteps\\nTrainedData/\\nTokensNo. of\\nProcessing UnitsProcessing\\nUnit TypeTrain.\\nTimeCalculated\\nTrain. CostTrain.\\nParallelism Library\\nWebGPT [126] arXiv'21 - OpenAI General 175B × GPT-3 - - - - - - - -\\nT0 [22] ICLR'22 Apache-2.0 BigScience General 11B ✓ T5 - 250B 512 TPU v3 270h 0.48 Mil - -\\nTk-Instruct [26] EMNLP'22 MIT AI2+ General 11B ✓ T5 1000 - 256 TPU v3 4h 0.0036 Mil - Google T5\\nOPT-IML [24] arXiv'22 - Meta General 175B × OPT 8k 2B 128 40G A100 - - D+T Megatron\\nFlan-U-PaLM [25] ICLR'22 Apache-2.0 Google General 540B ✓ U-PaLM 30k - 512 TPU v4 - - - JAX+T5X\\nmT0 [127] ACL'23 Apache-2.0 HuggingFace+ General 13B ✓ mT5 - - - - - - - -\\nWizardCoder [128] arXiv'23 Apache-2.0 HK Bapt. Coding 15B × StarCoder 200 S-78k - - - - - -\\nset. The instructions are iteratively evolved with re-writing\\ninstructions in complex wording and creating new instruc-\\ntions. With this style of automated instruction generation,\", metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content='instructions in complex wording and creating new instruc-\\ntions. With this style of automated instruction generation,\\nWizardLM [137] (fine-tuned LLaMA on 250k instructions),\\noutperforms Vicuna and Alpaca, and WizardCoder [128] (fine-\\ntuned StarCoder) beats Claude-Plus, Bard, and others.\\nSelf-Alignment Aligning LLMs with human feedback is slow\\nand costly. The literature suggests a semi-automated process to\\nalign LLMs by prompting LLMs to generate helpful, honest,\\nand ethical responses to the queries, and fine-tuning using\\nthe newly created dataset. Constitutional AI [138] replaces\\nhuman feedback in RLHF with AI, naming RF from AI\\nfeedback (RLAIF). Self-Align [139] prompts the LLM with\\nICL examples, instructing LLM about what the response\\nshould contain to be considered useful and ethical. The same\\nLLM is later fine-tuned with the new dataset.C. Robotics\\nLLMs have been rapidly adopted across various domains in\\nthe scientific community due to their multipurpose capabili-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content='LLMs have been rapidly adopted across various domains in\\nthe scientific community due to their multipurpose capabili-\\nties [33]. In robotics research, the LLMs have very promising\\napplications as well, such as enhancing human-robot inter-\\naction [140], [141], [142], [143], task planning [144], [145],\\n[146], navigation [147], [148], and learning [149], [150].\\nThey can enable robots to understand and generate natural\\nlanguage, aiding in instruction following, data annotation, and\\ncollaborative problem-solving. They can facilitate continuous\\nlearning by allowing robots to access and integrate information\\nfrom a wide range of sources. This can help robots acquire new\\nskills, adapt to changes, and refine their performance based on\\nreal-time data.\\nLLMs have also started assisting in simulating environments\\nfor testing and offer potential for innovative research in\\nrobotics, despite challenges like bias mitigation and integration', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 14}), Document(page_content='JOURNAL OF L ATEX 16\\ncomplexity. The work in [151] focuses on personalizing robot\\nhousehold cleanup tasks. By combining language-based plan-\\nning and perception with LLMs, such that having users provide\\nobject placement examples, which the LLM summarizes to\\ngenerate generalized preferences, they show that robots can\\ngeneralize user preferences from a few examples. An embod-\\nied LLM is introduced in [152], which employs a Transformer-\\nbased language model where sensor inputs are embedded\\nalongside language tokens, enabling joint processing to en-\\nhance decision-making in real-world scenarios. The model\\nis trained end-to-end for various embodied tasks, achieving\\npositive transfer from diverse training across language and\\nvision domains. LLMs have also been explored as zero-shot\\nhuman models for enhancing human-robot interaction.\\nThe study in [140] demonstrates that LLMs, trained on vast\\ntext data, can serve as effective human models for certain', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='The study in [140] demonstrates that LLMs, trained on vast\\ntext data, can serve as effective human models for certain\\nHRI tasks, achieving predictive performance comparable to\\nspecialized machine-learning models. However, limitations\\nwere identified, such as sensitivity to prompts and difficulties\\nwith spatial/numerical reasoning. In another study [153], the\\nauthors enable LLMs to reason over sources of natural lan-\\nguage feedback, forming an “inner monologue” that enhances\\ntheir ability to process and plan actions in robotic control\\nscenarios. They combine LLMs with various forms of textual\\nfeedback, allowing the LLMs to incorporate conclusions into\\ntheir decision-making process for improving the execution of\\nuser instructions in different domains, including simulated and\\nreal-world robotic tasks involving tabletop rearrangement and\\nmobile manipulation. All of these studies employ LLMs as the\\ncore mechanism for assimilating everyday intuitive knowledge', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='mobile manipulation. All of these studies employ LLMs as the\\ncore mechanism for assimilating everyday intuitive knowledge\\ninto the functionality of robotic systems.\\nIV. F INDINGS & INSIGHTS\\nTraining a billion-scale model is difficult as compared to\\na smaller model. LLMs are prone to various instabilities\\nduring training, such as hardware failure and instability. Other\\nthan this, LLMs exhibit different behaviors such as emergent\\nabilities, improved zero-shot, few-shot, and reasoning abilities.\\nResearchers report these essential details in their papers for\\nresults reproduction and field progress. We identify critical\\ninformation in Table I and II such as architecture, training\\nstrategies, and pipelines that improve LLMs’ performance\\nor other abilities acquired because of changes mentioned in\\nsection III.\\nV. M ODEL CONFIGURATIONS\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='We provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,\\nparallelism, etc in Table III and Table IV. Architecture details\\nof pre-trained LLMs are available in Table V. Providing\\nthese details for instruction-tuned models is unnecessary\\nbecause it fine-tunes pre-trained models for instruction\\ndatasets. Hence, architectural details are the same as the\\nbaselines. Moreover, optimization settings for various LLMs\\nare available in Table VI and Table VII. We do not include\\ndetails on precision, warmup, and weight decay in Table VII.Neither of these details are important as others to mention\\nfor instruction-tuned models nor provided by the papers.\\nVI. D ATASETS AND EVALUATION\\nLLMs are known to require a huge amount of data for\\ntraining. Hence, datasets for training and benchmarking these\\nmodels are currently a topic of key importance. In Fig. 11,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='training. Hence, datasets for training and benchmarking these\\nmodels are currently a topic of key importance. In Fig. 11,\\nwe show the distribution of datasets currently available for\\nbenchmarking language models for a variety of natural lan-\\nguage processing tasks. It is noteworthy that this distribution is\\nrestricted to only the tasks for which at least 20 datasets have\\nalready been proposed in the literature. LLMs can directly\\nbenefit from these dataset for training and evaluation. In\\ngeneral, the performance of LLMs greatly depends on the\\ntraining dataset. A model trained on a good-quality data is\\nlikely to perform better on evaluation benchmarks. Specific\\ntraining and evaluation datasets used by LLMs are summarized\\nin Table VIII and IX.\\nA. Evaluation Tasks\\nThe evaluation of LLMs is a critical step in gauging their\\nproficiency and identifying their limitations. This process\\nprovides a measure of the model’s ability to comprehend, gen-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='proficiency and identifying their limitations. This process\\nprovides a measure of the model’s ability to comprehend, gen-\\nerate, and interact with human language across a spectrum of\\ntasks. For Natural Language Understanding (NLU), these tasks\\nencompass sentiment analysis, natural language inference,\\nsemantic understanding, closed book question answering, and\\nreading comprehension, among others. In contrast, Natural\\nLanguage Generation (NLG) tasks include text summarization,\\ntranslation, and more. These tasks form part of established\\nbenchmarks that facilitate the comparison of different models.\\nB. Evaluation Datasets\\nThe role of specific datasets, particularly those commonly\\nused, is fundamental in the evaluation of Large Language\\nModels. These datasets, each with its unique design and set of\\nchallenges, serve as the basis for assessing the capabilities of\\nLLMs. They offer a comprehensive measure of performance\\nacross a variety of tasks, providing insights into the models’', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='LLMs. They offer a comprehensive measure of performance\\nacross a variety of tasks, providing insights into the models’\\nproficiency. In the following discussion, we provide a concise\\noverview of a selection of these key datasets. While the\\nTables VIII and IX include a larger set of datasets, we focus on\\nthe most commonly used ones in the evaluation of LLMs. Each\\ndataset description encapsulates the core aspects it evaluates in\\nan LLM, offering a snapshot of the model’s potential strengths\\nand limitations.\\n1. HellaSwag [154]: A dataset that challenges models to\\npick the best ending to a context uses Adversarial Filtering\\nto create a ‘Goldilocks’ zone of complexity, where generated\\ntext is absurd to humans but often misclassified by models.\\n2. PIQA [155]: A dataset that probes the physical knowl-\\nedge of models, aiming to understand how well they are\\nlearning about the real world.\\n3. TriviaQA [156]: A dataset that tests models on reading', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='edge of models, aiming to understand how well they are\\nlearning about the real world.\\n3. TriviaQA [156]: A dataset that tests models on reading\\ncomprehension and open domain question answering (QA)\\ntasks, with a focus on Information Retrieval (IR)-style QA.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 15}), Document(page_content='JOURNAL OF L ATEX 17\\nTABLE V: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the\\nnumber of attention heads, “HS” is the size of hidden statesModels TypeTraining\\nObjectiveVocab Tokenizer Norm PE Activation Bias nL nH HS\\nT5 (11B) Enc-Dec Span Corruption 32k SentencePiece Pre-RMS Relative ReLU × 24 128 1024\\nGPT3 (175B) Causal-Dec Next Token - - Layer Learned GeLU ✓ 96 96 12288\\nmT5 (13B) Enc-Dec Span Corruption 250k SentencePiece Pre-RMS Relative ReLU - - - -\\nPanGu- α(200B) Causal-Dec Next Token 40k BPE Layer - - - 64 128 16384\\nCPM-2 (198B) Enc-Dec Span Corruption 250k SentencePiece Pre-RMS Relative ReLU - 24 64 -\\nCodex (12B) Causal-Dec Next Token - BPE+ Pre-Layer Learned GeLU - 96 96 12288\\nERNIE 3.0 (10B) Causal-Dec Next Token - WordPiece Post-Layer Relative GeLU - 48 64 4096\\nJurassic-1 (178B) Causal-Dec Next Token 256k SentencePiece∗Pre-Layer Learned GeLU ✓ 76 96 13824', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='Jurassic-1 (178B) Causal-Dec Next Token 256k SentencePiece∗Pre-Layer Learned GeLU ✓ 76 96 13824\\nHyperCLOV A (82B) Causal-Dec Next Token - BPE* Pre-Layer Learned GeLU - 64 80 10240\\nYuan 1.0 (245B) Causal-Dec Next Token - - - - - - 76 -16384\\nGopher (280B) Causal-Dec Next Token 32k SentencePiece Pre-RMS Relative GeLU ✓ 80 128 16384\\nERNIE 3.0 Titan (260B) Causal-Dec Next Token - WordPiece Post-Layer Relative GeLU - 48 192 12288\\nGPT-NeoX-20B Causal-Dec Next Token 50k BPE Layer Rotary GeLU ✓ 44 64 -\\nOPT (175B) Causal-Dec Next Token - BPE - - ReLU ✓ 96 96 -\\nBLOOM (176B) Causal-Dec Next Token 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\\nGalactica (120B) Causal-Dec Next Token 50k BPE+custom Layer Learned GeLU × 96 80 10240\\nGLaM (1.2T) MoE-Dec Next Token 256k SentencePiece Layer Relative GeLU ✓ 64 128 32768\\nLaMDA (137B) Causal-Dec Next Token 32k BPE Layer Relative GeGLU - 64 128 8192\\nMT-NLG (530B) Causal-Dec Next Token 50k BPE Pre-Layer Learned GeLU ✓ 105 128 20480', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='MT-NLG (530B) Causal-Dec Next Token 50k BPE Pre-Layer Learned GeLU ✓ 105 128 20480\\nAlphaCode (41B) Enc-Dec Next Token 8k SentencePiece - - - - 64 128 6144\\nChinchilla (70B) Causal-Dec Next Token 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 80 64 8192\\nPaLM (540B) Causal-Dec Next Token 256k SentencePiece Layer RoPE SwiGLU × 118 48 18432\\nAlexaTM (20B) Enc-Dec Denoising 150k SentencePiece Pre-Layer Learned GeLU ✓ 78 32 4096\\nSparrow (70B) Causal-Dec Pref.&Rule RM 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 16∗64 8192\\nU-PaLM (540B) Non-Causal-Dec MoD 256k SentencePiece Layer RoPE SwiGLU × 118 48 18432\\nUL2 (20B) Enc-Dec MoD 32k SentencePiece - - - - 64 16 4096\\nGLM (130B) Non-Causal-Dec AR Blank Infilling 130k SentencePiece Deep RoPE GeGLU ✓ 70 96 12288\\nCodeGen (16B) Causal-Dec Next Token - BPE Layer RoPE - - 34 24 -\\nLLaMA (65B) Causal-Dec Next Token 32k BPE Pre-RMS RoPE SwiGLU - 80 64 8192\\nPanGu- Σ(1085B) Causal-Dec Next Token - BPE Fused Layer - FastGeLU - 40 40 5120', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='PanGu- Σ(1085B) Causal-Dec Next Token - BPE Fused Layer - FastGeLU - 40 40 5120\\nBloombergGPT (50B) Causal-Dec Next Token 131k Unigram Layer ALiBi GeLU ✓ 70 40 7680\\nXuan Yuan 2.0 (176B) Causal-Dec Next Token 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\\nCodeT5+ (16B) Enc-Dec SC+NT+Cont.+Match - Code-Specific - - - - - - -\\nStarCoder (15.5B) Causal-Dec FIM 49k BPE - Learned - - 40 48 6144\\nTABLE VI: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and\\ndropout are 0.1, 1.0, and 0.1, respectively, for most of the LLMs.Sequence LR Optimizers Precision Weight Grad\\nModels Batch Size Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout\\nT5 (11B) 211512 0.01 × inverse square root ✓ - - - ✓\\nGPT3 (175B) 32K - 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nmT5 (13B) 1024 1024 0.01 - inverse square root ✓ - - - ✓\\nPanGu- α(200B) - 1024 2e-5 - - - - ✓ - -\\nCPM-2 (198B) 1024 1024 0.001 - - ✓ - - - ✓\\nCodex (12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='PanGu- α(200B) - 1024 2e-5 - - - - ✓ - -\\nCPM-2 (198B) 1024 1024 0.001 - - ✓ - - - ✓\\nCodex (12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -\\nERNIE 3.0 (12B) 6144 512 1e-4 ✓ linear ✓ - ✓ - -\\nJurassic-1 (178B) 3.2M 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nHyperCLOV A (82B) 1024 - 6e-5 - cosine ✓ - ✓ - -\\nYuan 1.0 (245B) <10M 2048 1.6e-4 ✓ cosine decay to 10% ✓ - ✓ - -\\nGopher (280B) 3M 2048 4e-5 ✓ cosine decay to 10% ✓ ✓ - ✓ -\\nERNIE 3.0 Titan (260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -\\nGPT-NeoX-20B 1538 2048 0.97e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nOPT (175B) 2M 2048 1.2e-4 - linear ✓ ✓ ✓ ✓ ✓\\nBLOOM (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nGalactica (120B) 2M 2048 7e-6 ✓ linear decay to 10% ✓ - ✓ ✓ ✓\\nGLaM (1.2T) 1M 1024 0.01 - inverse square root ✓ FP32 + ✓ - ✓ ×\\nLaMDA (137B) 256K - - - - - - - - - - - - -\\nMT-NLG (530B) 1920 2048 5e-5 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nAlphaCode (41B) 2048 1536+768 1e-4 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nChinchilla (70B) 1.5M 2048 1e-4 ✓ cosine decay to 10% ✓ ✓ - - -', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='AlphaCode (41B) 2048 1536+768 1e-4 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nChinchilla (70B) 1.5M 2048 1e-4 ✓ cosine decay to 10% ✓ ✓ - - -\\nPaLM (540B) 2048 2048 0.01 - inverse square root ✓ - ✓ ✓ ×\\nAlexaTM (20B) 2M 1024 1e-4 - linear decay to 5% ✓ ✓ ✓ - ✓\\nSparrow (70B) RM: 8+16, RL:16 - 2e-6 ✓ cosine decay to 10% ✓ ✓ ✓ - ✓ ×\\nU-PaLM (540B) 32 2048 1e-4 - cosine ✓ - - - -\\nUL2 (20B) 1024 1024 - - inverse square root - - - - - - ×\\nGLM (130B) 4224 2048 8e-5 ✓ cosine ✓ ✓ ✓ ✓ ✓\\nCodeGen (16B) 2M 2048 5e-5 ✓ cosine ✓ - ✓ ✓ -\\nLLaMA (65B) 4M 2048 1.5e-4 ✓ cosine decay to 10% ✓ - ✓ ✓ -\\nPanGu- Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -\\nBloombergGPT (50B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nXuan Yuan 2.0 (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nCodeT5+ (16B) 2048 1024 2e-4 - linear ✓ ✓ ✓ - -\\nStarCoder (15.5B) 512 8k 3e-4 ✓ cosine ✓ ✓ ✓ - -', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 16}), Document(page_content='JOURNAL OF L ATEX 18\\nTABLE VII: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are\\nthe same as the pre-trained models, while no model is using weight decay for instruction tuning.\\nSequence Optimizer Grad\\nModels Batch Size Length LR Warmup LR_Decay AdaFactor Adam Clip Dropout\\nWebGPT (175B) BC:512, RM:32 -6e-5 - - ✓ - -\\nT0 (11B) 1024 1280 1e-3 - - ✓ - ✓\\nTk-Instruct (11B) 1024 -1e-5 - constant - - - -\\nOPT-IML (175B) 128 2048 5e-5 × linear ✓ ✓ ✓\\nFlan-U-PaLM (540B) 32 -1e-3 - constant ✓ - ✓\\nWizardCoder (15B) 512 2048 2e-5 ✓ cosine - - - -\\nFig. 11: Distribution of benchmark datasets available for different natural language processing tasks. We include only the tasks\\nfor which at least 20 datasets have already been proposed.\\n4. LAMBADA [157]: This dataset evaluates contextual text\\nunderstanding through a word prediction task. Models must\\npredict the last word of a passage, which is easy for humans', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 17}), Document(page_content='understanding through a word prediction task. Models must\\npredict the last word of a passage, which is easy for humans\\nwhen given the whole passage, but not when given only the\\nlast sentence.\\n5. WinoGrande [158]: A large-scale dataset inspired by the\\noriginal Winograd [159] Schema Challenge tests models on\\ntheir ability to resolve pronoun ambiguity and encourages the\\ndevelopment of models that understand the broad context in\\nnatural language text.\\n6. MMLU [160]: A benchmark that measures the knowl-\\nedge acquired by models during pretraining and evaluates\\nmodels in zero-shot and few-shot settings across 57 subjects,\\ntesting both world knowledge and problem-solving ability.\\n7. SuperGLUE [3]: A more challenging and diverse suc-\\ncessor to the GLUE [161] benchmark, SuperGLUE includes\\na variety of language understanding tasks, such as question\\nanswering, natural language inference, and coreference reso-\\nlution. It is designed to provide a rigorous test of language', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 17}), Document(page_content='answering, natural language inference, and coreference reso-\\nlution. It is designed to provide a rigorous test of language\\nunderstanding and requires significant progress in areas like\\nsample-efficient, transfer, multitasking, and unsupervised or\\nself-supervised learning.\\n8. StoryCloze [162]: It introduces a new “StoryCloze Test”,\\na commonsense reasoning framework for evaluating story\\nunderstanding, generation, and script learning. It considers\\na model’s ability to understand and generate coherent and\\nsensible stories.\\n9. BoolQ [163]: A dataset derived from Google search\\nqueries, BoolQ challenges models to answer binary (yes/no)\\nquestions. The questions are naturally occurring and are paired\\nwith a paragraph from a Wikipedia article containing the\\nanswer. It’s a test of reading comprehension and reasoning.10. RACE-High [164]: A subset of the RACE [164]\\ndataset, RACE-High consists of high school-level English\\nexam questions. It is designed to evaluate the comprehension', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 17}), Document(page_content='dataset, RACE-High consists of high school-level English\\nexam questions. It is designed to evaluate the comprehension\\nability of models in a more academic and challenging context.\\n11. RACE-Middle [164]: Another subset of the\\nRACE [164] dataset, RACE-Middle, contains middle\\nschool-level English exam questions. It offers a slightly less\\nchallenging but academically oriented evaluation of a model’s\\ncomprehension skills.\\n12. Truthful-QA [130]: A unique benchmark that measures\\na language model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some of which are designed to test the model\\nagainst common human misconceptions.\\n13. ANLI [165]: A large-scale dataset designed to test the\\nrobustness of machine learning models in Natural Language\\nInference (NLI) is created through an iterative, adversarial\\nprocess where humans try to generate examples that models\\ncannot correctly classify.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 17}), Document(page_content='Inference (NLI) is created through an iterative, adversarial\\nprocess where humans try to generate examples that models\\ncannot correctly classify.\\n14. ARC-Challenge [166]: A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval,\\ntesting the true comprehension capabilities of models.\\n15. XNLI [167]: A cross-lingual benchmark, XNLI extends\\nthe MultiNLI [168] corpus to 15 languages, including low-\\nresource ones like Urdu. It tests models on cross-lingual\\nsentence understanding, with 112,500 annotated pairs across\\nthree categories: entailment, contradiction, and neutral.\\n16. PAWS-X [169]: PAWS-X, or Cross-lingual Paraphrase\\nAdversaries from Word Scrambling, is a multilingual version\\nof the PAWS [170] dataset for paraphrase identification. It\\nincludes examples in seven languages and is designed to eval-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 17}), Document(page_content='JOURNAL OF L ATEX 19\\nuate the performance of cross-lingual paraphrase identification\\nmodels.\\n17. ARC [166]: A larger version of the ARC-Challenge,\\nthis dataset contains both easy and challenging grade-school\\nlevel, multiple-choice science questions. It’s a comprehensive\\ntest of a model’s ability to understand and answer complex\\nquestions.\\n18. ARC-Easy [166]: A subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\nIt’s a great starting point for models beginning to explore\\nadvanced question-answering.\\n19. CoQA [171]: A conversational question-answering\\ndataset, CoQA challenges models with questions that rely\\non conversation history and require free-form text answers.\\nIts diverse content from seven domains makes it a rigorous\\ntest for models’ ability to handle a wide range of topics and\\nconversational contexts.\\n20. DROP [172]: DROP, or Discrete Reasoning Over the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='test for models’ ability to handle a wide range of topics and\\nconversational contexts.\\n20. DROP [172]: DROP, or Discrete Reasoning Over the\\ncontent of Paragraphs, is designed to test a model’s ability to\\nunderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\n21. RTE [173]: The Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\n22. BIG-bench [174]: The BIG-bench (Behavior of Intel-\\nligent Generative Models Benchmark) is a large-scale bench-\\nmark designed to test the abilities of LLMs across a wide\\nrange of tasks, including reasoning, creativity, ethics, and\\nunderstanding of specific domains.\\n23. SQUADv2 [175]: The Stanford Question Answering\\nDataset (SQuAD) [176] is a collection of questions posed by', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='understanding of specific domains.\\n23. SQUADv2 [175]: The Stanford Question Answering\\nDataset (SQuAD) [176] is a collection of questions posed by\\ncrowdworkers on a set of Wikipedia articles, where the answer\\nto every question is a segment of text from the corresponding\\nreading passage. SQuADv2 combines the original SQuAD1.1\\ndataset with over 50,000 unanswerable questions. The aim is to\\nevaluate a model’s ability to understand and answer questions\\nbased on a given context and to determine when a question is\\nunanswerable.\\n24. GSM8K [177]: A dataset of diverse grade school math\\nword problems, testing a model’s ability to perform multi-step\\nmathematical reasoning.\\n25. WiC [178]: This dataset assesses a model’s ability\\nto discern word meanings based on context, aiding in tasks\\nrelated to Word Sense Disambiguation.\\n26. Math23k [179]: This one challenges a model’s ability\\nto understand and solve mathematical word problems. It con-\\ntains 23,000 Chinese arithmetic word problems that require', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='to understand and solve mathematical word problems. It con-\\ntains 23,000 Chinese arithmetic word problems that require\\nmodels to perform reasoning and computation based on the\\nproblem description.\\n27. LCQMC [180]: The Large-scale Chinese Question\\nMatching Corpus (LCQMC) is a dataset for evaluating the\\nperformance of models in semantic matching tasks. It contains\\npairs of questions in Chinese and their matching status,making it a valuable resource for research in Chinese language\\nunderstanding.\\n28. MATH [181]: This dataset is a platform for evaluating\\nthe mathematical problem-solving abilities of AI models. It\\ncontains a diverse set of math problems, ranging from arith-\\nmetic to calculus, and is designed to test the model’s ability\\nto understand and solve complex mathematical problems.\\n29. ETHOS [182]: ETHOS is a hate speech detection\\ndataset built from YouTube and Reddit comments. It’s a tool\\nin the fight against online hate speech, offering binary and', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='dataset built from YouTube and Reddit comments. It’s a tool\\nin the fight against online hate speech, offering binary and\\nmulti-label variants for robust content moderation.\\n30. StereoSet [183]: StereoSet is a comprehensive dataset\\ndesigned to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. By contrasting stereo-\\ntypical bias against language modeling ability, it provides a\\nvaluable tool for understanding and mitigating biases in large\\nlanguage models.\\n31. HumanEval [184]: A dataset for the problem-solving\\nability of AI models, which includes a diverse set of tasks that\\nrequire various cognitive abilities, makes it a comprehensive\\ntool for assessing general intelligence in AI.\\n32. WebQA [185]: A dataset for open-domain question\\nanswering, WebQA offers a large collection of web-based\\nquestion-answer pairs. It is designed to assess the ability of', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='answering, WebQA offers a large collection of web-based\\nquestion-answer pairs. It is designed to assess the ability of\\nAI models to understand and answer questions based on web\\ncontent.\\n33. CMRC2018 [186]: This dataset is a test of Chinese\\nlanguage models’ ability to reason comprehensively and is\\ndesigned with a challenging span-extraction format that pushes\\nthe boundaries of machine performance.\\n34. Wikitext103 [187]: With over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as\\nlanguage modeling and translation.\\n35. PG19 [188]: This is a digital library of diverse books\\nfrom Project Gutenberg. It’s specifically designed to facilitate\\nresearch in unsupervised learning and language modeling, with\\na special focus on long-form content.\\n36. C4 [11]: A clean, multilingual dataset, C4 offers bil-\\nlions of tokens from web-crawled data. It’s a comprehensive', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='36. C4 [11]: A clean, multilingual dataset, C4 offers bil-\\nlions of tokens from web-crawled data. It’s a comprehensive\\nresource for training advanced Transformer models on various\\nlanguages.\\n37. QuAC [189]: This dataset simulates an information-\\nseeking dialog between students and teachers using hidden\\nWikipedia text. It introduces unique challenges not found\\nin machine comprehension datasets, making it a valuable\\nresource for advancing dialog systems.\\n38. COPA [190]: This dataset evaluates a model’s progress\\nin open-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability\\nto understand and reason about cause and effect.\\n39. WSC [159]: The Winograd Schema Challenge (WSC)\\nis a reading comprehension task in which a system must\\nresolve references in a text, often requiring world knowledge\\nand reasoning about the text.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 18}), Document(page_content='JOURNAL OF L ATEX 20\\n40. RACE [164]: The RACE is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge\\nof a real-world examination.\\n41. StrategyQA [191]: A question-answering dataset that\\nrequires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the\\nboundaries of what machines can understand and answer.\\n42. CSQA [192]: The CommonsenseQA is a question-\\nanswering dataset that requires commonsense knowledge to\\nanswer the ability of AI models to understand and answer\\nquestions that require commonsense reasoning.\\n43. GLUE [161]: The General Language Understanding\\nEvaluation (GLUE) benchmark is a collection of resources\\nfor training, evaluating, and analyzing natural language under-\\nstanding systems. It includes a variety of tasks that test a wide', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='for training, evaluating, and analyzing natural language under-\\nstanding systems. It includes a variety of tasks that test a wide\\nrange of linguistic phenomena, making it a comprehensive tool\\nfor evaluating language understanding in AI.\\nVII. S UMMARY AND DISCUSSION\\nA. Architecture\\nDue to the gigantic scale of LLMs, minor changes\\nin architecture and training strategies have a big impact\\non performance and stability. Here, we summarize key\\narchitectural modules used in various LLMs, leading to better\\nperformance, reduced training time and memory, and better\\ntraining stability.\\nLayer Normalization is found to have a significant effect on\\nthe performance and training stability of LLMs. Pre-norm,\\nthat is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [8], [104],\\n[84]. BLOOM [9] and AlexaTM [101] utilize an additional\\nlayer normalization before embedding layer to stabilize\\nthe training of large-scale models, while the model’s zero-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='layer normalization before embedding layer to stabilize\\nthe training of large-scale models, while the model’s zero-\\nshot generalization ability can be negatively impacted [9].\\nHowever, another study [102] finds that pre-norm degrades\\nfine-tuned model performance as compared to post-norm,\\nand there are no stability benefits of pre-norm beyond the\\n100B scale. Therefore, GLM-130B [102] used deep-norm\\nwhich is a variant of post-norm for better downstream task\\nperformance after fine-tuning.\\nPositional Encoding effect performance and training stability\\nof LLMs like other building blocks of a model. BLOOM [9]\\nfinds ALiBi outperforming learned and rotary positional\\nencodings. Contrary to this, GLM-130B [102] identifies\\nrotary positional encoding better than ALiBi. So, there is no\\nconclusion in literature about the positional encodings yet.\\nParallel Attention where attention and feed-forward layers\\nare parallel to each other rather than sequential in transformer', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='Parallel Attention where attention and feed-forward layers\\nare parallel to each other rather than sequential in transformer\\nblock has shown to reduce training time by 15%. There is no\\nevidence of performance drop due to this change in literature\\nand used by the models PaLM [14], GPT-NeoX [94], and\\nCodeGen [107].\\nMulti-Query Attention has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speedsup sampling in autoregressive decoding. No performance\\ndegradation has been observed with this change and makes\\nthe training efficient allowing larger batch sizes. Multi-query\\nattention is used in [14], [109].\\nMixture of Experts allows easily scaling model to trillion\\nof parameters [106], [97]. Only a few experts are activated\\nduring the computation making them compute-efficient. The\\nperformance of MoE models is better than the dense models\\nfor the same amount of data and requires less computation', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='performance of MoE models is better than the dense models\\nfor the same amount of data and requires less computation\\nduring fine-tuning to achieve performance similar to the\\ndense models as discussed in [97]. MoE architectures are less\\nprone to catastrophic forgetting, therefore are more suited for\\ncontinual learning [106]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [106].\\nSparse vs Dense Activated GPT-3 [8] uses sparse\\ntransformers [45] whereas GLaM [97] and PanGu-P[106]\\nuse MoE [98] architecture to lower computational costs\\nand increase the model size and capacity. According to\\nthe literature, sparse modules do not degrade the model’s\\nperformance [45]. However, more experiments are required\\nto verify this statement.\\nB. Training Strategies\\nTraining models at a huge scale require some tricks to\\nreduce training costs, avoid loss divergence and achieve better', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='B. Training Strategies\\nTraining models at a huge scale require some tricks to\\nreduce training costs, avoid loss divergence and achieve better\\nperformance. We summarize and discuss some of these key\\ntricks used in different LLMs.\\nMixed Precision is a famous method for LLMs to reduce\\nmemory usage and improve training efficiency. In mixed\\nprecision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept\\nin FP32 format [295]. A drawback associated with this format\\nchange is training instability due to a smaller value range\\nresulting in loss spikes [102]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs some\\nprecision-sensitive operations like gradient accumulation and\\nsoftmax in FP32 [9]. BF16 has better performance and training\\nstability but uses more memory and is supported on specific\\nhardware, for example, A100 GPUs. Therefore, its adoption\\nin LLMs is limited.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='stability but uses more memory and is supported on specific\\nhardware, for example, A100 GPUs. Therefore, its adoption\\nin LLMs is limited.\\nTraining Instability is a common issue in LLMs where loss\\ndivergence or spiking is observed multiple times during train-\\ning. This happens in the presence of gradient clipping [14].\\nTo mitigate this problem, many approaches suggest restarting\\ntraining from an earlier checkpoint [14], [102], [97], skipping\\n200-500 earlier data batches at the point of divergence in [14]\\nand re-shuffling batches in [97]. The embedding layer gradient\\nshrink proves to further stabilize the training as its gradient\\nnorm is significantly larger than the other layers [102]. Another\\nsuggestion to improve training stability for larger models is not\\nto use biases in dense and norm layers as in [14].\\nWeight Initialization plays a significant role in model conver-\\ngence and training stability. GPT-NeoX [94] initializes feed-\\nforward layers before residuals with2\\nL√', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='gence and training stability. GPT-NeoX [94] initializes feed-\\nforward layers before residuals with2\\nL√\\ndas in [124] and\\nother layers with small initialization scheme [296]. This avoids', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 19}), Document(page_content='JOURNAL OF L ATEX 21\\nTABLE VIII: Training and evaluation dataset for pre-trained LLMs. Here,“D” denotes Dialogue, “QA” denotes question\\nanswering, “CR” is for commonsense reasoning, “CoT” is for chain-of-thought, “RC” for reading comprehension, “LU”\\nfor language understanding, “IRC” for in-context reading comprehension, “NLI” for natural language inference, “WT”\\nfor winograd-style tasks, “SC” for sentence completion, “WSD” for word sense disambiguation, “CorefR” for coreference\\nresolution.Models Training Dataset Evaluation Dataset\\nT5 C4 [11]GLUE [161], CNNDM, SQuAD [176], SuperGLUE [3], EnDe, ENFr, EnRo,\\nQQP [193], MNLI-m [194], MNLI-mm [194], QNLI [176],\\nWNLI [159], CB [195],\\nWiC [178], WMT [196], CNN/DM\\nGPT-3 Common Crawl, WebText, Books Corpora, WikipediaQA: NaturalQS, WebQS, TriviaQA, ARC, CoQA, DROP\\nSuperGLUE, WMT, LAMBADA, StoryCloze, HellaSwag\\nmT5 mC4 [12]SP: XNLI [167], PAWS-X [169] S: WikiAnn NER [197]\\nQA: MLQA [198], TyDiQA-GoldP [199]\\nPanGu- α 1.1TB Chinese Text Corpus -', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='mT5 mC4 [12]SP: XNLI [167], PAWS-X [169] S: WikiAnn NER [197]\\nQA: MLQA [198], TyDiQA-GoldP [199]\\nPanGu- α 1.1TB Chinese Text Corpus -\\nCPM-2 WuDaoCorpus [85]CCPM [200], C3[201], Sogou-Log,\\nWMT20 [202], Math23k [179], LCSTS [203],\\nLCQMC [180], AdGen [204], CUGE [205]\\nCodex54 million public software repositories hosted on GitHub\\ncontaining python files under 1MBHumanEval [184],\\n64 original programming problems with unit test\\nERNIE3.0Chinese text corpora, Baidu Search, Web text,\\nQA-long, QA-short, Poetry & Couplet\\nDomain-specific data from medical, law and financial area\\nBaidu knowledge graph with more than 50 million factsNLU: NLPCC2014-SC, SE-ABSA16_PHNS, SE-ABSA16_CAME,\\nBDCI2019, COTE-BD [206], COTE-DP [206], COTE-MFW [206],\\nXNLI [167], OCNLI [207], CMNLI [207], CLUEWSC2020 [207],\\nFinRE [208], SanWen [209], CCKS2020, AFQMC [207],\\nLCQMC [180], CSL [207], PAWS-X [169], BQ Corpus [210],\\nTNEWS, IFLYTEK [211], THUCNEWS, CNSE [212], CNSS [212],\\nNLPCC-DBQA, CHIP2019, cMedQA [213],', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='LCQMC [180], CSL [207], PAWS-X [169], BQ Corpus [210],\\nTNEWS, IFLYTEK [211], THUCNEWS, CNSE [212], CNSS [212],\\nNLPCC-DBQA, CHIP2019, cMedQA [213],\\ncMedQA2 [214], CKBQA 13 [215], WebQA [185],\\nCLUENER [207], Weibo [216], OntoNotes [217], CCKS2019,\\nCMRC 2018 [186], CMRC2019 [218], DRCD [219],\\nDuReader [220], Dureader robust [221], Dureader checklist , Dureader yesno,\\nC3[201], CHID [222], CAIL2018-Task1 & Task2 [223],\\nDogWhistle Insider & Outsider [224], Sogou-log [225];\\nNLG: LCSTS [203], KBQG, DuReader-QG [220],\\nDureader robust -QG [221], MATINF-QA [226], Math23KMath23k [179],\\nAdGen [204], WMT20-enzh [202], KdConv [227]\\nJurassic-1Wikipedia, OWT, Books, C4 [11],\\nPileCC [228], arXiv, GitHubARC-Challenge [166], ARC-Easy [166], BoolQ [163],\\nHellaSwag [154], PIQA [155],\\nRACE-high [164], RACE-middle [164],\\nRTE [173], StoryCloze [162], WinoGrande [158]\\nHyperCLOV AKorean blogs, Community sites, News, KiN\\nKorean Wikipedia, Wikipedia (English and Japanese);\\nModu-Corpus: Messenger, News,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='HyperCLOV AKorean blogs, Community sites, News, KiN\\nKorean Wikipedia, Wikipedia (English and Japanese);\\nModu-Corpus: Messenger, News,\\nSpoken and written language corpus, Web corpusNSMC: a movie review dataset from NA VER movies;\\nKorQuAD 1.0 [229], Korean ML dataset\\nAI Hub Korean-English, YNAT [230],\\nKLUE-TC [230], KLUE-STS [230]\\nYuan 1.0Common Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, BooksFewCLUE [231], ZeroCLUE [207],\\nCMRC2018 [186], WebQA [185]\\nGophersubsets of MassiveWeb [92]\\nBooks, C4 [11], News, GitHub and\\nWikipedia samples from MassiveText [92]LM: Pile [228], LAMBADA [157],\\nWikitext103 [187], PG-19 [188], C4 [11];\\nLU: MMLU [160], BIG-bench [174];\\nRC: RACE-middle [164], RACE-high [164]\\nQA: TriviaQA [156], TruthfulQA [130], Natural Questions [232];\\nFact Checking on Fever [233], MultiFC [234];\\nHellaSwag [154], PIQA [155], WinoGrande [158], SIQA [235];\\nRealToxicityPrompts [236], Twitter Dataset [237],\\nCivilComments toxicity classification [238]', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='RealToxicityPrompts [236], Twitter Dataset [237],\\nCivilComments toxicity classification [238]\\nERNIE3.0 TITANChinese text corpora, Baidu Search, Web text,\\nQA-long, QA-short, Poetry & Couplet\\nDomain-specific data from medical, law and financial area\\nBaidu knowledge graph with more than 50 million facts\\nERNIE 3.0 adversarial dataset, ERNIE 3.0 controllable datasetNLU: NLPCC2014-SC, SE-ABSA16_PHNS, SE-ABSA16_CAME,\\nBDCI2019, EPRSTMT [231], COTE-BD [206], COTE-MFW [206],\\nOCNLI [207], CMNLI [207], OCNLI-FC [231], CLUEWSC [207]\\nCLUEWSC-FC [231], FinRE [208], SanWen [209], AFQMC [207],\\nLCQMC [180], PAWS-X [169], BQ Corpus [210], CSL [207]\\nCSL-FC [231], BUSTM, TNEWS, TNEWS-FC [231], IFLYTEK [211], IFLYTEK-FC\\nTHUCNEWS, CNSE [212], CNSS [212], CSLDCP\\nNLPCC-DBQA, CHIP2019, cMedQA [213],\\ncMedQA2 [214], CKBQA 13 [215], WebQA [185],\\nPD&CFT, CMRC2017 [239], CMRC2019 [218]\\nCHID [222], CHID-FC [231], WPLC, DRCD [219],\\nDuReader [220], Dureader robust [221], Dureader checklist , Dureader yesno,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='CHID [222], CHID-FC [231], WPLC, DRCD [219],\\nDuReader [220], Dureader robust [221], Dureader checklist , Dureader yesno,\\nC3[201], CMRC 2018 [186], CAIL2018-Task1 & Task2 [223]\\nDogWhistle Insider & Outsider [224]\\nGPT-NeoX-20B Pile [228]ANLI [165], ARC [166], HeadQA [240], HellaSwag [154],\\nLAMBADA [157], LogiQA [241], OpenBookQA [242], PIQA [155],\\nPROST [243], QA4MRE [244], SciQ [245], TriviaQA [156],\\nWinoGrande [158], SuperGLUE [3], MATH [181],\\nAdvanced Knowledge-Based Tasks\\nOPTRoBERTa [246], Pile [228],\\nPushShift.io Reddit [247]HellaSwag [154], StoryCloze [162], PIQA [155],\\nARC-Easy [166], ARC-Challenge [166], OpenBookQA [242],\\nWinoGrad [159], WinoGrande [158], SuperGLUE [3],\\nWizard of Wikipedia [248], Empathetic Dialogues [249],\\nConvAI2 [250], Blended Skill Talk [251], Wizard of Internet [252]\\nETHOS [182], CrowS-Pairs [253], StereoSet [183],\\nRealToxicPrompts [236], Dialogue Responsible AI evaluationsTable Continued on Next Page', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 20}), Document(page_content='JOURNAL OF L ATEX 22Models Training Dataset Evaluation Dataset\\nBLOOM ROOTS [254] -\\nGalacticaarXiv, PMC, Semantic Scholar\\nWikipedia, StackExchange, LibreText, Open Textbooks\\nRefSeq Genome, OEIS, LIPID MAPS, NASAExoplanet\\nCommon Crawl, ScientificCC, AcademicCC\\nGitHub repositories\\nKhan Problems [255], GSM8K [177], OneSmallStepKnowledge probes, Latex equations,\\nAminoProbe [116], BioLAMA [116], Chemical Reactions [116],\\nGalaxy Clusters [116], Mineral Groups [116]\\nGLaMFiltered Webpages, Social media conversations\\nWikipedia, Forums, Books, NewsNLG: TriviaQA [156], NQS, WebQS, SQuADv2 [175],\\nLAMBADA [157], DROP [172], QuAC [189], CoQA [171];\\nNLU: HellaSwag [154], StoryCloze [162], WinoGrad [159],\\nWinoGrande [158], RACE-middle [164], RACE-high [164], PIQA [155],\\nARC-Challenge [166], ARC-Easy [166], OpenbookQA [242],\\nBoolQ [163], COPA [256], RTE [173], WiC [178],\\nMultiRC [257], WSC [159], ReCoRD [258], CB [195],\\nANLI R1 [165], ANLI R2 [165], ANLI R3 [165]', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='BoolQ [163], COPA [256], RTE [173], WiC [178],\\nMultiRC [257], WSC [159], ReCoRD [258], CB [195],\\nANLI R1 [165], ANLI R2 [165], ANLI R3 [165]\\nLaMDA Infiniset [118]: Public documents, Dialogs, UtterancesMini-Turing Benchmark (MTB) [4];\\nSelf-collected dialogs with turns by asking\\ncrowdworkers to interact with LaMDA;\\nWizard of Wikipedia [248]\\nMT-NLGTwosnapshots of Common Crawl and Books3,\\nOpenWebText2, Stack Exchange, PubMed Abstracts,\\nWikipedia, PG-19 [188], BookCorpus2, NIH ExPorter,\\nPileCC [228], CC-Stories [259], RealNews [260]Completionprediction: LAMBADA [157]\\nRC: RACE [164], BoolQ [163]\\nCR:PiQA [155]\\nNaturalLanguage Interface: ANLI [165], HANS [261]\\nAlphaCodeSelected GitHub repositories\\nCodeContests [109]: Codeforces [262],\\nDescription2Code [263], CodeNet [264]Codeforces competitions, CodeContests [109], APPS [181]\\nChinchillaMassiveWeb [92], MassiveText [92]\\nBooks, C4 [11], News, GitHub, WikipediaLM: Pile [228], LAMBADA [157],\\nWikitext103 [187], PG-19 [188], C4 [11];', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='Books, C4 [11], News, GitHub, WikipediaLM: Pile [228], LAMBADA [157],\\nWikitext103 [187], PG-19 [188], C4 [11];\\nLU: 57 MMLU [160] tasks, 62 BIG-bench [174] tasks;\\nQA: TriviaQA [156], Natural Questions [232];\\nRC: RACE-middle [164], RACE-high [164];\\nHellaSwag [154], PIQA [155], WinoGrande [158],\\nSIQA [235], BoolQ [163], TruthfulQA [130]\\nPaLMwebpages, books, wikipedia, news, articles,\\nsource code, social media conversationsQA: TriviaQA [156], Natural Questions [232], Web Questions [265],\\nTyDiQA-GoldP [199]; CR: PIQA [155], ARC [166], OpenBookQA [242];\\nIRC: DROP [172], CoQA [171], QuAC [189], SQuADv2 [175], RACE [164];\\nNLI: ANLI [165]; WT: WinoGrad [159], WinoGrande [158];\\nCoT: GSM8K [177], StrategyQA [191], CSQA [192],\\nSV AMP [266], MAWPS [267], AQuA [268];\\nLU: MMLU [160] SuperGLUE [3], LAMBADA [157],\\nHellaSwag [154], StoryCloze [162], BIG-bench [174], WMT language pairs\\nAlexaTM Wikipedia, mC4 [12]NLG: MLSum [269], XSum [270], E2E [271], WebNLG [272];', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='AlexaTM Wikipedia, mC4 [12]NLG: MLSum [269], XSum [270], E2E [271], WebNLG [272];\\nMachine Translation: Flores-101 [273], English-German WMT’16,\\nEnglish-French WMT’14, German-French WMT’19 [274];\\nNLP: XNLI [167], XCOPA [190], PAWS-X [169], XWinograd [275],\\nSuperGLUE [3], SQUADv2 [175], MultiArith [276]\\nSparrowHuman data for rule violations\\nand per-turn response preferences,\\nSelf-play data accumulated through training,\\nGopherCite FilteredELI5 [277]Per-turn response preference and adversarial probing,\\nMulti-turn dialogues, Information-seeking dialogues,\\nChinchilla-generated [100] conversational questions,\\nGopherCite [277] human evaluation interface,\\nFilteredELI5 [277] “Free” dialogues, DPC-generated [100] dialogues\\nWinoGender [278], Winobias [279], BBQ [280],\\nNatural Questions [232], Quiz Bowl [281], TriviaQA [156]\\nU-PaLM Same as PaLMMMLU [160],\\nQA: TriviaQA [156], Natural Questions [232], TydiQA [199];\\nRC: LAMBADA [157];\\nCR: BoolQ [163], PIQA [155], HellaSwag [154], WinoGrande [158];', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='QA: TriviaQA [156], Natural Questions [232], TydiQA [199];\\nRC: LAMBADA [157];\\nCR: BoolQ [163], PIQA [155], HellaSwag [154], WinoGrande [158];\\nCoT: GSM8K [177], BBH [174], StrategyQA [191], CSQA [192];\\nLU: MMLU [160] SuperGLUE [3], MGSM [282]\\nUL2 -SuperGLUE [3], GSM8K [177], SV AMP [266], ASDiv [283],\\nMAWPS [267], AQuA [268]\\nGLM-130B -LAMBADA [157], Pile [228], MMLU [160],\\nCLUE [207], CrowS-Pairs [253], StereoSet [183],\\nETHOS [182], RealToxicPrompts [236]\\nCodeGen Pile [228], BigQuery, BigPython [107] Mostly Basic Python Problems\\nLLaMACommonCrawl, C4 [11], Github,\\nWikipedia, Books, arXiv, StackExchangeCR: BoolQ [163], PIQA [155], SIQA [235], HellaSwag [154],\\nWinoGrande [158], ARC-Challenge [166], OpenBookQA [242];\\nQA: TriviaQA [156], Natural Questions [232];\\nRC: RACE-middle [164], RACE-high [164];\\nMathematical Reasoning: MATH [181], GSM8K [177];\\nCode Generation: HumanEval [184], MBPP [284];\\nMMLU [160], RealToxicityPrompts [236],\\nCrowS-Pairs [253], WinoGender [278], TruthfulQA [130]', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='Code Generation: HumanEval [184], MBPP [284];\\nMMLU [160], RealToxicityPrompts [236],\\nCrowS-Pairs [253], WinoGender [278], TruthfulQA [130]\\nPanGU Σ WuDaoCorpora [85], CLUE [207], Pile [228], C4 [11], and Python code -\\nBloombergGPT FinPile [121], The Pile [228], C4 [11], Wikipedia [17]Financial Data, BIG-bench [174], MMLU [160], ARC, PiQA [155],\\nCommonsenseQA [192], BoolQ [163], OpenBookQA [242], RACE [164], MultiRC [257],\\nReCoRD [258], ANLI [165], RTE [173], COPA [190], WIC [178], WinoGrad [159],\\nWinoGrande [158], HellaSWAG [154], StoryCloze [162]\\nXuanYuan 2.0 Internet -\\nCodeT5+ CodeSearchNet [285], Github Code HumanEval [184], MathQA [286], GSM8K [177]\\nStarCoder The Stack v1.2 [287]HumanEval [184], MBPP [284], DS-1000 [288], HELM [289],\\nMulti-Language Evaluation, GSM8K [177], MMLU [160], CoQA [171]', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 21}), Document(page_content='JOURNAL OF L ATEX 23\\nTABLE IX: Training and evaluation datasets for instruction-tuned LLMs. All the abbreviations are the same as Table VIII\\nModels Training Datasets Evaluation Datasets\\nT0 -NLI: ANLI [165], CB [195], RTE [173];\\nSC: COPA [256], HellaSwag [154] StoryCloze [162];\\nWSD: WiC [178]; CorefR: WSC [159], Wino (XL) [158]\\nWebGPTELI5 [129], ELI5 fact-check [126], TriviaQA [156],\\nARC-Challenge [166], ARC-Easy [166],\\nHand-written data, Demonstrations of humans,\\nComparisons between model-generated answersELI5 [129], TruthfulQA [130], TriviaQA [156]\\nTk-INSTRUCT SUP-NATINST [26] SUP-NATINST [26]\\nmT0 xP3 [127] -\\nOPT-IMLPromptSource [22], FLAN [25],\\nSuper-NaturalInstructions [290],\\nUnifiedSKG [291], CrossFit [292],\\nExMix [293], T5 [11], ReasoningPromptSource [22], FLAN [25], Super-NaturalInstructions [290],\\nUnifiedSKG [291], CrossFit [292], ExMix [293], T5 [11],\\nReasoning, MMLU [160], BBH [174], RAFT [294]\\nFlan Muffin, T0-SF, NIv2, CoT MMLU [160], BBH [174], TyDiQA [199], MGSM [282]', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='Reasoning, MMLU [160], BBH [174], RAFT [294]\\nFlan Muffin, T0-SF, NIv2, CoT MMLU [160], BBH [174], TyDiQA [199], MGSM [282]\\nWizardCoder Code Alpaca HumanEval [184], MBPP [284], DS-1000 [288]\\nactivations growing exponentially with the increasing depth.\\nMT-NLG [21] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [296]. Various models perform random weight ini-\\ntialization which can cause bad initialization, Galactica [116]\\nsuggests a longer warmup to negate the effect.\\nLearning Rate is important for stable training. It is suggested\\nto use a lower value [9], [14], [20] with warmup and decay\\n(cosine or linear). Usually, the learning rate is within the\\nrange 1e−4to8e−4. Moreover, MT-NLG (530B) [21] and\\nGPT-NeoX (20B) [94] suggest interpolating learning rates\\nbased on the model size using the GPT-3 [8] models ranging\\nbetween 13B and 175B. This avoids tuning the learning rate\\nhyperparameter.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='based on the model size using the GPT-3 [8] models ranging\\nbetween 13B and 175B. This avoids tuning the learning rate\\nhyperparameter.\\nTraining Parallelism 3D parallelism, a combination of data,\\npipeline and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [102], [14], [10], [9], [21], [91],\\n[88]. In addition to the 3D parallelism, BLOOM [9] uses zero\\noptimizer [61] to shard optimizer states. PanGu- α[84] and\\nPanGu- Σ[106] go beyond the 3D parallelism and apply 5D\\nparallelism which additionally contains optimizer parallelism\\nand rematerialization.\\nMode Switching adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve the downstream task performance\\nin [15], [20], [101]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation Generating credible and con-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='are appended based on the downstream tasks.\\nControllable Text Generation Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-\\n3 [8] and other LLMs use in-context learning to control\\ngenerated text. While in-context learning helps in controlling\\nthe generated text, ERNIE 3.0 Titan [93] suggests using\\nadversarial loss to rank its generated text for credibility and\\nsoft prompts such as genre, topic, keywords, sentiment, and\\nlength for better control on generated text.\\nAlignment using human feedback . Utilizing human prefer-\\nence in Large Language Models (LLMs) offers a significant\\nadvantage in addressing misalignment issues and ensuring\\naccurate outputs. Reinforcement Learning from Human Feed-\\nback (RLHF) [79] uses human feedback as a reward to guide\\nmodel learning naturally, such as ranking and rating, reducingmanual effort and annotation costs, enhancing model perfor-\\nmance, and aligning with human values for ethical decision-\\nmaking.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='mance, and aligning with human values for ethical decision-\\nmaking.\\nOn the other hand, human involvement makes data col-\\nlection time-consuming and resource-intensive. Designing a\\nsuitable reward function demands domain expertise, and com-\\nplex labelling tasks may lead to ambiguous results. The\\ngrowing data diversity poses scalability challenges for human\\nannotators. Additionally, there’s a risk of unintentionally trans-\\nferring biases from human feedback to the model’s behav-\\nior [297]. Lastly, the subjectivity in \"HHH\" evaluation criteria\\nrequires careful definition and implementation for effective\\nalignment [78]. Recently Reinforcement Learning from AI\\nFeedback (RLAIF) and context distillation, use labels without\\nhuman annotations, while enhancing reward modeling (RM)\\nby generating synthetic comparison data from different-sized\\nvanilla LLMs and prompts leading to an improved model for\\nalignment [298], [299], [300], [301].\\nC. Pre-Training vs Instruction Tuning', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='vanilla LLMs and prompts leading to an improved model for\\nalignment [298], [299], [300], [301].\\nC. Pre-Training vs Instruction Tuning\\nWhile pre-training is important for the generalization of\\nLLMs, instruction-tuning improves the performance of LLMs\\nfurther and makes them useable. Therefore, it is suggested\\nto perform instruction fine-tuning of pre-trained LLMs to use\\nthem effectively [25], [26], [302], [24], [126].\\nD. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing\\ndiverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks\\nby a large margin as shown in [8], [14], [26].\\nE. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance difference between zero-shot and few-shot\\nis large for pre-trained models [8], [14], naming LLMs as', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='the performance difference between zero-shot and few-shot\\nis large for pre-trained models [8], [14], naming LLMs as\\nmeta-learners [8]. LLMs zero-shot evaluations underperform\\nunsupervised methods in neural machine translation [8]. The', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 22}), Document(page_content='JOURNAL OF L ATEX 24\\nliterature shows pre-training is not enough for good zero-\\nshot performance [14], [25]. To improve the zero-shot per-\\nformance the literature suggests using instruction fine-tuning\\nthat improves the zero-shot performance significantly and\\noutperforms baselines. Instruction fine-tuning has also been\\nshown to improve zero-shot generalization to unseen tasks.\\nAnother model Flan-PaLM [25] unlocks zero-shot reasoning\\nwith CoT training.\\nF . Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for different\\ntasks, for example, encoder-only for NLU tasks, decoder-\\nonly for NLG, and encoder-decoder for sequence2sequence\\nmodeling. Encoder-only models are famous for smaller models\\nsuch as Bert [5], RoBERTa [303], etc, whereas LLMs are\\neither decoder-only [8], [94], [9] or encoder-decoder [11],\\n[12], [101]. While decoder-only models are good at NLG\\ntasks, various LLMs, PaLM [14], OPT [10], GPT-3 [8],', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 23}), Document(page_content='[12], [101]. While decoder-only models are good at NLG\\ntasks, various LLMs, PaLM [14], OPT [10], GPT-3 [8],\\nBLOOM [9], LLaMA [133], are decoder-only models with\\nsignificant performance gains on both NLU and NLG tasks. In\\ncontradiction to this, T5 [11] and UL2 [15] identify encoder-\\ndecoder models out-performing decoder-only models. In an-\\nother study, PaLM [14] finds increasing the size of decoder-\\nonly models can reduce the performance gap between decoder-\\nonly and encoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [15], [101] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5+ [113]\\nuses an encoder-decoder architecture with multiple training\\nobjectives for different tasks, activating the encoder, decoder,\\nor both according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 23}), Document(page_content='or both according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\nVIII. C ONCLUSION\\nThis paper has reviewed various LLMs, discussing the pros\\nand cons of multiple models. Our review concluded significant\\nfindings and provided a detailed analysis of the design aspects\\nof each LLM, including architecture, datasets, and training\\npipelines. We have identified crucial architectural compo-\\nnents and training strategies employed by different LLMs\\nand presented a summary and discussion. Moreover, we have\\ncompared the performance of LLMs in zero-shot and few-shot\\nsettings, explored the impact of fine-tuning, and compared\\nsupervised vs generalized models, and encoder vs decoder\\nvs encoder-decoder architectures. This paper will serve as a\\nvaluable resource for researchers, offering insights into the', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 23}), Document(page_content='vs encoder-decoder architectures. This paper will serve as a\\nvaluable resource for researchers, offering insights into the\\nrecent advancements in LLMs and providing fundamental\\nconcepts and details to develop improved LLMs.\\nIX. V ERSIONING\\nWe keep track of the versions of this paper we release as\\nthe content updates.\\nVersion 1.0: We covered 30 pre-trained models and 6\\ninstruction-tuned models, including their overview, findings,training, and evaluation datasets, and discussed important\\narchitectural and training tricks by various LLMs.\\nVersion 1.1: Further pre-trained LLMs added along with\\ndiscussion on on self-instruct LLMs. Categorized LLMs ac-\\ncording to the application, provided descriptions of widely\\nused evaluation datasets, added a section on robotics, and\\nextended discussion in section VII. Tables have been updated.\\nNote: If you find any mistakes, or have issues and conflicts\\nwith the writing in this paper, please email us. We welcome\\nsuggestions to improve this paper.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 23}), Document(page_content='JOURNAL OF L ATEX 25\\nREFERENCES\\n[1] B. A. y Arcas, “Do large language models understand us?” Daedalus ,\\nvol. 151, no. 2, pp. 183–197, 2022. 1\\n[2] A. Chernyavskiy, D. Ilvovsky, and P. Nakov, “Transformers:“the end\\nof history” for natural language processing?” in Machine Learning\\nand Knowledge Discovery in Databases. Research Track: European\\nConference, ECML PKDD 2021, Bilbao, Spain, September 13–17,\\n2021, Proceedings, Part III 21 . Springer, 2021, pp. 677–693. 1\\n[3] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, and S. Bowman, “Superglue: A stickier benchmark for\\ngeneral-purpose language understanding systems,” Advances in neural\\ninformation processing systems , vol. 32, 2019. 1, 18, 21, 22\\n[4] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thop-\\npilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y . Lu et al. , “Towards\\na human-like open-domain chatbot,” arXiv preprint arXiv:2001.09977 ,\\n2020. 1, 22', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='pilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y . Lu et al. , “Towards\\na human-like open-domain chatbot,” arXiv preprint arXiv:2001.09977 ,\\n2020. 1, 22\\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805 , 2018. 1, 24\\n[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations,” in\\nNAACL-HLT . Association for Computational Linguistics, 2018, pp.\\n2227–2237. 1\\n[7] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,” arXiv preprint arXiv:1910.13461 , 2019. 1\\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems , vol. 33, pp. 1877–1901, 2020. 1, 2, 8, 9, 11, 14, 15, 20, 23,\\n24\\n[9] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé et al. , “Bloom: A 176b-\\nparameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100 , 2022. 1, 2, 5, 9, 11, 15, 20, 23, 24\\n[10] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V . Lin et al. , “Opt: Open pre-trained\\ntransformer language models,” arXiv preprint arXiv:2205.01068 , 2022.\\n1, 2, 9, 11, 15, 23, 24\\n[11] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learn-\\ning with a unified text-to-text transformer,” The Journal of Machine', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learn-\\ning with a unified text-to-text transformer,” The Journal of Machine\\nLearning Research , vol. 21, no. 1, pp. 5485–5551, 2020. 1, 2, 6, 7, 8,\\n15, 19, 21, 22, 23, 24\\n[12] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934 , 2020. 1, 2,\\n6, 8, 9, 15, 21, 22, 24\\n[13] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi,\\nJ. Guan, P. Ke et al. , “Cpm-2: Large-scale cost-effective pre-trained\\nlanguage models,” AI Open , vol. 2, pp. 216–224, 2021. 1, 8, 15\\n[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. , “Palm: Scaling\\nlanguage modeling with pathways,” arXiv preprint arXiv:2204.02311 ,\\n2022. 1, 2, 10, 15, 20, 23, 24\\n[15] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='2022. 1, 2, 10, 15, 20, 23, 24\\n[15] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng et al. , “Ul2: Unifying language\\nlearning paradigms,” in The Eleventh International Conference on\\nLearning Representations , 2022. 2, 6, 10, 15, 23, 24\\n[16] “Common crawl.” [Online]. Available: https://commoncrawl.org/ 2\\n[17] “Wikipedia.” [Online]. Available: https://en.wikipedia.org/wiki/Main_\\nPage 2, 22\\n[18] “Openwebtext corpus.” [Online]. Available: http://Skylion007.github.\\nio/OpenWebTextCorpus 2\\n[19] “Bigquery dataset.” [Online]. Available: https://cloud.google.com/\\nbigquery?hl=zh-cn 2\\n[20] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al. , “Transcending scaling\\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399 ,\\n2022. 2, 6, 10, 15, 23\\n[21] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='2022. 2, 6, 10, 15, 23\\n[21] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti et al. , “Us-\\ning deepspeed and megatron to train megatron-turing nlg 530b, a large-\\nscale generative language model,” arXiv preprint arXiv:2201.11990 ,\\n2022. 2, 9, 15, 23[22] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al. , “Multitask\\nprompted training enables zero-shot task generalization,” arXiv preprint\\narXiv:2110.08207 , 2021. 2, 11, 15, 23\\n[23] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf\\net al. , “Crosslingual generalization through multitask finetuning,” arXiv\\npreprint arXiv:2211.01786 , 2022. 2, 13\\n[24] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al. , “Opt-iml: Scaling language', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='[24] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al. , “Opt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,”\\narXiv preprint arXiv:2212.12017 , 2022. 2, 13, 15, 23\\n[25] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma et al. , “Scaling instruction-finetuned\\nlanguage models,” arXiv preprint arXiv:2210.11416 , 2022. 2, 11, 13,\\n14, 15, 23, 24\\n[26] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap et al. ,\\n“Super-naturalinstructions: Generalization via declarative instructions\\non 1600+ nlp tasks,” in Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing , 2022, pp. 5085–\\n5109. 2, 13, 14, 15, 23\\n[27] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='5109. 2, 13, 14, 15, 23\\n[27] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\\nparameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691 ,\\n2021. 2, 8, 11\\n[28] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards\\na unified view of parameter-efficient transfer learning,” arXiv preprint\\narXiv:2110.04366 , 2021. 2, 7\\n[29] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, and\\nS. Poria, “Llm-adapters: An adapter family for parameter-efficient fine-\\ntuning of large language models,” arXiv preprint arXiv:2304.01933 ,\\n2023. 2, 7\\n[30] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\\nparameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691 ,\\n2021. 2, 7\\n[31] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts\\nfor generation,” arXiv preprint arXiv:2101.00190 , 2021. 2, 7\\n[32] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='for generation,” arXiv preprint arXiv:2101.00190 , 2021. 2, 7\\n[32] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al. , “A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt,” arXiv preprint arXiv:2302.09419 , 2023.\\n2, 3\\n[33] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\\nB. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language\\nmodels,” arXiv preprint arXiv:2303.18223 , 2023. 2, 3, 7, 15\\n[34] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al. , “Augmented language models: a survey,” arXiv preprint\\narXiv:2302.07842 , 2023. 2\\n[35] U. Naseem, I. Razzak, S. K. Khan, and M. Prasad, “A comprehensive\\nsurvey on word representation models: From classical to state-of-the-\\nart word representation language models,” Transactions on Asian and\\nLow-Resource Language Information Processing , vol. 20, no. 5, pp.\\n1–35, 2021. 3', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='art word representation language models,” Transactions on Asian and\\nLow-Resource Language Information Processing , vol. 20, no. 5, pp.\\n1–35, 2021. 3\\n[36] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, and D. Roth, “Recent advances in natural language\\nprocessing via large pre-trained language models: A survey,” arXiv\\npreprint arXiv:2111.01243 , 2021. 3\\n[37] J. J. Webster and C. Kit, “Tokenization as the initial phase in nlp,”\\ninCOLING 1992 volume 4: The 14th international conference on\\ncomputational linguistics , 1992. 4\\n[38] T. Kudo, “Subword regularization: Improving neural network transla-\\ntion models with multiple subword candidates,” in Proceedings of the\\n56th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , 2018, pp. 66–75. 4\\n[39] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\\nof rare words with subword units,” in Proceedings of the 54th Annual', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='[39] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\\nof rare words with subword units,” in Proceedings of the 54th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers) , 2016, pp. 1715–1725. 4\\n[40] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y . Lee, B. Sagot et al. , “Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization\\nin nlp,” arXiv preprint arXiv:2112.10508 , 2021. 4\\n[41] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in\\n2012 IEEE international conference on acoustics, speech and signal\\nprocessing (ICASSP) . IEEE, 2012, pp. 5149–5152. 4', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 24}), Document(page_content='JOURNAL OF L ATEX 26\\n[42] C. W. Eriksen and J. E. Hoffman, “Some characteristics of selective\\nattention in visual perception determined by vocal reaction time,”\\nPerception & Psychophysics , vol. 11, no. 2, pp. 169–171, 1972. 4\\n[43] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473 ,\\n2014. 4\\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems , vol. 30, 2017. 4,\\n5, 7, 8, 9, 10, 11\\n[45] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\\nsequences with sparse transformers,” arXiv preprint arXiv:1904.10509 ,\\n2019. 4, 8, 20\\n[46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, “Flashattention: Fast\\nand memory-efficient exact attention with io-awareness,” Advances in\\nNeural Information Processing Systems , vol. 35, pp. 16 344–16 359,\\n2022. 4', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='and memory-efficient exact attention with io-awareness,” Advances in\\nNeural Information Processing Systems , vol. 35, pp. 16 344–16 359,\\n2022. 4\\n[47] O. Press, N. Smith, and M. Lewis, “Train short, test long: Attention\\nwith linear biases enables input length extrapolation,” in International\\nConference on Learning Representations , 2022. [Online]. Available:\\nhttps://openreview.net/forum?id=R8sQPpGCv0 4\\n[48] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, and Y . Liu, “Roformer:\\nEnhanced transformer with rotary position embedding,” arXiv preprint\\narXiv:2104.09864 , 2021. 4, 9, 10\\n[49] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy,\\n“The impact of positional encoding on length generalization in trans-\\nformers,” arXiv preprint arXiv:2305.19466 , 2023. 4\\n[50] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward\\nnetworks are universal approximators,” Neural networks , vol. 2, no. 5,\\npp. 359–366, 1989. 5', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='networks are universal approximators,” Neural networks , vol. 2, no. 5,\\npp. 359–366, 1989. 5\\n[51] V . Nair and G. E. Hinton, “Rectified linear units improve restricted\\nboltzmann machines,” in Proceedings of the 27th international confer-\\nence on machine learning (ICML-10) , 2010, pp. 807–814. 5\\n[52] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),”\\narXiv preprint arXiv:1606.08415 , 2016. 5\\n[53] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\\ndinov, “Dropout: a simple way to prevent neural networks from\\noverfitting,” The journal of machine learning research , vol. 15, no. 1,\\npp. 1929–1958, 2014. 5\\n[54] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R. Ke,\\nA. Goyal, Y . Bengio, A. Courville, and C. Pal, “Zoneout: Regulariz-\\ning rnns by randomly preserving hidden activations,” arXiv preprint\\narXiv:1606.01305 , 2016. 5\\n[55] N. Shazeer, “Glu variants improve transformer,” arXiv preprint\\narXiv:2002.05202 , 2020. 5, 10', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='arXiv:1606.01305 , 2016. 5\\n[55] N. Shazeer, “Glu variants improve transformer,” arXiv preprint\\narXiv:2002.05202 , 2020. 5, 10\\n[56] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling\\nwith gated convolutional networks,” in International conference on\\nmachine learning . PMLR, 2017, pp. 933–941. 5\\n[57] B. Zhang and R. Sennrich, “Root mean square layer normalization,”\\nAdvances in Neural Information Processing Systems , vol. 32, 2019. 5,\\n10\\n[58] A. Baevski and M. Auli, “Adaptive input representations for neural\\nlanguage modeling,” arXiv preprint arXiv:1809.10853 , 2018. 5\\n[59] S. Shleifer, J. Weston, and M. Ott, “Normformer: Improved\\ntransformer pretraining with extra normalization,” arXiv preprint\\narXiv:2110.09456 , 2021. 5\\n[60] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei,\\n“Deepnet: Scaling transformers to 1,000 layers,” arXiv preprint\\narXiv:2203.00555 , 2022. 5\\n[61] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: Memory', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='arXiv:2203.00555 , 2022. 5\\n[61] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: Memory\\noptimizations toward training trillion parameter models,” in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis . IEEE, 2020, pp. 1–16. 5, 23\\n[62] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-\\nzaro, “Megatron-lm: Training multi-billion parameter language models\\nusing model parallelism,” arXiv preprint arXiv:1909.08053 , 2019. 5\\n[63] “\"bmtrain: Efficient training for big models.\".” [Online]. Available:\\nhttps://github.com/OpenBMB/BMTrain 5\\n[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\\nP. Cistac, T. Rault, R. Louf, M. Funtowicz et al. , “Transformers:\\nState-of-the-art natural language processing,” in Proceedings of the\\n2020 conference on empirical methods in natural language processing:\\nsystem demonstrations , 2020, pp. 38–45. 5\\n[65] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He, “Deepspeed: Sys-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='system demonstrations , 2020, pp. 38–45. 5\\n[65] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He, “Deepspeed: Sys-\\ntem optimizations enable training deep learning models with over\\n100 billion parameters,” in Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining ,\\n2020, pp. 3505–3506. 5\\n[66] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary,\\nD. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-\\nMilne et al. , “Jax: composable transformations of python+ numpy\\nprograms,” 2018. 5\\n[67] S. Li, J. Fang, Z. Bian, H. Liu, Y . Liu, H. Huang, B. Wang, and\\nY . You, “Colossal-ai: A unified deep learning system for large-scale\\nparallel training,” arXiv preprint arXiv:2110.14883 , 2021. 5\\n[68] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, “Fastmoe: A fast\\nmixture-of-expert training system,” arXiv preprint arXiv:2103.13262 ,\\n2021. 5\\n[69] L. Huawei Technologies Co., “Huawei mindspore ai development', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='mixture-of-expert training system,” arXiv preprint arXiv:2103.13262 ,\\n2021. 5\\n[69] L. Huawei Technologies Co., “Huawei mindspore ai development\\nframework,” in Artificial Intelligence Technology . Springer, 2022, pp.\\n137–162. 5\\n[70] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\\nimperative style, high-performance deep learning library,” Advances\\nin neural information processing systems , vol. 32, 2019. 5\\n[71] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard et al. , “Tensorflow: a system for\\nlarge-scale machine learning.” in Osdi , vol. 16, no. 2016. Savannah,\\nGA, USA, 2016, pp. 265–283. 5\\n[72] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,\\nC. Zhang, and Z. Zhang, “Mxnet: A flexible and efficient machine\\nlearning library for heterogeneous distributed systems,” arXiv preprint\\narXiv:1512.01274 , 2015. 5', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='learning library for heterogeneous distributed systems,” arXiv preprint\\narXiv:1512.01274 , 2015. 5\\n[73] P. J. Liu*, M. Saleh*, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and\\nN. Shazeer, “Generating wikipedia by summarizing long sequences,”\\ninInternational Conference on Learning Representations , 2018.\\n[Online]. Available: https://openreview.net/forum?id=Hyg0vbWC- 6\\n[74] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, and C. Raffel, “What language model architecture and\\npretraining objective works best for zero-shot generalization?” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n22 964–22 984. 6\\n[75] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\\nnatural language understanding and generation,” Advances in neural\\ninformation processing systems , vol. 32, 2019. 6\\n[76] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='information processing systems , vol. 32, 2019. 6\\n[76] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt\\nunderstands, too,” arXiv preprint arXiv:2103.10385 , 2021. 7\\n[77] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer\\nlearning for nlp,” in International Conference on Machine Learning .\\nPMLR, 2019, pp. 2790–2799. 7\\n[78] A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,\\nA. Jones, N. Joseph, B. Mann, N. DasSarma et al. , “A general\\nlanguage assistant as a laboratory for alignment,” arXiv preprint\\narXiv:2112.00861 , 2021. 7, 23\\n[79] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,\\nD. Amodei, P. Christiano, and G. Irving, “Fine-tuning language models\\nfrom human preferences,” arXiv preprint arXiv:1909.08593 , 2019. 7,\\n23\\n[80] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='from human preferences,” arXiv preprint arXiv:1909.08593 , 2019. 7,\\n23\\n[80] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\\narXiv:2301.00234 , 2022. 7\\n[81] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\\nmodels: A survey,” arXiv preprint arXiv:2212.10403 , 2022. 7\\n[82] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,\\n“Language models are unsupervised multitask learners,” OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019. 8, 9\\n[83] S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team, “An empirical\\nmodel of large-batch training,” arXiv preprint arXiv:1812.06162 , 2018.\\n8\\n[84] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang et al. , “Pangu- α: Large-scale autoregressive\\npretrained chinese language models with auto-parallel computation,”\\narXiv preprint arXiv:2104.12369 , 2021. 8, 15, 20, 23', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='pretrained chinese language models with auto-parallel computation,”\\narXiv preprint arXiv:2104.12369 , 2021. 8, 15, 20, 23\\n[85] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y . Cen, X. Zou, Z. Yang,\\nand J. Tang, “Wudaocorpora: A super large-scale chinese corpora for\\npre-training language models,” AI Open , vol. 2, pp. 65–68, 2021. 8,\\n21, 22\\n[86] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu et al. , “Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation,” arXiv preprint\\narXiv:2107.02137 , 2021. 8, 15', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 25}), Document(page_content='JOURNAL OF L ATEX 27\\n[87] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\\n“Transformer-xl: Attentive language models beyond a fixed-length\\ncontext,” arXiv preprint arXiv:1901.02860 , 2019. 8\\n[88] O. Lieber, O. Sharir, B. Lenz, and Y . Shoham, “Jurassic-1: Technical\\ndetails and evaluation,” White Paper. AI21 Labs , vol. 1, 2021. 8, 9, 15,\\n23\\n[89] Y . Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua, “Limits to\\ndepth efficiencies of self-attention,” Advances in Neural Information\\nProcessing Systems , vol. 33, pp. 22 640–22 651, 2020. 8\\n[90] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo et al. , “What changes can large-scale language\\nmodels bring? intensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers,” arXiv preprint arXiv:2109.04650 ,\\n2021. 8, 15\\n[91] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='2021. 8, 15\\n[91] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu,\\nJ. Luo, L. Xu et al. , “Yuan 1.0: Large-scale pre-trained language model\\nin zero-shot and few-shot learning,” arXiv preprint arXiv:2110.04725 ,\\n2021. 8, 15, 23\\n[92] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al. , “Scaling language\\nmodels: Methods, analysis & insights from training gopher,” arXiv\\npreprint arXiv:2112.11446 , 2021. 9, 15, 21, 22\\n[93] S. Wang, Y . Sun, Y . Xiang, Z. Wu, S. Ding, W. Gong, S. Feng,\\nJ. Shang, Y . Zhao, C. Pang et al. , “Ernie 3.0 titan: Exploring larger-\\nscale knowledge enhanced pre-training for language understanding and\\ngeneration,” arXiv preprint arXiv:2112.12731 , 2021. 9, 15, 23\\n[94] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold-\\ning, H. He, C. Leahy, K. McDonell, J. Phang et al. , “Gpt-neox-\\n20b: An open-source autoregressive language model,” arXiv preprint', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='ing, H. He, C. Leahy, K. McDonell, J. Phang et al. , “Gpt-neox-\\n20b: An open-source autoregressive language model,” arXiv preprint\\narXiv:2204.06745 , 2022. 9, 20, 23, 24\\n[95] W. Ben and K. Aran, “Gpt-j-6b: A 6 billion parameter autoregressive\\nlanguage model,” 2021. 9\\n[96] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al. , “Mixed\\nprecision training,” arXiv preprint arXiv:1710.03740 , 2017. 9\\n[97] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scaling of\\nlanguage models with mixture-of-experts,” in International Conference\\non Machine Learning . PMLR, 2022, pp. 5547–5569. 9, 15, 20\\n[98] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017. 9,\\n20', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='and J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017. 9,\\n20\\n[99] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,” The\\nJournal of Machine Learning Research , vol. 23, no. 1, pp. 5232–5270,\\n2022. 9\\n[100] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al. ,\\n“Training compute-optimal large language models,” arXiv preprint\\narXiv:2203.15556 , 2022. 9, 11, 15, 22\\n[101] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al. ,\\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,” arXiv preprint arXiv:2208.01448 , 2022. 9, 15, 20,\\n23, 24\\n[102] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='seq2seq model,” arXiv preprint arXiv:2208.01448 , 2022. 9, 15, 20,\\n23, 24\\n[102] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia et al. , “Glm-130b: An open bilingual pre-trained\\nmodel,” arXiv preprint arXiv:2210.02414 , 2022. 10, 15, 20, 23\\n[103] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang,\\n“Glm: General language model pretraining with autoregressive blank\\ninfilling,” in Proceedings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers) , 2022, pp. 320–\\n335. 10\\n[104] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al. , “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971 , 2023. 10, 15, 20\\n[105] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaudhary, F. Guzmán,\\nA. Joulin, and E. Grave, “Ccnet: Extracting high quality monolingual', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='[105] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaudhary, F. Guzmán,\\nA. Joulin, and E. Grave, “Ccnet: Extracting high quality monolingual\\ndatasets from web crawl data,” arXiv preprint arXiv:1911.00359 , 2019.\\n10\\n[106] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov et al. , “Pangu-P: Towards trillion\\nparameter language model with sparse heterogeneous computing,”\\narXiv preprint arXiv:2303.10845 , 2023. 10, 11, 15, 20, 23[107] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,\\nS. Savarese, and C. Xiong, “Codegen: An open large language\\nmodel for code with multi-turn program synthesis,” arXiv preprint\\narXiv:2203.13474 , 2022. 10, 15, 20, 22\\n[108] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\\nH. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large\\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374 ,\\n2021. 10, 15', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='language models trained on code,” arXiv preprint arXiv:2107.03374 ,\\n2021. 10, 15\\n[109] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al. , “Competition-\\nlevel code generation with alphacode,” Science , vol. 378, no. 6624, pp.\\n1092–1097, 2022. 10, 15, 20, 22\\n[110] N. Shazeer, “Fast transformer decoding: One write-head is all you\\nneed,” arXiv preprint arXiv:1911.02150 , 2019. 10\\n[111] R. Y . Pang and H. He, “Text generation by learning from demonstra-\\ntions,” arXiv preprint arXiv:2009.07839 , 2020. 10\\n[112] R. Dabre and A. Fujita, “Softmax tempering for training neural\\nmachine translation models,” arXiv preprint arXiv:2009.09372 , 2020.\\n10\\n[113] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,\\n“Codet5+: Open code large language models for code understanding\\nand generation,” arXiv preprint arXiv:2305.07922 , 2023. 10, 15, 24\\n[114] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='and generation,” arXiv preprint arXiv:2305.07922 , 2023. 10, 15, 24\\n[114] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware\\nunified pre-trained encoder-decoder models for code understanding and\\ngeneration,” arXiv preprint arXiv:2109.00859 , 2021. 10\\n[115] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al. , “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161 , 2023. 11, 15\\n[116] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V . Kerkez, and R. Stojnic, “Galactica: A large\\nlanguage model for science,” arXiv preprint arXiv:2211.09085 , 2022.\\n11, 15, 22, 23\\n[117] FairScale authors, “Fairscale: A general purpose modular pytorch\\nlibrary for high performance and large scale training,” https://github.\\ncom/facebookresearch/fairscale, 2021. 11\\n[118] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='com/facebookresearch/fairscale, 2021. 11\\n[118] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y . Du et al. , “Lamda: Language models\\nfor dialog applications,” arXiv preprint arXiv:2201.08239 , 2022. 11,\\n15, 22\\n[119] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al. , “Improving\\nalignment of dialogue agents via targeted human judgements,” arXiv\\npreprint arXiv:2209.14375 , 2022. 11, 15\\n[120] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\\nforcement learning,” in International conference on machine learning .\\nPMLR, 2016, pp. 1928–1937. 11\\n[121] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large\\nlanguage model for finance,” arXiv preprint arXiv:2303.17564 , 2023.\\n11, 15, 22', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large\\nlanguage model for finance,” arXiv preprint arXiv:2303.17564 , 2023.\\n11, 15, 22\\n[122] Y . Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua, “Limits to\\ndepth efficiencies of self-attention,” Advances in Neural Information\\nProcessing Systems , vol. 33, pp. 22 640–22 651, 2020. 11\\n[123] X. Zhang, Q. Yang, and D. Xu, “Xuanyuan 2.0: A large chinese\\nfinancial chat model with hundreds of billions parameters,” arXiv\\npreprint arXiv:2305.12002 , 2023. 11, 15\\n[124] W. Ben, “Mesh-transformer-jax: Model-parallel implementation of\\ntransformer language model with jax,” 2021. 12, 20\\n[125] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold-\\ning, H. He, C. Leahy, K. McDonell, J. Phang et al. , “Gpt-neox-\\n20b: An open-source autoregressive language model,” arXiv preprint\\narXiv:2204.06745 , 2022. 15\\n[126] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='arXiv:2204.06745 , 2022. 15\\n[126] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al. , “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332 , 2021. 11, 15, 23\\n[127] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf\\net al. , “Crosslingual generalization through multitask finetuning,” arXiv\\npreprint arXiv:2211.01786 , 2022. 15, 23\\n[128] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nand D. Jiang, “Wizardcoder: Empowering code large language models\\nwith evol-instruct,” arXiv preprint arXiv:2306.08568 , 2023. 14, 15\\n[129] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190 ,\\n2019. 11, 13, 23', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 26}), Document(page_content='JOURNAL OF L ATEX 28\\n[130] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958 , 2021. 13,\\n18, 21, 22, 23\\n[131] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\\ngenerated instructions,” arXiv preprint arXiv:2212.10560 , 2022. 14\\n[132] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, and K.-W. Chang,\\n“Dynosaur: A dynamic growth paradigm for instruction-tuning data\\ncuration,” arXiv preprint arXiv:2305.14327 , 2023. 14\\n[133] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue et al. , “Llama-adapter v2: Parameter-efficient visual\\ninstruction model,” arXiv preprint arXiv:2304.15010 , 2023. 14, 24\\n[134] “Openai. gpt-4 technical report,” 2023. 14\\n[135] T. Liu and B. K. H. Low, “Goat: Fine-tuned llama outperforms gpt-4\\non arithmetic tasks,” arXiv preprint arXiv:2305.14201 , 2023. 14', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='[135] T. Liu and B. K. H. Low, “Goat: Fine-tuned llama outperforms gpt-4\\non arithmetic tasks,” arXiv preprint arXiv:2305.14201 , 2023. 14\\n[136] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, “Huatuo:\\nTuning llama model with chinese medical knowledge,” arXiv preprint\\narXiv:2304.06975 , 2023. 14\\n[137] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and\\nD. Jiang, “Wizardlm: Empowering large language models to follow\\ncomplex instructions,” arXiv preprint arXiv:2304.12244 , 2023. 14, 15\\n[138] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,\\nA. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et al. , “Constitutional\\nai: Harmlessness from ai feedback,” arXiv preprint arXiv:2212.08073 ,\\n2022. 15\\n[139] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang,\\nand C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047 , 2023. 15', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='els from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047 , 2023. 15\\n[140] B. Zhang and H. Soh, “Large language models as zero-shot human\\nmodels for human-robot interaction,” arXiv preprint arXiv:2303.03548 ,\\n2023. 15, 16\\n[141] A. Lykov and D. Tsetserukou, “Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model,” arXiv preprint\\narXiv:2305.19352 , 2023. 15\\n[142] E. Billing, J. Rosén, and M. Lamb, “Language models for human-robot\\ninteraction,” in ACM/IEEE International Conference on Human-Robot\\nInteraction, March 13–16, 2023, Stockholm, Sweden . ACM Digital\\nLibrary, 2023, pp. 905–906. 15\\n[143] Y . Ye, H. You, and J. Du, “Improved trust in human-robot collaboration\\nwith chatgpt,” IEEE Access , 2023. 15\\n[144] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,\\nD. Fox, J. Thomason, and A. Garg, “Progprompt: Generating situated\\nrobot task plans using large language models,” in 2023 IEEE Interna-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='D. Fox, J. Thomason, and A. Garg, “Progprompt: Generating situated\\nrobot task plans using large language models,” in 2023 IEEE Interna-\\ntional Conference on Robotics and Automation (ICRA) . IEEE, 2023,\\npp. 11 523–11 530. 15\\n[145] Y . Zhen, S. Bi, L. Xing-tong, P. Wei-qin, S. Hai-peng, C. Zi-rui,\\nand F. Yi-shu, “Robot task planning based on large language model\\nrepresenting knowledge with directed graph structures,” arXiv preprint\\narXiv:2306.05171 , 2023. 15\\n[146] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models\\nas zero-shot planners: Extracting actionable knowledge for embodied\\nagents,” in International Conference on Machine Learning . PMLR,\\n2022, pp. 9118–9147. 15\\n[147] Y . Ding, X. Zhang, C. Paxton, and S. Zhang, “Task and motion planning\\nwith large language models for object rearrangement,” arXiv preprint\\narXiv:2303.06247 , 2023. 15\\n[148] ——, “Leveraging commonsense knowledge from large language mod-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='arXiv:2303.06247 , 2023. 15\\n[148] ——, “Leveraging commonsense knowledge from large language mod-\\nels for task and motion planning,” in RSS 2023 Workshop on Learning\\nfor Task and Motion Planning , 2023. 15\\n[149] Y . Ge, W. Hua, J. Ji, J. Tan, S. Xu, and Y . Zhang, “Openagi: When llm\\nmeets domain experts,” arXiv preprint arXiv:2304.04370 , 2023. 15\\n[150] T. Zhong, Y . Wei, L. Yang, Z. Wu, Z. Liu, X. Wei, W. Li, J. Yao,\\nC. Ma, X. Li et al. , “Chatabl: Abductive learning via natural language\\ninteraction with chatgpt,” arXiv preprint arXiv:2304.11107 , 2023. 15\\n[151] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, and T. Funkhouser, “Tidybot: Personalized robot as-\\nsistance with large language models,” arXiv preprint arXiv:2305.05658 ,\\n2023. 16\\n[152] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu et al. , “Palm-e: An embodied\\nmultimodal language model,” arXiv preprint arXiv:2303.03378 , 2023.\\n16', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='A. Wahid, J. Tompson, Q. Vuong, T. Yu et al. , “Palm-e: An embodied\\nmultimodal language model,” arXiv preprint arXiv:2303.03378 , 2023.\\n16\\n[153] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, and brian ichter, “Inner\\nmonologue: Embodied reasoning through planning with languagemodels,” in 6th Annual Conference on Robot Learning , 2022.\\n[Online]. Available: https://openreview.net/forum?id=3R3Pz5i0tye 16\\n[154] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830 , 2019. 16, 21, 22, 23\\n[155] Y . Bisk, R. Zellers, J. Gao, Y . Choi et al. , “Piqa: Reasoning about\\nphysical commonsense in natural language,” in Proceedings of the\\nAAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp.\\n7432–7439. 16, 21, 22', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='AAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp.\\n7432–7439. 16, 21, 22\\n[156] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551 , 2017. 16, 21, 22, 23\\n[157] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, and R. Fernández, “The lambada\\ndataset: Word prediction requiring a broad discourse context,” arXiv\\npreprint arXiv:1606.06031 , 2016. 18, 21, 22\\n[158] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande:\\nAn adversarial winograd schema challenge at scale,” Communications\\nof the ACM , vol. 64, no. 9, pp. 99–106, 2021. 18, 21, 22, 23\\n[159] H. Levesque, E. Davis, and L. Morgenstern, “The winograd schema\\nchallenge,” in Thirteenth international conference on the principles of\\nknowledge representation and reasoning , 2012. 18, 19, 21, 22, 23', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='challenge,” in Thirteenth international conference on the principles of\\nknowledge representation and reasoning , 2012. 18, 19, 21, 22, 23\\n[160] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020. 18, 21, 22, 23\\n[161] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\\n“Glue: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,” arXiv preprint arXiv:1804.07461 , 2018. 18,\\n20, 21\\n[162] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, and J. Allen, “A corpus and evaluation framework\\nfor deeper understanding of commonsense stories,” arXiv preprint\\narXiv:1604.01696 , 2016. 18, 21, 22, 23\\n[163] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='[163] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\\nyes/no questions,” arXiv preprint arXiv:1905.10044 , 2019. 18, 21, 22\\n[164] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, “Race: Large-scale\\nreading comprehension dataset from examinations,” arXiv preprint\\narXiv:1704.04683 , 2017. 18, 20, 21, 22\\n[165] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela,\\n“Adversarial nli: A new benchmark for natural language understand-\\ning,” arXiv preprint arXiv:1910.14599 , 2019. 18, 21, 22, 23\\n[166] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\\n18, 19, 21, 22, 23\\n[167] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, and V . Stoyanov, “Xnli: Evaluating cross-lingual sentence', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='[167] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, and V . Stoyanov, “Xnli: Evaluating cross-lingual sentence\\nrepresentations,” arXiv preprint arXiv:1809.05053 , 2018. 18, 21, 22\\n[168] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage\\nchallenge corpus for sentence understanding through inference,” in\\nProceedings of the 2018 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers) . New Orleans, Louisiana:\\nAssociation for Computational Linguistics, Jun. 2018, pp. 1112–1122.\\n[Online]. Available: https://aclanthology.org/N18-1101 18\\n[169] Y . Yang, Y . Zhang, C. Tar, and J. Baldridge, “Paws-x: A cross-\\nlingual adversarial dataset for paraphrase identification,” arXiv preprint\\narXiv:1908.11828 , 2019. 18, 21, 22\\n[170] Y . Zhang, J. Baldridge, and L. He, “PAWS: Paraphrase adversaries\\nfrom word scrambling,” in Proceedings of the 2019 Conference of', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='[170] Y . Zhang, J. Baldridge, and L. He, “PAWS: Paraphrase adversaries\\nfrom word scrambling,” in Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short\\nPapers) . Minneapolis, Minnesota: Association for Computational\\nLinguistics, Jun. 2019, pp. 1298–1308. [Online]. Available: https:\\n//aclanthology.org/N19-1131 18\\n[171] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational\\nquestion answering challenge,” Transactions of the Association for\\nComputational Linguistics , vol. 7, pp. 249–266, 2019. 19, 22\\n[172] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner,\\n“Drop: A reading comprehension benchmark requiring discrete rea-\\nsoning over paragraphs,” arXiv preprint arXiv:1903.00161 , 2019. 19,\\n22\\n[173] I. Dagan, O. Glickman, and B. Magnini, “The pascal recognising tex-\\ntual entailment challenge,” in Machine learning challenges workshop .', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='22\\n[173] I. Dagan, O. Glickman, and B. Magnini, “The pascal recognising tex-\\ntual entailment challenge,” in Machine learning challenges workshop .\\nSpringer, 2005, pp. 177–190. 19, 21, 22, 23', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 27}), Document(page_content='JOURNAL OF L ATEX 29\\n[174] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al. , “Beyond\\nthe imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models,” arXiv preprint arXiv:2206.04615 , 2022. 19, 21, 22,\\n23\\n[175] P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know:\\nUnanswerable questions for squad,” arXiv preprint arXiv:1806.03822 ,\\n2018. 19, 22\\n[176] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250 , 2016. 19, 21\\n[177] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al. , “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168 , 2021.\\n19, 22\\n[178] M. T. Pilehvar and J. Camacho-Collados, “Wic: 10,000 example\\npairs for evaluating context-sensitive representations,” arXiv preprint', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='19, 22\\n[178] M. T. Pilehvar and J. Camacho-Collados, “Wic: 10,000 example\\npairs for evaluating context-sensitive representations,” arXiv preprint\\narXiv:1808.09121 , vol. 6, 2018. 19, 21, 22, 23\\n[179] Y . Wang, X. Liu, and S. Shi, “Deep neural solver for math word\\nproblems,” in Proceedings of the 2017 conference on empirical methods\\nin natural language processing , 2017, pp. 845–854. 19, 21\\n[180] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, and B. Tang,\\n“Lcqmc: A large-scale chinese question matching corpus,” in Proceed-\\nings of the 27th international conference on computational linguistics ,\\n2018, pp. 1952–1962. 19, 21\\n[181] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song et al. , “Measuring coding\\nchallenge competence with apps,” arXiv preprint arXiv:2105.09938 ,\\n2021. 19, 21, 22\\n[182] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas, “Ethos: an', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='2021. 19, 21, 22\\n[182] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas, “Ethos: an\\nonline hate speech detection dataset,” arXiv preprint arXiv:2006.08328 ,\\n2020. 19, 21, 22\\n[183] M. Nadeem, A. Bethke, and S. Reddy, “Stereoset: Measuring\\nstereotypical bias in pretrained language models,” arXiv preprint\\narXiv:2004.09456 , 2020. 19, 21, 22\\n[184] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\\nH. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large\\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374 ,\\n2021. 19, 21, 22, 23\\n[185] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, “We-\\nbqa: Multihop and multimodal qa,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2022, pp.\\n16 495–16 504. 19, 21\\n[186] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu,\\n“A span-extraction dataset for chinese machine reading comprehen-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='[186] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu,\\n“A span-extraction dataset for chinese machine reading comprehen-\\nsion,” arXiv preprint arXiv:1810.07366 , 2018. 19, 21\\n[187] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843 , 2016. 19, 21, 22\\n[188] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\\n“Compressive transformers for long-range sequence modelling,” arXiv\\npreprint arXiv:1911.05507 , 2019. 19, 21, 22\\n[189] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang, and\\nL. Zettlemoyer, “Quac: Question answering in context,” arXiv preprint\\narXiv:1808.07036 , 2018. 19, 22\\n[190] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli ´c, and A. Korho-\\nnen, “Xcopa: A multilingual dataset for causal commonsense reason-\\ning,” arXiv preprint arXiv:2005.00333 , 2020. 19, 22\\n[191] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='ing,” arXiv preprint arXiv:2005.00333 , 2020. 19, 22\\n[191] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics , vol. 9, pp. 346–361, 2021. 20, 22\\n[192] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937 , 2018. 20, 22\\n[193] S. Iyer, N. Dandekar, and K. Csernai, “First quora\\ndataset release: Question pairs,” https://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs. 21\\n[194] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage\\nchallenge corpus for sentence understanding through inference,” arXiv\\npreprint arXiv:1704.05426 , 2017. 21\\n[195] M.-C. De Marneffe, M. Simons, and J. Tonhauser, “The commit-\\nmentbank: Investigating projection in naturally occurring discourse,”', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='[195] M.-C. De Marneffe, M. Simons, and J. Tonhauser, “The commit-\\nmentbank: Investigating projection in naturally occurring discourse,”\\ninproceedings of Sinn und Bedeutung , vol. 23, no. 2, 2019, pp. 107–\\n124. 21, 22, 23\\n[196] O. Bojar, R. Chatterjee, C. Federmann, Y . Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V . Logacheva, C. Monz et al. , “Find-\\nings of the 2016 conference on machine translation,” in Proceedings ofthe First Conference on Machine Translation: Volume 2, Shared Task\\nPapers , 2016, pp. 131–198. 21\\n[197] X. Pan, B. Zhang, J. May, J. Nothman, K. Knight, and H. Ji, “Cross-\\nlingual name tagging and linking for 282 languages,” in Proceedings\\nof the 55th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , 2017, pp. 1946–1958. 21\\n[198] P. Lewis, B. O ˘guz, R. Rinott, S. Riedel, and H. Schwenk, “Mlqa:\\nEvaluating cross-lingual extractive question answering,” arXiv preprint\\narXiv:1910.07475 , 2019. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='Evaluating cross-lingual extractive question answering,” arXiv preprint\\narXiv:1910.07475 , 2019. 21\\n[199] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V . Niko-\\nlaev, and J. Palomaki, “Tydi qa: A benchmark for information-seeking\\nquestion answering in typologically diverse languages,” Transactions\\nof the Association for Computational Linguistics , vol. 8, pp. 454–470,\\n2020. 21, 22, 23\\n[200] W. Li, F. Qi, M. Sun, X. Yi, and J. Zhang, “Ccpm: A chinese classical\\npoetry matching dataset,” arXiv preprint arXiv:2106.01979 , 2021. 21\\n[201] K. Sun, D. Yu, D. Yu, and C. Cardie, “Investigating prior knowledge\\nfor challenging chinese machine reading comprehension,” Transactions\\nof the Association for Computational Linguistics , vol. 8, pp. 141–155,\\n2020. 21\\n[202] B. Loïc, B. Magdalena, B. Ond ˇrej, F. Christian, G. Yvette, G. Roman,\\nH. Barry, H. Matthias, J. Eric, K. Tom et al. , “Findings of the\\n2020 conference on machine translation (wmt20),” in Proceedings', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='H. Barry, H. Matthias, J. Eric, K. Tom et al. , “Findings of the\\n2020 conference on machine translation (wmt20),” in Proceedings\\nof the Fifth Conference on Machine Translation . Association for\\nComputational Linguistics„ 2020, pp. 1–55. 21\\n[203] B. Hu, Q. Chen, and F. Zhu, “Lcsts: A large scale chinese short text\\nsummarization dataset,” arXiv preprint arXiv:1506.05865 , 2015. 21\\n[204] Z. Shao, M. Huang, J. Wen, W. Xu, and X. Zhu, “Long and diverse text\\ngeneration with planning-based hierarchical variational model,” arXiv\\npreprint arXiv:1908.06605 , 2019. 21\\n[205] Y . Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie et al. , “Cuge: A chinese language understanding and\\ngeneration evaluation benchmark,” arXiv preprint arXiv:2112.13610 ,\\n2021. 21\\n[206] Y . Li, T. Liu, D. Li, Q. Li, J. Shi, and Y . Wang, “Character-based\\nbilstm-crf incorporating pos and dictionaries for chinese opinion target\\nextraction,” in Asian Conference on Machine Learning . PMLR, 2018,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='bilstm-crf incorporating pos and dictionaries for chinese opinion target\\nextraction,” in Asian Conference on Machine Learning . PMLR, 2018,\\npp. 518–533. 21\\n[207] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y . Li, Y . Xu, K. Sun, D. Yu,\\nC. Yu et al. , “Clue: A chinese language understanding evaluation\\nbenchmark,” arXiv preprint arXiv:2004.05986 , 2020. 21, 22\\n[208] Z. Li, N. Ding, Z. Liu, H. Zheng, and Y . Shen, “Chinese relation extrac-\\ntion with multi-grained information and external linguistic knowledge,”\\ninProceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics , 2019, pp. 4377–4386. 21\\n[209] J. Xu, J. Wen, X. Sun, and Q. Su, “A discourse-level named entity\\nrecognition and relation extraction dataset for chinese literature text,”\\narXiv preprint arXiv:1711.07010 , 2017. 21\\n[210] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, and B. Tang, “The bq corpus:\\nA large-scale domain-specific chinese corpus for sentence semantic', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='[210] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, and B. Tang, “The bq corpus:\\nA large-scale domain-specific chinese corpus for sentence semantic\\nequivalence identification,” in Proceedings of the 2018 conference on\\nempirical methods in natural language processing , 2018, pp. 4946–\\n4951. 21\\n[211] L. CO, “Iflytek: a multiple categories chinese text classifier. competi-\\ntion official website,” 2019. 21\\n[212] B. Liu, D. Niu, H. Wei, J. Lin, Y . He, K. Lai, and Y . Xu, “Matching\\narticle pairs with graphical decomposition and convolutions,” arXiv\\npreprint arXiv:1802.07459 , 2018. 21\\n[213] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, and Z. Ding, “Chinese\\nmedical question answer matching using end-to-end character-level\\nmulti-scale cnns,” Applied Sciences , vol. 7, no. 8, p. 767, 2017. 21\\n[214] S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu, “Multi-scale\\nattentive interaction networks for chinese medical question answer\\nselection,” IEEE Access , vol. 6, pp. 74 061–74 071, 2018. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='attentive interaction networks for chinese medical question answer\\nselection,” IEEE Access , vol. 6, pp. 74 061–74 071, 2018. 21\\n[215] P. Li, W. Li, Z. He, X. Wang, Y . Cao, J. Zhou, and W. Xu, “Dataset\\nand neural recurrent sequence labeling model for open-domain factoid\\nquestion answering,” arXiv preprint arXiv:1607.06275 , 2016. 21\\n[216] N. Peng and M. Dredze, “Named entity recognition for chinese social\\nmedia with jointly trained embeddings,” in Proceedings of the 2015\\nconference on empirical methods in natural language processing , 2015,\\npp. 548–554. 21\\n[217] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin et al. , “Ontonotes\\nrelease 4.0,” LDC2011T03, Philadelphia, Penn.: Linguistic Data Con-\\nsortium , 2011. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 28}), Document(page_content='JOURNAL OF L ATEX 30\\n[218] Y . Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, and G. Hu,\\n“A sentence cloze dataset for chinese machine reading comprehension,”\\narXiv preprint arXiv:2004.03116 , 2020. 21\\n[219] C. C. Shao, T. Liu, Y . Lai, Y . Tseng, and S. Tsai, “Drcd: A\\nchinese machine reading comprehension dataset,” arXiv preprint\\narXiv:1806.00920 , 2018. 21\\n[220] W. He, K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu, Y . Wang,\\nH. Wu, Q. She et al. , “Dureader: a chinese machine reading\\ncomprehension dataset from real-world applications,” arXiv preprint\\narXiv:1711.05073 , 2017. 21\\n[221] H. Tang, J. Liu, H. Li, Y . Hong, H. Wu, and H. Wang, “Dureaderrobust:\\nA chinese dataset towards evaluating the robustness of machine reading\\ncomprehension models,” arXiv preprint arXiv:2004.11142 , 2020. 21\\n[222] C. Zheng, M. Huang, and A. Sun, “Chid: A large-scale chinese idiom\\ndataset for cloze test,” arXiv preprint arXiv:1906.01265 , 2019. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='[222] C. Zheng, M. Huang, and A. Sun, “Chid: A large-scale chinese idiom\\ndataset for cloze test,” arXiv preprint arXiv:1906.01265 , 2019. 21\\n[223] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y . Feng, X. Han,\\nZ. Hu, H. Wang et al. , “Cail2018: A large-scale legal dataset for\\njudgment prediction,” arXiv preprint arXiv:1807.02478 , 2018. 21\\n[224] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, and F. Wei, “Blow the dog\\nwhistle: A chinese dataset for cant understanding with common sense\\nand world knowledge,” arXiv preprint arXiv:2104.02704 , 2021. 21\\n[225] C. Xiong, Z. Dai, J. Callan, Z. Liu, and R. Power, “End-to-end\\nneural ad-hoc ranking with kernel pooling,” in Proceedings of the 40th\\nInternational ACM SIGIR conference on research and development in\\ninformation retrieval , 2017, pp. 55–64. 21\\n[226] C. Xu, J. Pei, H. Wu, Y . Liu, and C. Li, “Matinf: A jointly labeled large-\\nscale dataset for classification, question answering and summarization,”', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='scale dataset for classification, question answering and summarization,”\\narXiv preprint arXiv:2004.12302 , 2020. 21\\n[227] H. Zhou, C. Zheng, K. Huang, M. Huang, and X. Zhu, “Kdconv: A\\nchinese multi-domain dialogue dataset towards multi-turn knowledge-\\ndriven conversation,” arXiv preprint arXiv:2004.04100 , 2020. 21\\n[228] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima et al. , “The pile: An\\n800gb dataset of diverse text for language modeling,” arXiv preprint\\narXiv:2101.00027 , 2020. 21, 22\\n[229] S. Lim, M. Kim, and J. Lee, “Korquad1. 0: Korean qa dataset for\\nmachine reading comprehension,” arXiv preprint arXiv:1909.07005 ,\\n2019. 21\\n[230] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song,\\nJ. Kim, Y . Song, T. Oh et al. , “Klue: Korean language understanding\\nevaluation,” arXiv preprint arXiv:2105.09680 , 2021. 21\\n[231] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='evaluation,” arXiv preprint arXiv:2105.09680 , 2021. 21\\n[231] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin et al. , “Fewclue: A chinese few-shot learning evaluation\\nbenchmark,” arXiv preprint arXiv:2107.07498 , 2021. 21\\n[232] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al. , “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics , vol. 7, pp. 453–466,\\n2019. 21, 22\\n[233] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355 , 2018. 21\\n[234] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen,\\nC. Hansen, and J. G. Simonsen, “Multifc: A real-world multi-domain\\ndataset for evidence-based fact checking of claims,” arXiv preprint\\narXiv:1909.03242 , 2019. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='dataset for evidence-based fact checking of claims,” arXiv preprint\\narXiv:1909.03242 , 2019. 21\\n[235] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y . Choi, “Socialiqa:\\nCommonsense reasoning about social interactions,” arXiv preprint\\narXiv:1904.09728 , 2019. 21, 22\\n[236] S. Gehman, S. Gururangan, M. Sap, Y . Choi, and N. A. Smith,\\n“Realtoxicityprompts: Evaluating neural toxic degeneration in language\\nmodels,” arXiv preprint arXiv:2009.11462 , 2020. 21, 22\\n[237] S. L. Blodgett, L. Green, and B. O’Connor, “Demographic dialectal\\nvariation in social media: A case study of african-american english,”\\narXiv preprint arXiv:1608.08868 , 2016. 21\\n[238] D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman,\\n“Nuanced metrics for measuring unintended bias with real data for\\ntext classification,” in Companion proceedings of the 2019 world wide\\nweb conference , 2019, pp. 491–500. 21\\n[239] Y . Cui, T. Liu, Z. Chen, W. Ma, S. Wang, and G. Hu, “Dataset for', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='web conference , 2019, pp. 491–500. 21\\n[239] Y . Cui, T. Liu, Z. Chen, W. Ma, S. Wang, and G. Hu, “Dataset for\\nthe first evaluation on chinese machine reading comprehension,” arXiv\\npreprint arXiv:1709.08299 , 2017. 21\\n[240] D. Vilares and C. Gómez-Rodríguez, “Head-qa: A healthcare dataset\\nfor complex reasoning,” arXiv preprint arXiv:1906.04701 , 2019. 21\\n[241] J. Liu, L. Cui, H. Liu, D. Huang, Y . Wang, and Y . Zhang, “Logiqa:\\nA challenge dataset for machine reading comprehension with logical\\nreasoning,” arXiv preprint arXiv:2007.08124 , 2020. 21[242] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor\\nconduct electricity? a new dataset for open book question answering,”\\narXiv preprint arXiv:1809.02789 , 2018. 21, 22\\n[243] S. Aroca-Ouellette, C. Paik, A. Roncone, and K. Kann, “Prost: Phys-\\nical reasoning of objects through space and time,” arXiv preprint\\narXiv:2106.03634 , 2021. 21\\n[244] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, and R. Morante,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='arXiv:2106.03634 , 2021. 21\\n[244] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, and R. Morante,\\n“Qa4mre 2011-2013: Overview of question answering for machine\\nreading evaluation,” in Information Access Evaluation. Multilinguality,\\nMultimodality, and Visualization: 4th International Conference of the\\nCLEF Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013.\\nProceedings 4 . Springer, 2013, pp. 303–320. 21\\n[245] J. Welbl, N. F. Liu, and M. Gardner, “Crowdsourcing multiple choice\\nscience questions,” arXiv preprint arXiv:1707.06209 , 2017. 21\\n[246] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019. 21\\n[247] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn,\\n“The pushshift reddit dataset,” in Proceedings of the international AAAI\\nconference on web and social media , vol. 14, 2020, pp. 830–839. 21', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='“The pushshift reddit dataset,” in Proceedings of the international AAAI\\nconference on web and social media , vol. 14, 2020, pp. 830–839. 21\\n[248] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241 , 2018. 21, 22\\n[249] H. Rashkin, E. M. Smith, M. Li, and Y .-L. Boureau, “Towards\\nempathetic open-domain conversation models: A new benchmark and\\ndataset,” arXiv preprint arXiv:1811.00207 , 2018. 21\\n[250] E. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Ur-\\nbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe et al. , “The second\\nconversational intelligence challenge (convai2),” in The NeurIPS’18\\nCompetition: From Machine Learning to Intelligent Conversations .\\nSpringer, 2020, pp. 187–208. 21\\n[251] E. M. Smith, M. Williamson, K. Shuster, J. Weston, and Y .-L. Boureau,\\n“Can you put it all together: Evaluating conversational agents’ ability', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='[251] E. M. Smith, M. Williamson, K. Shuster, J. Weston, and Y .-L. Boureau,\\n“Can you put it all together: Evaluating conversational agents’ ability\\nto blend skills,” arXiv preprint arXiv:2004.08449 , 2020. 21\\n[252] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue\\ngeneration,” arXiv preprint arXiv:2107.07566 , 2021. 21\\n[253] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, “Crows-pairs:\\nA challenge dataset for measuring social biases in masked language\\nmodels,” arXiv preprint arXiv:2010.00133 , 2020. 21, 22\\n[254] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. V on Werra, C. Mou, E. González Ponferrada, H. Nguyen\\net al. , “The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset,” Advances in Neural Information Processing Systems , vol. 35,\\npp. 31 809–31 826, 2022. 22\\n[255] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='[255] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\\nwith the math dataset,” arXiv preprint arXiv:2103.03874 , 2021. 22\\n[256] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible\\nalternatives: An evaluation of commonsense causal reasoning.” in AAAI\\nspring symposium: logical formalizations of commonsense reasoning ,\\n2011, pp. 90–95. 22, 23\\n[257] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n“Looking beyond the surface: A challenge set for reading comprehen-\\nsion over multiple sentences,” in Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long Papers) ,\\n2018, pp. 252–262. 22\\n[258] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, and B. Van Durme, “Record:\\nBridging the gap between human and machine commonsense reading\\ncomprehension,” arXiv preprint arXiv:1810.12885 , 2018. 22', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='Bridging the gap between human and machine commonsense reading\\ncomprehension,” arXiv preprint arXiv:1810.12885 , 2018. 22\\n[259] T. H. Trinh and Q. V . Le, “A simple method for commonsense\\nreasoning,” arXiv preprint arXiv:1806.02847 , 2018. 22\\n[260] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner,\\nand Y . Choi, “Defending against neural fake news,” Advances in neural\\ninformation processing systems , vol. 32, 2019. 22\\n[261] R. T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons:\\nDiagnosing syntactic heuristics in natural language inference,” arXiv\\npreprint arXiv:1902.01007 , 2019. 22\\n[262] M. Mirzayanov, “Codeforces: Results of 2020,” https://codeforces.com/\\nblog/entry/89502. 22\\n[263] E. Caballero, . OpenAI, and I. Sutskever, “Description2Code Dataset,”\\n8 2016. [Online]. Available: https://github.com/ethancaballero/\\ndescription2code 22\\n[264] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V . Zolotov,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='description2code 22\\n[264] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V . Zolotov,\\nJ. Dolby, J. Chen, M. Choudhury, L. Decker et al. , “Codenet: A large-\\nscale ai for code dataset for learning a diversity of coding tasks,” arXiv\\npreprint arXiv:2105.12655 , 2021. 22', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 29}), Document(page_content='JOURNAL OF L ATEX 31\\n[265] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing , 2013,\\npp. 1533–1544. 22\\n[266] A. Patel, S. Bhattamishra, and N. Goyal, “Are nlp models really able to\\nsolve simple math word problems?” arXiv preprint arXiv:2103.07191 ,\\n2021. 22\\n[267] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Ha-\\njishirzi, “Mawps: A math word problem repository,” in Proceedings of\\nthe 2016 conference of the north american chapter of the association\\nfor computational linguistics: human language technologies , 2016, pp.\\n1152–1157. 22\\n[268] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, “Program induction\\nby rationale generation: Learning to solve and explain algebraic word\\nproblems,” arXiv preprint arXiv:1705.04146 , 2017. 22\\n[269] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Sta-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='problems,” arXiv preprint arXiv:1705.04146 , 2017. 22\\n[269] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Sta-\\niano, “Mlsum: The multilingual summarization corpus,” arXiv preprint\\narXiv:2004.14900 , 2020. 22\\n[270] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary!” Topic-Aware Convolutional Neural Networks for\\nExtreme Summarization. ArXiv, abs , 1808. 22\\n[271] J. Novikova, O. Dušek, and V . Rieser, “The e2e dataset: New challenges\\nfor end-to-end generation,” arXiv preprint arXiv:1706.09254 , 2017. 22\\n[272] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, and A. Shimorina, “The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg+ 2020),”\\ninProceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+) , 2020. 22\\n[273] N. Goyal, C. Gao, V . Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Kr-', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='Generation from the Semantic Web (WebNLG+) , 2020. 22\\n[273] N. Goyal, C. Gao, V . Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Kr-\\nishnan, M. Ranzato, F. Guzmán, and A. Fan, “The flores-101 evaluation\\nbenchmark for low-resource and multilingual machine translation,”\\nTransactions of the Association for Computational Linguistics , vol. 10,\\npp. 522–538, 2022. 22\\n[274] Y . Xia, X. Tan, F. Tian, F. Gao, W. Chen, Y . Fan, L. Gong, Y . Leng,\\nR. Luo, Y . Wang et al. , “Microsoft research asia’s systems for wmt19,”\\narXiv preprint arXiv:1911.06191 , 2019. 22\\n[275] A. Tikhonov and M. Ryabinin, “It’s all in the heads: Using attention\\nheads as a baseline for cross-lingual transfer in commonsense reason-\\ning,” arXiv preprint arXiv:2106.12066 , 2021. 22\\n[276] S. Roy and D. Roth, “Solving general arithmetic word problems,” arXiv\\npreprint arXiv:1608.01413 , 2016. 22\\n[277] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='preprint arXiv:1608.01413 , 2016. 22\\n[277] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving et al. ,\\n“Teaching language models to support answers with verified quotes,”\\narXiv preprint arXiv:2203.11147 , 2022. 22\\n[278] R. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme, “Gender\\nbias in coreference resolution,” arXiv preprint arXiv:1804.09301 , 2018.\\n22\\n[279] J. Zhao, T. Wang, M. Yatskar, V . Ordonez, and K.-W. Chang, “Gender\\nbias in coreference resolution: Evaluation and debiasing methods,”\\narXiv preprint arXiv:1804.06876 , 2018. 22\\n[280] A. Parrish, A. Chen, N. Nangia, V . Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, and S. R. Bowman, “Bbq: A hand-built bias benchmark\\nfor question answering,” arXiv preprint arXiv:2110.08193 , 2021. 22\\n[281] J. Boyd-Graber, B. Satinoff, H. He, and H. Daumé III, “Besting\\nthe quiz master: Crowdsourcing incremental classification games,”', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='[281] J. Boyd-Graber, B. Satinoff, H. He, and H. Daumé III, “Besting\\nthe quiz master: Crowdsourcing incremental classification games,”\\ninProceedings of the 2012 joint conference on empirical methods\\nin natural language processing and computational natural language\\nlearning , 2012, pp. 1290–1301. 22\\n[282] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. V osoughi, H. W.\\nChung, Y . Tay, S. Ruder, D. Zhou et al. , “Language models are multi-\\nlingual chain-of-thought reasoners,” arXiv preprint arXiv:2210.03057 ,\\n2022. 22, 23\\n[283] S.-Y . Miao, C.-C. Liang, and K.-Y . Su, “A diverse corpus for evaluating\\nand developing english math word problem solvers,” arXiv preprint\\narXiv:2106.15772 , 2021. 22\\n[284] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al. , “Program synthesis with large\\nlanguage models,” arXiv preprint arXiv:2108.07732 , 2021. 22, 23\\n[285] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='language models,” arXiv preprint arXiv:2108.07732 , 2021. 22, 23\\n[285] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” CoRR , vol. abs/1909.09436, 2019. 22\\n[286] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V . Le, and C. Sutton, “Program\\nsynthesis with large language models,” CoRR , vol. abs/2108.07732,\\n2021. 22[287] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis,\\nY . Jernite, M. Mitchell, S. Hughes, T. Wolf et al. , “The stack: 3 tb of\\npermissively licensed source code,” arXiv preprint arXiv:2211.15533 ,\\n2022. 22\\n[288] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-\\nt. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and\\nreliable benchmark for data science code generation,” in International\\nConference on Machine Learning . PMLR, 2023, pp. 18 319–18 345.\\n22, 23', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='reliable benchmark for data science code generation,” in International\\nConference on Machine Learning . PMLR, 2023, pp. 18 319–18 345.\\n22, 23\\n[289] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY . Zhang, D. Narayanan, Y . Wu, A. Kumar et al. , “Holistic evaluation\\nof language models,” arXiv preprint arXiv:2211.09110 , 2022. 22\\n[290] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap et al. ,\\n“Benchmarking generalization via in-context instructions on 1,600+\\nlanguage tasks,” arXiv preprint arXiv:2204.07705 , 2022. 23\\n[291] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-\\nS. Wu, M. Zhong, P. Yin, S. I. Wang et al. , “Unifiedskg: Unifying\\nand multi-tasking structured knowledge grounding with text-to-text\\nlanguage models,” arXiv preprint arXiv:2201.05966 , 2022. 23\\n[292] Q. Ye, B. Y . Lin, and X. Ren, “Crossfit: A few-shot learning challenge', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='language models,” arXiv preprint arXiv:2201.05966 , 2022. 23\\n[292] Q. Ye, B. Y . Lin, and X. Ren, “Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp,” arXiv preprint arXiv:2104.08835 ,\\n2021. 23\\n[293] V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V .\\nMehta, H. Zhuang, V . Q. Tran, D. Bahri, J. Ni et al. , “Ext5: To-\\nwards extreme multi-task scaling for transfer learning,” arXiv preprint\\narXiv:2111.10952 , 2021. 23\\n[294] N. Alex, E. Lifland, L. Tunstall, A. Thakur, P. Maham, C. J.\\nRiedel, E. Hine, C. Ashurst, P. Sedille, A. Carlier et al. , “Raft:\\nA real-world few-shot text classification benchmark,” arXiv preprint\\narXiv:2109.14076 , 2021. 23\\n[295] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al. , “Mixed\\nprecision training,” arXiv preprint arXiv:1710.03740 , 2017. 20\\n[296] T. Q. Nguyen and J. Salazar, “Transformers without tears: Improving', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='precision training,” arXiv preprint arXiv:1710.03740 , 2017. 20\\n[296] T. Q. Nguyen and J. Salazar, “Transformers without tears: Improving\\nthe normalization of self-attention,” CoRR , vol. abs/1910.05895, 2019.\\n20, 23\\n[297] Y . Wolf, N. Wies, Y . Levine, and A. Shashua, “Fundamental lim-\\nitations of alignment in large language models,” arXiv preprint\\narXiv:2304.11082 , 2023. 23\\n[298] Y . Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang, and T. B. Hashimoto, “Alpacafarm: A simulation frame-\\nwork for methods that learn from human feedback,” arXiv preprint\\narXiv:2305.14387 , 2023. 23\\n[299] S. Kim, S. Bae, J. Shin, S. Kang, D. Kwak, K. M. Yoo, and M. Seo,\\n“Aligning large language models through synthetic feedback,” arXiv\\npreprint arXiv:2305.13735 , 2023. 23\\n[300] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang,\\nand C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30}), Document(page_content='and C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047 , 2023. 23\\n[301] K. Yang, D. Klein, A. Celikyilmaz, N. Peng, and Y . Tian, “Rlcd:\\nReinforcement learning from contrast distillation for language model\\nalignment,” arXiv preprint arXiv:2307.12950 , 2023. 23\\n[302] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nNeural Information Processing Systems , vol. 35, pp. 27 730–27 744,\\n2022. 23\\n[303] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019. 24', metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 30})]\n",
      "====================\n",
      "page_content='JOURNAL OF L ATEX 3\\nFig. 3: A broader overview of LLMs, dividing LLMs into four branches: 1. Training 2. Inference 3. Applications 4. Challenges\\narticle summarizes more details of the individual models as\\ncompared to the existing efforts. On the other, it also covers\\nmore models in providing their summaries. It also delves\\ninto the details of model development, architectures, training\\ndatasets, and other related concepts to provide a self-contained\\ncomprehensive overview of this direction. Hence, this article\\naddresses an important gap of providing a concise yet compre-\\nhensive overview of the rapidly developing general direction\\nof LLM research. Our key contributions are summarized as\\nfollows.\\n•We present the first survey on the developments in LLM\\nresearch with the specific aim of providing concise yet\\ncomprehensive overview of the direction. We present\\nextensive summaries that include fine-grained details of\\nthe reviewed contributions.' metadata={'source': 'C:\\\\Users\\\\raman\\\\OneDrive\\\\Documents\\\\Datasets/large_language_models.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(type(docs))\n",
    "print(len(docs))\n",
    "# print((docs[10]))\n",
    "print(\"====================\")\n",
    "print(docs)\n",
    "print(\"====================\")\n",
    "print(docs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5374269",
   "metadata": {},
   "source": [
    "<b>Begin - Clean the Data using NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eace4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3a824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0704ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69121f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde5243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70bce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5625352c",
   "metadata": {},
   "source": [
    "<b>End - Clean the Data using NLTK</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940cd58",
   "metadata": {},
   "source": [
    "<b>Step3</b> - 3–) We can now create the embedding from these docs. Embedding creates a vector representation of a piece of text. Embedding represent every docs in a vector space and similar docs will have similar vectors. It will help us to find the docs based on the user query. We can easily do semantic search (similarity search ) where we look for pieces of text that are most similar in the vector space. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) which has integration with LangChain. We will use one of the sentence transformer model all-miniLM-L6-v2 and will load this from local. This model maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40289c0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceEmbeddings\nmodel\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      7\u001b[0m encode_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m----> 8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;43;03m#   model_name = modelPath,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\embeddings\\huggingface.py:54\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the sentence_transformer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEmbeddings\nmodel\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# modelPath = \"/model/sentence-transformer/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings':False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "#   model_name = modelPath,\n",
    "    model = model,\n",
    "  model_kwargs = model_kwargs,\n",
    "  encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63259cfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # embeddings = model.encode(sentences)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    160\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m--> 161\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentence_transformers\\models\\Transformer.py:102\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    100\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 102\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    103\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    104\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# # embeddings = model.encode(sentences)\n",
    "embeddings = model.encode(docs)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf57f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8d0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e574039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65001763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
